<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Michael U. Gutmann | Talks</title>
  <meta name="description" content="Research homepage of Michael U. Gutmann
">

<!--  <link rel="shortcut icon" href="https://michaelgutmann.github.io/assets/img/favicon.ico"> -->

<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/main.css">
  <link rel="canonical" href="https://michaelgutmann.github.io/talks/">

  <!-- added roboto font family -->
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <link rel="stylesheet"  href="https://michaelgutmann.github.io/assets/css/w3.css">
</head>


  <body>

    <header class="site-header">
  <div class="wrapper">
  
    <div class="w3-bar w3-padding-small">
        
      
      <a class="w3-bar-item w3-button w3-hover-none w3-border-white w3-hover-text-theme-color w3-mobile"  href="https://michaelgutmann.github.io/">  <strong>Michael</strong> U. Gutmann </a>
      
      
      
      
      
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/contact/">contact</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/teaching/">teaching</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/code/">code</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/talks/">talks</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/publications/">publications</a> 
      
      
      
      
      <a href="javascript:void(0)" class="w3-bar-item w3-hover-none w3-border-white  w3-button w3-hide-large w3-hide-medium w3-mobile" onclick="myFunction()">&#9776;</a>
    </div>
  </div>

   <div id="mobile" class="w3-bar-block w3-hide w3-hide-large w3-hide-medium">  
      
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/publications/">publications</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/talks/">talks</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/code/">code</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/teaching/">teaching</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/contact/">contact</a> 
      
      
      
      
      
      
    </div>
  </div>


  <script>
    function myFunction() {
    var x = document.getElementById("mobile");
    if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
    } else { 
    x.className = x.className.replace(" w3-show", "");
    }
    }
  </script>
  
</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    
    <h1 class="post-title">Selected Talks</h1>
    
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Talks clearfix">
    <ol class="bibliography"><li>

   

  <span class="title">Machine Learning for Complex Data Analysis</span>
  <span class="periodical"> <i> Helsinki AI Day </i> </span>
  <span class="periodical">Helsinki, Finland, Dec 2017</span>
  <div id="Gutmann-2017-12-13">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-12-13.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="http://cs.aalto.fi/en/current/events/2017-10-04-006" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>In science and engineering, we are often confronted with the analysis of complex and noisy data. Statistical modelling and inference provide fundamental principles for data analysis. But for complex models and data sources, the computational cost of implementing the principles and performing inference exactly is prohibitively large. I present some of my work on approximate yet computationally feasible inference and explain how machine learning helped us to reduce the computational cost by factors of 1000 or more.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Efficient Likelihood-Free Inference</span>
  <span class="periodical"> <i> Gatsby Neuroscience Unit </i> </span>
  <span class="periodical">London, UK, Nov 2017</span>
  <div id="Gutmann-2017-11-08">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-11-08.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>We consider the problem of performing statistical inference when evaluating the likelihood function is prohibitively costly but sampling from the model is possible. Such inference problems occur in a wide range of disciplines ranging from infectious disease epidemiology to econometrics and computer vision. I give an overview of my work and then explain how we can accelerate the inference substantially by combining probabilistic modelling with decision making under uncertainty.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Bayesian Inference for Intractable Infectious Disease Models</span>
  <span class="periodical"> <i> Edinburgh Infectious Diseases: Big Data and Infection Workshop </i> </span>
  <span class="periodical">Edinburgh, UK, Nov 2017</span>
  <div id="Gutmann-2017-11-29">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-11-29.pdf" target="_blank">slides</a>]
      
      
      
      [<a href="http://www.eid.ed.ac.uk/event/edinburgh-infectious-diseases-big-data-and-infection-workshop" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

  


</li>
<li>

   

  <span class="title">Efficient Statistical Inference for Intractable Models</span>
  <span class="periodical"> <i> (industry engagement) </i> </span>
  <span class="periodical">Edinburgh, UK, Aug 2017</span>
  <div id="Gutmann-2017-08-31">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-08-31.pdf" target="_blank">slides</a>]
      
      
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Bayesian Inference by Density Ratio Estimation</span>
  <span class="periodical"> <i> Dept of Computer Science, University of Oxford </i> </span>
  <span class="periodical">Oxford, UK, Jun 2017</span>
  <div id="Gutmann-2017-06-20">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-06-20.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>This talk is about Bayesian inference when the likelihood function cannot be computed but data can be generated from the model. The model’s data generating process is allowed to be arbitrarily complex. Exact solutions are then not possible. But by re-formulating the original problem as a problem of estimating the ratio between two probability density functions, I show how e.g. logistic regression can be used to obtain approximate solutions. The proposed inference framework is illustrated on stochastic nonlinear dynamical models.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Bayesian Inference by Density Ratio Estimation</span>
  <span class="periodical"> <i> Workshop in Inverse Problems and Data Science </i> </span>
  <span class="periodical">Edinburgh, UK, May 2017</span>
  <div id="Gutmann-2017-05-09">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-05-09.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="https://www.turing.ac.uk/events/inverse-problems-data-science/" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>This talk is about Bayesian inference when the likelihood function cannot be computed but data can be generated from the model. The model’s data generating process is allowed to be arbitrarily complex. Exact solutions are then not possible. But by re-formulating the original problem as a problem of estimating the ratio between two probability density functions, I show how e.g. logistic regression can be used to obtain approximate solutions. The proposed inference framework is illustrated on stochastic nonlinear dynamical models.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Likelihood-Free Inference: An Introduction to My Research</span>
  <span class="periodical"> <i> NQIT Seminar </i> </span>
  <span class="periodical">Edinburgh, UK, Apr 2017</span>
  <div id="Gutmann-2017-04-21">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-04-21.pdf" target="_blank">slides</a>]
      
      
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Modelling the Model for Approximate Bayesian Computation</span>
  <span class="periodical"> <i> BIRS Workshop on Validating and Expanding Approximate Bayesian Computation Methods </i> </span>
  <span class="periodical">Banff International Research Station, Banff, Alberta, Canada, Feb 2017</span>
  <div id="Gutmann-2017-02-20">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2017-02-20.pdf" target="_blank">slides</a>]
      
      
      
      [<a href="http://www.birs.ca/events/2017/5-day-workshops/17w5025" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Noise-contrastive estimation of unnormalised statistical models</span>
  <span class="periodical"> <i> School of Mathematics, University of Edinburgh </i> </span>
  <span class="periodical">Edinburgh, UK, Nov 2016</span>
  <div id="Gutmann-2016-11-11">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-11-11.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="http://www.maths.ed.ac.uk/school-of-mathematics" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>Parametric statistical models are often not properly normalised, that is, they do not integrate to unity. While unnormalised models can, in principle, be normalised by dividing them by their integral, the cost of computing the integral is generally prohibitively large. This is an issue because without normalisation, the likelihood function is not available for performing inference.

I present a method called "noise-contrastive estimation" where unnormalised models are estimated by solving a classification problem. I explain some of its properties and applications, and show that it is part of a general estimation framework based on the Bregman divergence.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Bayesian Inference via Classification</span>
  <span class="periodical"> <i> ANC Seminar </i> </span>
  <span class="periodical">Edinburgh, UK, Oct 2016</span>
  <div id="Gutmann-2016-10-04">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-10-04.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="http://www.anc.ed.ac.uk/events/anc-workshop-yuanhua-huang-and-chris-williams-chair-heru-praptono" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>I present my work on how to infer parameters of complex generative models by solving a series of classification problems.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Bayesian Optimization for Likelihood-Free Inference</span>
  <span class="periodical"> <i> Gaussian Process and Uncertainty Quantification Summer School </i> </span>
  <span class="periodical">Sheffield, UK, Sep 2016</span>
  <div id="Gutmann-2016-09-16">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-09-16.pdf" target="_blank">slides</a>]
      
      
      
      [<a href="http://gpss.cc/gpuqss16/program" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Tutorial on Approximate Bayesian Computation</span>
  <span class="periodical"> <i> ABCruise </i> </span>
  <span class="periodical">Helsinki, Finland, May 2016</span>
  <div id="Gutmann-2016-05-16">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-05-16.pdf" target="_blank">slides</a>]
      
      
      
      [<a href="http://www.hiit.fi/ABCruise/programme.php" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Fast Likelihood-Free Inference via Bayesian Optimization</span>
  <span class="periodical"> <i> ABCruise </i> </span>
  <span class="periodical">Helsinki, Finland, May 2016</span>
  <div id="Gutmann-2016-05-17">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-05-17.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="http://www.hiit.fi/ABCruise/programme.php" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>Statistical models may be specified in terms of stochastic computer program - a simulator - which can generate samples from the model for any configuration of the parameters. While such models support complex data generating mechanisms, the likelihood function is generally incomputable which renders statistical inference difficult. Several likelihoodfree inference methods have been proposed which share the basic idea of identifying the model parameters by finding values for which the discrepancy between simulated and observed data is small. Examples are indirect inference and approximate Bayesian computation. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">Noise-Contrastive Estimation and its Generalizations</span>
  <span class="periodical"> <i> CRiSM Workshop: Estimating Constants </i> </span>
  <span class="periodical">Department of Statistics, University of Warwick, Coventry, UK, Apr 2016</span>
  <div id="Gutmann-2016-04-21">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-04-21.pdf" target="_blank">slides</a>]
      
      
      [<a class="abstract">abs</a>]
      
      
      [<a href="https://www2.warwick.ac.uk/fac/sci/statistics/crism/workshops/estimatingconstants/" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
      <span class="abstract hidden">
        <p>Parametric statistical models are often not properly normalized, that is, they do not integrate to unity. While unnormalized models can, in principle, be normalized by dividing them by their integral, the cost of computing the integral is generally prohibitively large. This is an issue because without normalization, the likelihood function is not available for performing inference. I present a method called "noise-contrastive estimation" where unnormalized models are estimated by solving a classification problem. I explain some of its properties and applications, and show that it is part of a general estimation framework based on the SBregman divergence.</p>
      </span>
    
    
  </div>
  


</li>
<li>

   

  <span class="title">A Short Introduction to the Lasso Methodology</span>
  <span class="periodical"> <i> Research group in Statistics and Biostatistics, Department of Mathematics, University of Oslo </i> </span>
  <span class="periodical">Oslo, Norway, Mar 2016</span>
  <div id="Gutmann-2016-03-09">
    <span class="links">
      
      [<a href="https://michaelgutmann.github.io/assets/slides/Gutmann-2016-03-09.pdf" target="_blank">slides</a>]
      
      
      
      [<a href="http://www.mn.uio.no/math/english/research/groups/statistics-biostatistics/" target="_blank">url</a>]
      
   
    </span>
    <!-- Hidden abstract block -->
    
    
  </div>
  


</li></ol>

  </article>
  
  

  

</div>

      </div>
    </div>

    
<footer>

  <div class="wrapper">
    
        Last updated: 22 January 2018.
    
    Powered by <a href="http://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> and <a href="https://github.com/inukshuk/jekyll-scholar/">Jekyll-Scholar</a>.

    &copy; Copyright 2018 Michael U. Gutmann.
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://michaelgutmann.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="https://michaelgutmann.github.io/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/academicons.min.css">


<!-- Load Mathjax -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
