<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Michael U. Gutmann | Publications</title>
  <meta name="description" content="Research homepage of Michael U. Gutmann
">

<!--  <link rel="shortcut icon" href="https://michaelgutmann.github.io/assets/img/favicon.ico"> -->

  <link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/main.css">
  <link rel="canonical" href="https://michaelgutmann.github.io/publications/">

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Michael</strong> U. Gutmann 
    </span>
    

    <nav class="site-nav">

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://michaelgutmann.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://michaelgutmann.github.io/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
            <a class="page-link" href="https://michaelgutmann.github.io/publications/">publications</a>
          
        
          
            <a class="page-link" href="https://michaelgutmann.github.io/talks/">talks</a>
          
        
          
            <a class="page-link" href="https://michaelgutmann.github.io/code/">code</a>
          
        
          
            <a class="page-link" href="https://michaelgutmann.github.io/teaching/">teaching</a>
          
        
          
            <a class="page-link" href="https://michaelgutmann.github.io/contact/">contact</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="https://michaelgutmann.github.io/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
      
    <h1 class="post-title">Publications</h1>
    
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Publications clearfix">
    
<h3 id="manuscripts">Manuscripts</h3>
<ol class="bibliography"><li>
  





<div id="Arnold2017">
  
    <span class="title">Weak epistasis may drive adaptation in recombining bacteria</span>
    <span class="author">
      
        
          
            B. Arnold,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            Y. Grad,
          
        
      
        
          
            S. Sheppard,
          
        
      
        
          
            J. Corander,
          
        
      
        
          
            M. Lipsitch,
          
        
      
        
          and
          
            W. Hanage
          
        
      
    </span>

    <span class="periodical">
    
      <em>bioRxiv:119958</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Arnold2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.biorxiv.org/content/early/2017/03/23/119958" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The impact of epistasis on the evolution of multilocus traits depends on recombination. Population genetic theory has been largely developed for eukaryotes, many of which recombine so frequently that epistasis between polymorphisms has not been considered to play a large role in adaptation and has been compared to the fleeting influence of non-heritable effects. Many bacteria also recombine, some to the degree that their populations are described as ’panmictic’ or ’freely recombining’. However, whether this recombination is sufficient to limit the ability of selection to act on epistatic contributions to fitness is unknown. We create a sensitive method to quantify homologous recombination in five bacterial pathogens and use these parameter estimates in a multilocus model of bacterial evolution with additive and epistatic effects. We find that even for highly recombining species (e.g. Streptococcus pneumoniae or Helicobacter pylori), selection may act on the cumulative effects of weak (as well as strong) interactions between distant mutations since homologous recombination typically transfers only short segments. Furthermore, whether selection acts more efficiently on physically proximal loci depends on the average recombination tract length. Epistasis may thus play an important role in the adaptive evolution of bacteria and, unlike in eukaryotes, does not need to be strong, involve near loci, or require specific metapopulation dynamics.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Arnold2017,
  title = {Weak epistasis may drive adaptation in recombining bacteria},
  author = {Arnold, Brian J. and Gutmann, Michael U. and Grad, Yonatan and Sheppard, Sam K. and Corander, Jukka and Lipsitch, Marc and Hanage, William P.},
  journal = {bioRxiv:119958},
  year = {2017},
  doi = {https://doi.org/10.1101/119958},
  url = {http://www.biorxiv.org/content/early/2017/03/23/119958}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Jarvenpaa2017">
  
    <span class="title">Efficient acquisition rules for model-based approximate Bayesian computation</span>
    <span class="author">
      
        
          
            M. Järvenpää,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            A. Vehtari,
          
        
      
        
          and
          
            P. Marttinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv:1704.00520</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Jarvenpaa2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1704.00520" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, surrogate models and Bayesian optimisation (BO) have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next, but standard BO strategies are designed for optimisation and not specifically for ABC inference. Our paper addresses this gap in the literature. We propose a new acquisition rule that selects the next evaluation where the uncertainty in the posterior distribution is largest. Experiments show that the proposed method often produces the most accurate approximations, especially in high-dimensional cases or in the presence of strong prior information, compared to common alternatives.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Jarvenpaa2017,
  title = {Efficient acquisition rules for model-based approximate {B}ayesian computation},
  author = {J\"arvenp\"a\"a, M. and Gutmann, M.U. and Vehtari, A. and Marttinen, P.},
  journal = {arXiv:1704.00520},
  year = {2017},
  keywords = {Approximation Bayesian computation, Bayesian optimisation},
  url = {https://arxiv.org/abs/1704.00520}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2017b">
  
    <span class="title">ELFI: Engine for Likelihood Free Inference</span>
    <span class="author">
      
        
          
            J. Lintusaari,
          
        
      
        
          
            H. Vuollekoski,
          
        
      
        
          
            A. Kangasrääsiö,
          
        
      
        
          
            K. Skytén,
          
        
      
        
          
            M. Järvenpää,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            A. Vehtari,
          
        
      
        
          
            J. Corander,
          
        
      
        
          and
          
            S. Kaski
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv:1708.00707</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2017b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1708.00707" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for specifying LFI models commonly composed of priors, simulators, summaries, distances and other custom operations. These can be implemented in a wide variety of languages. Separating the modelling task from the inference makes it possible to use the same model with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes addition of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2017b,
  title = {ELFI: Engine for Likelihood Free Inference},
  author = {Lintusaari, Jarno and Vuollekoski, Henri and Kangasr\"a\"asi\"o, Antti and Skyt\'en, Kusti and J\"arvenp\"a\"a, Marko and Gutmann, Michael U. and Vehtari, Aki and Corander, Jukka and Kaski, Samuel},
  journal = {arXiv:1708.00707},
  year = {2017},
  keywords = {Likelihood-free inference, approximate Bayesian computation, Python, software, parallel computing},
  url = {https://arxiv.org/abs/1708.00707}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Todorovic2017">
  
    <span class="title">Efficient Bayesian Inference of Atomistic Structure in Complex Functional Materials</span>
    <span class="author">
      
        
          
            M. Todorović,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            J. Corander,
          
        
      
        
          and
          
            P. Rinke
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv:1708.09274</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Todorovic2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1708.09274" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Tailoring the functional properties of advanced organic/inorganic heterogeonous devices to their intended technological applications requires knowledge and control of the microscopic structure inside the device. Atomistic quantum mechanical simulation methods deliver accurate energies and properties for individual configurations, however, finding the most favourable configurations remains computationally prohibitive. We propose a ’building block’-based Bayesian Optimisation Structure Search (BOSS) approach for addressing extended organic/inorganic interface problems and demonstrate its feasibility in a molecular surface adsorption study. In BOSS, a likelihood-free Bayesian scheme accelerates the identification of material energy landscapes with the number of sampled configurations during active learning, enabling structural inference with high chemical accuracy and featuring large simulation cells. This allowed us to identify several most favourable molecular adsorption configurations for C60 on the (101) surface of TIO2 anatase and clarify the key molecule-surface interactions governing structural assembly. Inferred structures were in good agreement with detailed experimental images of this surface adsorbate, demonstrating good predictive power of BOSS and opening the route towards large-scale surface adsorption studies of molecular aggregates and films.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Todorovic2017,
  title = {Efficient Bayesian Inference of Atomistic Structure in Complex Functional Materials},
  author = {Todorovi\'c, Milica and Gutmann, Michael U. and Corander, Jukka and Rinke, Patrick},
  journal = {arXiv:1708.09274},
  year = {2017},
  keywords = {Atomistic structure search, quantum mechanical simulation, Bayesian optimisation},
  url = {https://arxiv.org/abs/1708.09274}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Dutta2016">
  
    <span class="title">Likelihood-Free Inference by Ratio Estimation</span>
    <span class="author">
      
        
          
            R. Dutta,
          
        
      
        
          
            J. Corander,
          
        
      
        
          
            S. Kaski,
          
        
      
        
          and
          
            <em> M. Gutmann</em>
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv:1611.10242</em>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Dutta2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1611.10242" target="_blank">url</a>]
    
    
    
    [<a href="/assets/papers/Dutta2016.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of "closeness" is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on toy problems and use it to perform inference for stochastic nonlinear dynamical systems.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Dutta2016,
  title = {Likelihood-Free Inference by Ratio Estimation},
  author = {Dutta, R. and Corander, J. and Kaski, S. and Gutmann, M.U.},
  journal = {arXiv:1611.10242},
  year = {2016},
  keywords = {approximate Bayesian computation, synthetic likelihood, summary statistics selection, density-ratio estimation; logistic regression; probabilistic classification, stochastic dynamical systems},
  url = {https://arxiv.org/abs/1611.10242}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Jarvenpaa2016">
  
    <span class="title">Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria</span>
    <span class="author">
      
        
          
            M. Järvenpää,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            A. Vehtari,
          
        
      
        
          and
          
            P. Marttinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv:1610.06462</em>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Jarvenpaa2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1502.05503" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Some statistical models are specified via a data generating process for which the likelihood function cannot be computed in closed form. Standard likelihood-based inference is then not feasible but the model parameters can be inferred by finding the values which yield simulated data that resemble the observed data. This approach faces at least two major difficulties: The first difficulty is the choice of the discrepancy measure which is used to judge whether the simulated data resemble the observed data. The second difficulty is the computationally efficient identification of regions in the parameter space where the discrepancy is low. We give here an introduction to our recent work where we tackle the two difficulties through classification and Bayesian optimization.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Jarvenpaa2016,
  title = {Gaussian process modeling in approximate {B}ayesian computation to estimate horizontal gene transfer in bacteria},
  author = {J\"arvenp\"a\"a, M. and Gutmann, M.U. and Vehtari, A. and Marttinen, P.},
  journal = {arXiv:1610.06462},
  year = {2016},
  keywords = {Approximation Bayesian computation, emulation, Gaussian processes, model selection},
  url = {https://arxiv.org/abs/1502.05503}
}
</pre>
  </span>
  
  
</div>


</li></ol>

<h3 id="refereed-papers">Refereed papers</h3>
<ol class="bibliography"><li>
  





<div id="Gutmann2017">
  
    <span class="title">Likelihood-free inference via classification</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            R. Dutta,
          
        
      
        
          
            S. Kaski,
          
        
      
        
          and
          
            J. Corander
          
        
      
    </span>

    <span class="periodical">
    
      <em>Statistics and Computing</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2017.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2017.supp.pdf" target="_blank">supp</a>]
    
    
    [<a href="/assets/papers/Gutmann2017.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1007/s11222-017-9738-6" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2017.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2017.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2017,
  title = {Likelihood-free inference via classification},
  author = {Gutmann, M.U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal = {Statistics and Computing},
  year = {2017},
  month = mar,
  volume = {in press},
  day = {13},
  doi = {10.1007/s11222-017-9738-6},
  issn = {1573-1375},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {https://doi.org/10.1007/s11222-017-9738-6},
  month_numeric = {3}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2017">
  
    <span class="title">Fundamentals and Recent Developments in Approximate Bayesian Computation</span>
    <span class="author">
      
        
          
            J. Lintusaari,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            R. Dutta,
          
        
      
        
          
            S. Kaski,
          
        
      
        
          and
          
            J. Corander
          
        
      
    </span>

    <span class="periodical">
    
      <em>Systematic Biology</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1093/sysbio/syw077" target="_blank">url</a>]
    
    
    
    [<a href="/assets/papers/Lintusaari2017.slides.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Lintusaari2017.slides1.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible. We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2017,
  title = {Fundamentals and Recent Developments in Approximate {B}ayesian Computation},
  author = {Lintusaari, J. and Gutmann, M.U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal = {Systematic Biology},
  year = {2017},
  month = jan,
  number = {1},
  pages = {e66--e82},
  volume = {66},
  doi = {10.1093/sysbio/syw077},
  issn = {1063-5157},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {http://dx.doi.org/10.1093/sysbio/syw077},
  month_numeric = {1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2017">
  
    <span class="title">Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure</span>
    <span class="author">
      
        
          
            H. Sasaki,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            H. Shouno,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>Neural Computation</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they usually concentrated on higher-order correlations such as energy (square) correlations. Yet, linear correlations are a most fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods, so they can only be analyzed by developing new methods which explicitly allow for linearly correlated components. In this paper, we propose a probabilistic model of linear non-Gaussian components which are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parametrized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score matching (HyvÃ¤rinen, 2005), the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method improves identifiability of non-Gaussian components by simultaneously learning their correlation structure. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data show that the method finds new kinds of dependencies between the components.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Sasaki2017,
  title = {Simultaneous Estimation of Non-{G}aussian Components and their Correlation Structure},
  author = {Sasaki, H. and Gutmann, M.U. and Shouno, H. and Hyv\"arinen, A.},
  journal = {Neural Computation},
  year = {2017},
  volume = {in press},
  keywords = {Extensions of ICA, speech data, natural images}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Srivastava2017">
  
    <span class="title">VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning</span>
    <span class="author">
      
        
          
            A. Srivastava,
          
        
      
        
          
            A. Valkov,
          
        
      
        
          
            C. Russell,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            C. Sutton
          
        
      
    </span>

    <span class="periodical">
    
      <em>Advances in Neural Information Processing Systems 30 (NIPS 2017)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    
    
    [<a href="/assets/papers/Srivastava2017.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1705.07761" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Srivastava2017,
  title = {VEEGAN: Reducing Mode Collapse in {GAN}s using Implicit Variational Learning},
  author = {Srivastava, A. and Valkov, A. and Russell, C. and Gutmann, M.U. and Sutton, C.},
  journal = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  year = {2017},
  url = {https://arxiv.org/abs/1705.07761}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Tietavainen2017">
  
    <span class="title">Bayesian inference of physiologically meaningful parameters from body sway measurements</span>
    <span class="author">
      
        
          
            A. Tietäväinen,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            E. Keski-Vakkuri,
          
        
      
        
          
            J. Corander,
          
        
      
        
          and
          
            E. Haeggström
          
        
      
    </span>

    <span class="periodical">
    
      <em>Scientific Reports</em>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Tietavainen2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1038/s41598-017-02372-1" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The control of the human body sway by the central nervous system, muscles, and conscious brain is of interest since body sway carries information about the physiological status of a person. Several models have been proposed to describe body sway in an upright standing position, however, due to the statistical intractability of the more realistic models, no formal parameter inference has previously been conducted and the expressive power of such models for real human subjects remains unknown. Using the latest advances in Bayesian statistical inference for intractable models, we fitted a nonlinear control model to posturographic measurements, and we showed that it can accurately predict the sway characteristics of both simulated and real subjects. Our method provides a full statistical characterization of the uncertainty related to all model parameters as quantified by posterior probability density functions, which is useful for comparisons across subjects and test settings. The ability to infer intractable control models from sensor data opens new possibilities for monitoring and predicting body status in health applications.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Tietavainen2017,
  title = {Bayesian inference of physiologically meaningful parameters from body sway measurements},
  author = {Tiet\"av\"ainen, A. and Gutmann, M.U. and Keski-Vakkuri, E. and Corander, J. and Haeggstr\"om, E.},
  journal = {Scientific Reports},
  year = {2017},
  number = {3771},
  pages = {1--14},
  volume = {7},
  issn = {2045-2322},
  keywords = {Approximate Bayesian inference, nonlinear dynamical systems, balance and muscular control},
  url = {https://doi.org/10.1038/s41598-017-02372-1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2016a">
  
    <span class="title">Bayesian optimization for likelihood-free inference of simulator-based statistical models</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            J. Corander
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Machine Learning Research</em>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2016a.pdf" target="_blank">pdf</a>]
    
    
    
    [<a href="/assets/papers/Gutmann2016a.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://jmlr.org/papers/v17/15-017.html" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2016a.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2016a.slides2.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2016a.slides3.pdf" target="_blank">slides</a>]
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2016a,
  title = {Bayesian optimization for likelihood-free inference of simulator-based statistical models},
  author = {Gutmann, M.U. and Corander, J},
  journal = {Journal of Machine Learning Research},
  year = {2016},
  number = {125},
  pages = {1--47},
  volume = {17},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {http://jmlr.org/papers/v17/15-017.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2016">
  
    <span class="title">On the identifiability of transmission dynamic models for infectious diseases</span>
    <span class="author">
      
        
          
            J. Lintusaari,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            S. Kaski,
          
        
      
        
          and
          
            J. Corander
          
        
      
    </span>

    <span class="periodical">
    
      <em>Genetics</em>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    [<a href="/assets/papers/Lintusaari2016.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1534/genetics.115.180034" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Understanding the transmission dynamics of infectious diseases is important for both biological research and public health applications. It has been widely demonstrated that statistical modeling provides a firm basis for inferring relevant epidemiological quantities from incidence and molecular data. However, the complexity of transmission dynamic models causes two challenges: Firstly, the likelihood function of the models is generally not computable and computationally intensive simulation-based inference methods need to be employed. Secondly, the model may not be fully identifiable from the available data. While the first difficulty can be tackled by computational and algorithmic advances, the second obstacle is more fundamental. Identifiability issues may lead to inferences which are more driven by the prior assumptions than the data themselves. We here consider a popular and relatively simple, yet analytically intractable model for the spread of tuberculosis based on classical IS6110 fingerprinting data. We report on the identifiability of the model, presenting also some methodological advances regarding the inference. Using likelihood approximations, it is shown that the reproductive value cannot be identified from the data available and that the posterior distributions obtained in previous work have likely been substantially dominated by the assumed prior distribution. Further, we show that the inferences are influenced by the assumed infectious population size which has generally been kept fixed in previous work. We demonstrate that the infectious population size can be inferred if the remaining epidemiological parameters are already known with sufficient precision.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2016,
  title = {On the identifiability of transmission dynamic models for infectious diseases},
  author = {Lintusaari, J. and Gutmann, M.U. and Kaski, S. and Corander, J.},
  journal = {Genetics},
  year = {2016},
  number = {3},
  pages = {911--918},
  volume = {202},
  keywords = {Intractable likelihoods, epidemiology of infectious disease},
  url = {https://doi.org/10.1534/genetics.115.180034}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Numminen2016">
  
    <span class="title">The impact of host metapopulation structure on the population genetics of colonizing bacteria</span>
    <span class="author">
      
        
          
            E. Numminen,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            M. Shubin,
          
        
      
        
          
            P. Marttinen,
          
        
      
        
          
            G. Méric,
          
        
      
        
          
            W. Schaik,
          
        
      
        
          
            T. Coque,
          
        
      
        
          
            F. Baquero,
          
        
      
        
          
            R. Willems,
          
        
      
        
          
            S. Sheppard,
          
        
      
        
          
            E. Feil,
          
        
      
        
          
            W. Hanage,
          
        
      
        
          and
          
            J. Corander
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Theoretical Biology</em>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Numminen2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.sciencedirect.com/science/article/pii/S0022519316001156" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many key bacterial pathogens are frequently carried asymptomatically, and the emergence and spread of these opportunistic pathogens can be driven, or mitigated, via demographic changes within the host population. These inter-host transmission dynamics combine with basic evolutionary parameters such as rates of mutation and recombination, population size and selection, to shape the genetic diversity within bacterial populations. Whilst many studies have focused on how molecular processes underpin bacterial population structure, the impact of host migration and the connectivity of the local populations has received far less attention. A stochastic neutral model incorporating heightened local transmission has been previously shown to fit closely with genetic data for several bacterial species. However, this model did not incorporate transmission limiting population stratification, nor the possibility of migration of strains between subpopulations, which we address here by presenting an extended model. We study the consequences of migration in terms of shared genetic variation and show by simulation that the previously used summary statistic, the allelic mismatch distribution, can be insensitive to even large changes in microepidemic and migration rates. Using likelihood-free inference with genotype network topological summaries we fit a simpler model to commensal and hospital samples from the common nosocomial pathogens Staphylococcus aureus, Staphylococcus epidermidis, Enterococcus faecalis and Enterococcus faecium. Only the hospital data for E. faecium display clearly marked deviations from the model predictions which may be attributable to its adaptation to the hospital environment.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Numminen2016,
  title = {The impact of host metapopulation structure on the population genetics of colonizing bacteria},
  author = {Numminen, E. and Gutmann, Michael and Shubin, Mikhail and Marttinen, Pekka and M\'eric, Guillaume and van Schaik, Willem and Coque, Teresa and Baquero, Fernando and Willems, R.J.L. and Sheppard, S.K. and Feil, E.J. and Hanage, W.P. and Corander, Jukka},
  journal = {Journal of Theoretical Biology},
  year = {2016},
  pages = {53--62},
  volume = {396},
  keywords = {Bacterial evolution, Genetic structure, Migration, Population dynamics},
  url = {http://www.sciencedirect.com/science/article/pii/S0022519316001156}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Marttinen2015">
  
    <span class="title">Recombination produces coherent bacterial species clusters in both core and accessory genomes</span>
    <span class="author">
      
        
          
            P. Marttinen,
          
        
      
        
          
            N. Croucher,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            J. Corander,
          
        
      
        
          and
          
            W. Hanage
          
        
      
    </span>

    <span class="periodical">
    
      <em>Microbial Genomics</em>
    
    
      2015
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Marttinen2015.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Marttinen2015.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1099/mgen.0.000038" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Background:
Population samples show bacterial genomes can be divided into a core of ubiquitous genes and accessory genes that are present in a fraction of isolates. The ecological significance of this variation in gene content remains unclear. However, microbiologists agree that a bacterial species should be ’genomically coherent’, even though there is no consensus on how this should be determined.

Results:
We use a parsimonious model combining diversification in both the core and accessory genome, including mutation, homologous recombination (HR) and horizontal gene transfer (HGT) introducing new loci, to produce a population of interacting clusters of strains with varying genome content. New loci introduced by HGT may then be transferred on by HR. The model fits well to a systematic population sample of 616 pneumococcal genomes, capturing the major features of the population structure with parameter values that agree well with empirical estimates.

Conclusions:
The model does not include explicit selection on individual genes, suggesting that crude comparisons of gene content may be a poor predictor of ecological function. We identify a clearly divergent subpopulation of pneumococci that are inconsistent with the model and may be considered genomically incoherent with the rest of the population. These strains have a distinct disease tropism and may be rationally defined as a separate species. We also find deviations from the model that may be explained by recent population bottlenecks or spatial structure.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Marttinen2015,
  title = {Recombination produces coherent bacterial species clusters in both core and accessory genomes},
  author = {Marttinen, P. and Croucher, N.J. and Gutmann, M. and Corander, J. and Hanage, W.P.},
  journal = {Microbial Genomics},
  year = {2015},
  number = {5},
  volume = {1},
  doi = {http://dx.doi.org/10.1099/mgen.0.000038},
  keywords = {core/accessory genome, evolution, computational modeling, speciation, recombination},
  url = {http://dx.doi.org/10.1099/mgen.0.000038}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2014">
  
    <span class="title">Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            V. Laparra,
          
        
      
        
          
            A. Hyvärinen,
          
        
      
        
          and
          
            J. Malo
          
        
      
    </span>

    <span class="periodical">
    
      <em>PLOS ONE</em>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2014.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1371%2Fjournal.pone.0086481" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Independent component and canonical correlation analysis are two general-purpose statistical methods with wide applicability. In neuroscience, independent component analysis of chromatic natural images explains the spatio-chromatic structure of primary cortical receptive fields in terms of properties of the visual environment. Canonical correlation analysis explains similarly chromatic adaptation to different illuminations. But, as we show in this paper, neither of the two methods generalizes well to explain both spatio-chromatic processing and adaptation at the same time. We propose a statistical method which combines the desirable properties of independent component and canonical correlation analysis: It finds independent components in each data set which, across the two data sets, are related to each other via linear or higher-order correlations. The new method is as widely applicable as canonical correlation analysis, and also to more than two data sets. We call it higher-order canonical correlation analysis. When applied to chromatic natural images, we found that it provides a single (unified) statistical framework which accounts for both spatio-chromatic processing and adaptation. Filters with spatio-chromatic tuning properties as in the primary visual cortex emerged and corresponding-colors psychophysics was reproduced reasonably well. We used the new method to make a theory-driven testable prediction on how the neural response to colored patterns should change when the illumination changes. We predict shifts in the responses which are comparable to the shifts reported for chromatic contrast habituation.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2014,
  title = {Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images},
  author = {Gutmann, M.U. and Laparra, Valero and Hyv\"arinen, Aapo and Malo, Jes\'us},
  journal = {PLOS ONE},
  year = {2014},
  number = {2},
  pages = {e86481},
  volume = {9},
  doi = {10.1371/journal.pone.0086481},
  keywords = {Generalization of canonical correlation analysis, data fusion, natural image statistics, spatio-chromatic adaptation},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0086481}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Liu2014">
  
    <span class="title">Direct learning of sparse changes in Markov networks by density ratio estimation</span>
    <span class="author">
      
        
          
            S. Liu,
          
        
      
        
          
            J. Quinn,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            T. Suzuki,
          
        
      
        
          and
          
            M. Sugiyama
          
        
      
    </span>

    <span class="periodical">
    
      <em>Neural Computation</em>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Liu2014.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1162/NECO_a_00589" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, a critical bottleneck of the naive approach, can be remarkably mitigated. We also give the dual formulation of the optimization problem, which further reduces the computation cost for large-scale Markov networks. Through experiments, we demonstrate the usefulness of our method.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Liu2014,
  title = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  author = {Liu, S. and Quinn, J.A. and Gutmann, M.U. and Suzuki, T. and Sugiyama, M.},
  journal = {Neural Computation},
  year = {2014},
  number = {6},
  pages = {1169--1197},
  volume = {26},
  doi = {10.1162/NECO_a_00589},
  keywords = {Undirected graphical models, density ratio estimation},
  url = {http://dx.doi.org/10.1162/NECO_a_00589}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2014">
  
    <span class="title">Estimating dependency structures for non-Gaussian components with linear and energy correlations</span>
    <span class="author">
      
        
          
            H. Sasaki,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            H. Shouno,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</em>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2014.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Sasaki2014.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v33/sasaki14.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Sasaki2014,
  title = {Estimating dependency structures for non-{G}aussian components with linear and energy correlations},
  author = {Sasaki, H. and Gutmann, M.U. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2014},
  keywords = {Extensions of ICA, natural image statistics},
  url = {http://proceedings.mlr.press/v33/sasaki14.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2013">
  
    <span class="title">A three-layer model of natural image statistics</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Physiology-Paris</em>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2013.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.sciencedirect.com/science/article/pii/S0928425713000028" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2013.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2013.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>An important property of visual systems is to be simultaneously both selective to specific patterns found in the sensory input and invariant to possible variations. Selectivity and invariance (tolerance) are opposing requirements. It has been suggested that they could be joined by iterating a sequence of elementary selectivity and tolerance computations. It is, however, unknown what should be selected or tolerated at each level of the hierarchy. We approach this issue by learning the computations from natural images. We propose and estimate a probabilistic model of natural images that consists of three processing layers. Two natural image data sets are considered: image patches, and complete visual scenes downsampled to the size of small patches. For both data sets, we find that in the first two layers, simple and complex cell-like computations are performed. In the third layer, we mainly find selectivity to longer contours; for patch data, we further find some selectivity to texture, while for the downsampled complete scenes, some selectivity to curvature is observed.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2013,
  title = {A three-layer model of natural image statistics},
  author = {Gutmann, M.U. and Hyv\"arinen, A.},
  journal = {Journal of Physiology-Paris},
  year = {2013},
  number = {5},
  pages = {369--398},
  volume = {107},
  booktitle = {Special issue: Neural Coding and Natural Image Statistics},
  doi = {10.1016/j.jphysparis.2013.01.001},
  issn = {0928-4257},
  keywords = {Natural images, probabilistic modeling, visual processing, selectivity, invariance, sparse coding, unsupervised deep learning},
  url = {http://www.sciencedirect.com/science/article/pii/S0928425713000028}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2013b">
  
    <span class="title">Estimation of unnormalized statistical models without numerical integration</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Workshop on Information Theoretic Methods in Science and Engineering</em>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2013b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.me.inf.kyushu-u.ac.jp/witmse2013/" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Parametric statistical models of continuous or discrete valued data are often not properly normalized, that is, they do not integrate or sum to unity. The normalization is essential for maximum likelihood estimation. While in principle, models can always be normalized by dividing them by their integral or sum (their partition function), this can in practice be extremely difficult. We have been developing methods for the estimation of unnormalized models which do not approximate the partition function using numerical integration. We review these methods, score matching and noise-contrastive estimation, point out extensions and connections both between them and methods by other authors, and discuss their pros and cons.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2013b,
  title = {Estimation of unnormalized statistical models without numerical integration},
  author = {Gutmann, M.U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Workshop on Information Theoretic Methods in Science and Engineering},
  year = {2013},
  keywords = {Unnormalized models, energy-based models, intractable likelihood},
  url = {http://www.me.inf.kyushu-u.ac.jp/witmse2013/}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Liu2013">
  
    <span class="title">Direct learning of sparse changes in Markov networks by density ratio estimation</span>
    <span class="author">
      
        
          
            S. Liu,
          
        
      
        
          
            J. Quinn,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            M. Sugiyama
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</em>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Liu2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1007/978-3-642-40991-2_38" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, which is a critical computational bottleneck of the naive approach, can be remarkably mitigated. Through experiments on gene expression and Twitter data analysis, we demonstrate the usefulness of our method.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Liu2013,
  title = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  author = {Liu, Song and Quinn, JohnA. and Gutmann, Michael and Sugiyama, Masashi},
  booktitle = {Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  year = {2013},
  issn = {978-3-642-40990-5},
  keywords = {Undirected graphical models},
  url = {http://dx.doi.org/10.1007/978-3-642-40991-2_38}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2013">
  
    <span class="title">Correlated topographic analysis: estimating an ordering of correlated components</span>
    <span class="author">
      
        
          
            H. Sasaki,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            H. Shouno,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>Machine Learning</em>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1007/s10994-013-5351-x" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>This paper describes a novel method, which we call correlated topographic analysis (CTA), to estimate non-Gaussian components and their ordering (topography). The method is inspired by a central motivation of recent variants of independent component analysis (ICA), namely, to make use of the residual statistical dependency which ICA cannot remove. We assume that components nearby on the topographic arrangement have both linear and energy correlations, while far-away components are statistically independent. We use these dependencies to fix the ordering of the components. We start by proposing the generative model for the components. Then, we derive an approximation of the likelihood based on the model. Furthermore, since gradient methods tend to get stuck in local optima, we propose a three-step optimization method which dramatically improves topographic estimation. Using simulated data, we show that CTA estimates an ordering of the components and generalizes a previous method in terms of topography estimation. Finally, to demonstrate that CTA is widely applicable, we learn topographic representations for three kinds of real data: natural images, outputs of simulated complex cells and text data.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Sasaki2013,
  title = {Correlated topographic analysis: estimating an ordering of correlated components},
  author = {Sasaki, Hiroaki and Gutmann, Michael and Shouno, Hayaru and Hyv\"arinen, Aapo},
  journal = {Machine Learning},
  year = {2013},
  number = {2-3},
  pages = {285--317},
  volume = {92},
  issn = {0885-6125},
  keywords = {independent component analysis, topographic representation, natural image statistics, higher order features, natural language processing},
  publisher = {Springer US},
  url = {http://dx.doi.org/10.1007/s10994-013-5351-x}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2012a">
  
    <span class="title">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>Journal of Machine Learning Research</em>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2012a.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.jmlr.org/papers/v13/gutmann12a.html" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2012a.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2012a.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2012a,
  title = {{N}oise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  journal = {Journal of Machine Learning Research},
  year = {2012},
  pages = {307--361},
  volume = {13},
  keywords = {unnormalised models, unsupervised deep learning, natural image statistics},
  url = {http://www.jmlr.org/papers/v13/gutmann12a.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2012b">
  
    <span class="title">Learning a selectivity–invariance–selectivity feature extraction architecture for images</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Pattern Recognition (ICPR)</em>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2012b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2012b.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Selectivity and invariance are thought to be important ingredients in biological or artificial visual systems. A fundamental problem is, however, to know what the visual system should be selective to and what to be invariant to. Building a statistical model of images, we learn here a three-layer feature extraction system where the selectivity and invariance emerges from the properties of the images.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2012b,
  title = {Learning a selectivity--invariance--selectivity feature extraction architecture for images},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Pattern Recognition (ICPR)},
  year = {2012},
  keywords = {Natural image statistics,unnormalised model, unsupervised deep learning}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2012">
  
    <span class="title">Topographic analysis of correlated components</span>
    <span class="author">
      
        
          
            H. Sasaki,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            H. Shouno,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In JMLR: Workshop and Conference Proceedings</em>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2012.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v25/sasaki12" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants are assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the source where the components can have linear and higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Sasaki2012,
  title = {Topographic analysis of correlated components},
  author = {Sasaki, H. and Gutmann, M. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {JMLR: Workshop and Conference Proceedings},
  year = {2012},
  pages = {1--14},
  series = {Asian Conference on Machine Learning (ACML)},
  volume = {25},
  keywords = {independent component analysis, topographic representation, higher order correlation, linear correlation, natural image statistics, natural language processing.},
  url = {http://proceedings.mlr.press/v25/sasaki12}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2011b">
  
    <span class="title">Bregman divergence as general framework to estimate unnormalized statistical models</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            J. Hirayama
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</em>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2011b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1202.3727" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in unsupervised learning.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2011b,
  title = {{B}regman divergence as general framework to estimate unnormalized statistical models},
  author = {Gutmann, M.U. and Hirayama, J.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2011},
  keywords = {Unnormalized (energy-based) models, estimation theory, density ratio estimation},
  url = {https://arxiv.org/abs/1202.3727}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2011a">
  
    <span class="title">Extracting coactivated features from multiple data sets</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</em>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2011a.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2011a.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-21735-7_40" target="_blank">url</a>]
    
    
    
    [<a href="/assets/papers/Gutmann2011a.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a nonlinear generalization of Canonical Correlation Analysis (CCA) to find related structure in multiple data sets. The new method allows to analyze an arbitrary number of data sets, and the extracted features capture higher-order statistical dependencies. The features are independent components that are coupled across the data sets. The coupling takes the form of coactivation (dependencies of variances). We validate the new method on artificial data, and apply it to natural images and brain imaging data</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2011a,
  title = {{E}xtracting coactivated features from multiple data sets},
  author = {Gutmann, M. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2011},
  doi = {10.1007/978-3-642-21735-7_40},
  keywords = {Data fusion, natural image statistics},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-21735-7_40}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Laparra2011">
  
    <span class="title">Complex-valued independent component analysis of natural images</span>
    <span class="author">
      
        
          
            V. Laparra,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            J. Malo,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</em>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Laparra2011.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-21738-8_28" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Linear independent component analysis (ICA) learns simple cell receptive fields from natural images. Here, we show that linear complex-valued ICA learns complex cell properties from Fourier-transformed natural images, i.e. two Gabor-like filters with quadrature-phase relationship. Conventional methods for complex-valued ICA assume that the phases of the output signals have uniform distribution. We show here that for natural images the phase distributions are, however, often far from uniform. We thus relax the uniformity assumption and model also the phase of the sources in complex-valued ICA. Compared to the original complex ICA model, the new model provides a better fit to the data, and leads to Gabor filters of qualitatively different shape.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Laparra2011,
  title = {{C}omplex-valued independent component analysis of natural images},
  author = {Laparra, V. and Gutmann, M. and Malo, J. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2011},
  doi = {10.1007/978-3-642-21738-8_28},
  keywords = {complex independent components analysis, natural image statistics, modeling Fourier phase distribution},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-21738-8_28}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2010">
  
    <span class="title">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</em>
    
    
      2010
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2010.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v9/gutmann10a.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2010,
  title = {{N}oise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2010},
  keywords = {unnormalized models, energy-based models, estimation theory, natural image statistics},
  url = {http://proceedings.mlr.press/v9/gutmann10a.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Pihlaja2010">
  
    <span class="title">A family of computationally efficient and simple estimators for unnormalized statistical models</span>
    <span class="author">
      
        
          
            M. Pihlaja,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</em>
    
    
      2010
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Pihlaja2010.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We introduce a new family of estimators for unnormalized statistical models. Our family of estimators is parameterized by two nonlinear functions and uses a single sample from an auxiliary distribution, generalizing Maximum Likelihood Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we can estimate the partition function like any other parameter in the model. The estimation is done by optimizing an algebraically simple, well defined objective function, which allows for the use of dedicated optimization methods. We establish consistency of the estimator family and give an expression for the asymptotic covariance matrix, which enables us to further analyze the influence of the nonlinearities and the auxiliary density on estimation performance. Some estimators in our family are particularly stable for a wide range of auxiliary densities. Interestingly, a specific choice of the nonlinearity establishes a connection between density estimation and classification by nonlinear logistic regression. Finally, the optimal amount of auxiliary samples relative to the given amount of the data is considered from the perspective of computational efficiency.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Pihlaja2010,
  title = {{A} family of computationally efficient and simple estimators for unnormalized statistical models},
  author = {Pihlaja, M. and Gutmann, M.U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2010},
  keywords = {unnormalized models, energy-based models, estimation theory}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2009b">
  
    <span class="title">Learning features by contrasting natural images with noise</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</em>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2009b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-04277-5_63" target="_blank">url</a>]
    
    
    
    [<a href="/assets/papers/Gutmann2009b.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Modeling the statistical structure of natural images is interesting for reasons related to neuroscience as well as engineering. Currently, this modeling relies heavily on generative probabilistic models. The estimation of such models is, however, difficult, especially when they consist of multiple layers. If the goal lies only in estimating the features, i.e. in pinpointing structure in natural images, one could also estimate instead a discriminative probabilistic model where multiple layers are more easily handled. For that purpose, we propose to estimate a classifier that can tell natural images apart from reference data which has been constructed to contain some known structure of natural images. The features of the classifier then reveal the interesting structure. Here, we use a classifier with one layer of features and reference data which contains the covariance-structure of natural images. We show that the features of the classifier are similar to those which are obtained from generative probabilistic models. Furthermore, we investigate the optimal shape of the nonlinearity that is used within the classifier.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2009b,
  title = {{L}earning features by contrasting natural images with noise},
  author = {Gutmann, M. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2009},
  keywords = {natural image statistics, learning by comparison, unnormalized models, energy-based models},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-04277-5_63}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2009">
  
    <span class="title">Learning reconstruction and prediction of natural stimuli by a population of spiking neurons</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In European Symposium on Artificial Neural Networks (ESANN)</em>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2009.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2009.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a model for learning representations of time dependent data with a population of spiking neurons. Encoding is based on a standard spiking neuron model, and the spike timings of the neurons represent the stimulus. Learning is based on the sole principle of maximization of representation accuracy: the stimulus can be decoded from the spike timings with minimum error. Since the encoding is causal, we propose two different representation strategies: The spike timings represent the stimulus either in a predictive manner or by reconstructing past input. We apply the model to speech data and discuss differences between the emergent representations.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2009,
  title = {{L}earning reconstruction and prediction of natural stimuli by a population of spiking neurons},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
  year = {2009},
  keywords = {Learning data representations, spiking neurons}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Koster2009">
  
    <span class="title">Learning natural image structure with a horizonal product model</span>
    <span class="author">
      
        
          
            U. Köster,
          
        
      
        
          
            J. Lindgren,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            A. Hyvärinen
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings on the International Conference on Independent Component Analysis and Signal Separation</em>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Koster2009.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-00599-2_64" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a novel extension to Independent Component Analysis (ICA), where the data is generated as the product of two submodels, each of which follow an ICA model, and which combine in a horizontal fashion. This is in contrast to previous nonlinear extensions to ICA which were based on a hierarchy of layers. We apply the product model to natural image patches and report the emergence of localized masks in the additional network layer, while the Gabor features that are obtained in the primary layer change their tuning properties and become less localized. As an interpretation we suggest that the model learns to separate the localization of image features from other properties, since identity and position of a feature are plausibly independent. We also show that the horizontal model can be interpreted as an overcomplete model where the features are no longer independent.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Koster2009,
  title = {{L}earning natural image structure with a horizonal product model},
  author = {K\"oster, U. and Lindgren, J. and Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings on the International Conference on Independent Component Analysis and Signal Separation},
  year = {2009},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-00599-2_64}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2008">
  
    <span class="title">Toward data representation with spiking neurons</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            K. Aihara
          
        
      
    </span>

    <span class="periodical">
    
      <em>Artificial Life and Robotics</em>
    
    
      2008
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2008.pdf" target="_blank">pdf</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2008.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/article/10.1007/s10015-007-0471-7" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Notable advances in the understanding of neural processing have been made when sensory systems were investigated from the viewpoint of adaptation to the statistical structure of its input space. For this purpose, mathematical methods for data representation were used. Here, we point out that emphasis on the input structure has happened at cost of the biological plausibility of the corresponding neuron models which process the natural stimuli. The signal transformation of the data representation methods does not correspond well to the signal transformations happening on the single cell level in neural systems. Hence, we propose here data representation by means of spiking neuron models. We formulate the data representation problem as an optimization problem and derive the fundamental quantities for an iterative learning scheme.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2008,
  title = {{T}oward data representation with spiking neurons},
  author = {Gutmann, M. and Aihara, K.},
  journal = {Artificial Life and Robotics},
  year = {2008},
  pages = {223-226},
  volume = {12},
  doi = {10.1007/s10015-007-0471-7},
  keywords = {spiking neuron, encoding, decoding, learning data representation},
  url = {https://link.springer.com/article/10.1007/s10015-007-0471-7}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2008c">
  
    <span class="title">Learning encoding and decoding filters for data representation with a spiking neuron</span>
    <span class="author">
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          
            A. Hyvärinen,
          
        
      
        
          and
          
            K. Aihara
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em>
    
    
      2008
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2008c.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1109/IJCNN.2008.4633797" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Data representation methods related to ICA and sparse coding have successfully been used to model neural representation. However, they are highly abstract methods, and the neural encoding does not correspond to a detailed neuron model. This limits their power to provide deeper insight into the sensory systems on a cellular level. We propose here data representation where the encoding happens with a spiking neuron. The data representation problem is formulated as an optimization problem: Encode the input so that it can be decoded from the spike train, and optionally, so that energy consumption is minimized. The optimization leads to a learning rule for the encoder and decoder which features synergistic interaction: The decoder provides feedback affecting the plasticity of the encoder while the encoder provides optimal learning data for the decoder.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2008c,
  title = {{L}earning encoding and decoding filters for data representation with a spiking neuron},
  author = {Gutmann, M. and Hyv{\"a}rinen, A. and Aihara, K.},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  year = {2008},
  doi = {10.1109/IJCNN.2008.4633797},
  keywords = {spiking neurons, data representation},
  url = {https://doi.org/10.1109/IJCNN.2008.4633797}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Hyvarinen2005">
  
    <span class="title">Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in V2</span>
    <span class="author">
      
        
          
            A. Hyvärinen,
          
        
      
        
          
            <em>M. Gutmann</em>,
          
        
      
        
          and
          
            P. Hoyer
          
        
      
    </span>

    <span class="periodical">
    
      <em>BMC Neuroscience</em>
    
    
      2005
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Hyvarinen2005.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-6-12" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Background
It has been shown that the classical receptive fields of simple and complex cells in the primary visual cortex emerge from the statistical properties of natural images by forcing the cell responses to be maximally sparse or independent. We investigate how to learn features beyond the primary visual cortex from the statistical properties of modelled complex-cell outputs. In previous work, we showed that a new model, non-negative sparse coding, led to the emergence of features which code for contours of a given spatial frequency band.

Results
We applied ordinary independent component analysis to modelled outputs of complex cells that span different frequency bands. The analysis led to the emergence of features which pool spatially coherent across-frequency activity in the modelled primary visualcortex. Thus, the statistically optimal way of processing complex-cell outputs abandons separate frequency channels, while preserving and even enhancing orientation tuning and spatial localization. As a technical aside, we found that the non-negativity constraint is not necessary: ordinary independent component analysis produces essentially the same results as our previous work.

Conclusion
We propose that the pooling that emerges allows the features to code for realistic low-level image features related to step edges. Further, the results prove the viability of statistical modelling of natural images as a framework that produces quantitative predictions of visual processing.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Hyvarinen2005,
  title = {{S}tatistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in {V}2},
  author = {Hyv{\"a}rinen, A. and Gutmann, M. and Hoyer, P.O.},
  journal = {BMC Neuroscience},
  year = {2005},
  volume = {6:12},
  doi = {https://doi.org/10.1186/1471-2202-6-12},
  keywords = {Natural image statistics, computational neuroscience, V2},
  url = {https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-6-12}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Hyvarinen2005b">
  
    <span class="title">Statistical models of images and early vision</span>
    <span class="author">
      
        
          
            A. Hyvärinen,
          
        
      
        
          
            P. Hoyer,
          
        
      
        
          
            J. Hurri,
          
        
      
        
          and
          
            <em> M. Gutmann</em>
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR)</em>
    
    
      2005
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Hyvarinen2005b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.3281" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>A fundamental question in visual neuroscience is: Why are the receptive fields and response properties of visual neurons as they are? A modern approach to this problem emphasizes the importance of adaptation to ecologically valid input. In this paper, we will review work on modelling statistical regularities in ecologically valid visual input ("natural images") and the obtained functional explanation of the properties of visual neurons. A seminal statistical model for natural images was linear sparse coding which is equivalent to the model called independent component analysis (ICA). Linear features estimated by ICA resemble wavelets or Gabor functions, and provide a very good description of the properties of simple cells in the primary visual cortex. We have introduced extensions of ICA that are based on modelling dependencies of the "independent" components estimated by basic ICA. The dependencies of the components are used to define either a grouping or a topographic order between the components. With natural image data, these models lead to emergence of further properties of visual neurons: the topographic organization and complex cell receptive fields. We have also modelled the temporal structure of natural image sequences, which provides an alternative approach to the sparseness used in most models. These models can be combined in a unifying framework that we call bubble coding. Finally, we will discuss a promising new direction of research: predictive visual neuroscience. There, the goal is to try to predict response properties of neurons in areas that are poorly understood, still based on statistical modelling of natural input.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Hyvarinen2005b,
  title = {{S}tatistical models of images and early vision},
  author = {Hyv\"arinen, A. and Hoyer, P.O. and Hurri, J. and Gutmann, M.},
  booktitle = {Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR)},
  year = {2005},
  keywords = {natural image statistics, computational neuroscience},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.3281}
}
</pre>
  </span>
  
  
</div>


</li></ol>

  </article>
  
  

  

</div>

      </div>
    </div>

    
<footer>

  <div class="wrapper">
    
        Last updated: 23 September 2017.
    
    Powered by <a href="http://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> and <a href="https://github.com/inukshuk/jekyll-scholar/">Jekyll-Scholar</a>.

    &copy; Copyright 2017 Michael U. Gutmann.
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://michaelgutmann.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="https://michaelgutmann.github.io/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/academicons.min.css">


<!-- Load Mathjax -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
