<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Michael U. Gutmann | Publications</title>
  <meta name="description" content="Research homepage of Michael U. Gutmann
">

<!--  <link rel="shortcut icon" href="https://michaelgutmann.github.io/assets/img/favicon.ico"> -->

<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/main.css">
  <link rel="canonical" href="https://michaelgutmann.github.io/publications/">

  <!-- added roboto font family -->
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <link rel="stylesheet"  href="https://michaelgutmann.github.io/assets/css/w3.css">
</head>


  <body>

    <header class="site-header">
  <div class="wrapper">
  
    <div class="w3-bar w3-padding-small">
        
      
      <a class="w3-bar-item w3-button w3-hover-none w3-border-white w3-hover-text-theme-color w3-mobile"  href="https://michaelgutmann.github.io/">  <strong>Michael</strong> U. Gutmann </a>
      
      
      
      
      
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/contact/">contact</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/teaching/">teaching</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/code/">code</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/talks/">talks</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-right w3-hover-none w3-border-white w3-hover-text-theme-color w3-hide-small w3-mobile" href="https://michaelgutmann.github.io/publications/">publications</a> 
      
      
      
      
      <a href="javascript:void(0)" class="w3-bar-item w3-hover-none w3-border-white  w3-button w3-hide-large w3-hide-medium w3-mobile" onclick="myFunction()">&#9776;</a>
    </div>
  </div>

   <div id="mobile" class="w3-bar-block w3-hide w3-hide-large w3-hide-medium">  
      
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/publications/">publications</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/talks/">talks</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/code/">code</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/teaching/">teaching</a> 
      
      
      
      <a class="w3-bar-item w3-button w3-mobile" href="https://michaelgutmann.github.io/contact/">contact</a> 
      
      
      
      
      
      
    </div>
  </div>


  <script>
    function myFunction() {
    var x = document.getElementById("mobile");
    if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
    } else { 
    x.className = x.className.replace(" w3-show", "");
    }
    }
  </script>
  
</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
      
    <h1 class="post-title">Publications</h1>
    
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Publications clearfix">
    
<h3 id="manuscripts">Manuscripts</h3>
<ol class="bibliography"><li>
  





<div id="Kleinegesse2020a">
  
    <span class="title">Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation</span>
    <span class="author">
      
      

      
            
            S. Kleinegesse,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>arXiv:2002.08129</i>
    
    
      2020
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/2002.08129" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Kleinegesse2020a,
  title = {Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation},
  author = {Kleinegesse, S. and Gutmann, M.U.},
  journal = {arXiv:2002.08129},
  year = {2020},
  arxiv = {https://arxiv.org/abs/2002.08129}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Simola2019">
  
    <span class="title">Adaptive Approximate Bayesian Computation Tolerance Selection</span>
    <span class="author">
      
      

      
            
            U. Simola,
            
            
      
      

      
            
            J. Cisewski-Kehe,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>arXiv:1907.01505</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/1907.01505" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Approximate Bayesian Computation (ABC) methods are increasingly used for inference in situations in which the likelihood function is either computationally costly or intractable to evaluate. Extensions of the basic ABC rejection algorithm have improved the computational efficiency of the procedure and broadened its applicability. The ABC-Population Monte Carlo (ABC-PMC) approach of Beaumont et al. (2009) has become a popular choice for approximate sampling from the posterior. ABC-PMC is a sequential sampler with an iteratively decreasing value of the tolerance, which specifies how close the simulated data need to be to the real data for acceptance. We propose a method for adaptively selecting a sequence of tolerances that improves the computational efficiency of the algorithm over other common techniques. In addition we define a stopping rule as a by-product of the adaptation procedure, which assists in automating termination of sampling. The proposed automatic ABC-PMC algorithm can be easily implemented and we present several examples demonstrating its benefits in terms of computational efficiency.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Simola2019,
  title = {Adaptive Approximate Bayesian Computation Tolerance Selection},
  author = {Simola, Umberto and Cisewski-Kehe, Jessica and Gutmann, Michael U. and Corander, Jukka},
  journal = {arXiv:1907.01505},
  year = {2019},
  arxiv = {https://arxiv.org/abs/1907.01505}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Dinev2018">
  
    <span class="title">Dynamic Likelihood-free Inference via Ratio Estimation (DIRE)</span>
    <span class="author">
      
      

      
            
            T. Dinev,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>arXiv:1810.09899</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/1810.09899" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Parametric statistical models that are implicitly defined in terms of a stochastic data generating process are used in a wide range of scientific disciplines because they enable accurate modeling. However, learning the parameters from observed data is generally very difficult because their likelihood function is typically intractable. Likelihood-free Bayesian inference methods have been proposed which include the frameworks of approximate Bayesian computation (ABC), synthetic likelihood, and its recent generalization that performs likelihood-free inference by ratio estimation (LFIRE). A major difficulty in all these methods is choosing summary statistics that reduce the dimensionality of the data to facilitate inference. While several methods for choosing summary statistics have been proposed for ABC, the literature for synthetic likelihood and LFIRE is very thin to date. We here address this gap in the literature, focusing on the important special case of time-series models. We show that convolutional neural networks trained to predict the input parameters from the data provide suitable summary statistics for LFIRE. On a wide range of time-series models, a single neural network architecture produced equally or more accurate posteriors than alternative methods.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Dinev2018,
  title = {Dynamic Likelihood-free Inference via Ratio Estimation ({DIRE})},
  author = {Dinev, Traiko and Gutmann, M.U.},
  journal = {arXiv:1810.09899},
  year = {2018},
  arxiv = {https://arxiv.org/abs/1810.09899},
  keywords = {likelihood-free inference, approximate Bayesian computation, time-series}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Neyton2018">
  
    <span class="title">Multiomic definition of generalizable endotypes in human acute pancreatitis</span>
    <span class="author">
      
      

      
            
            L. Neyton,
            
            
      
      

      
            
            X. Zheng,
            
            
      
      

      
            
            C. Skouras,
            
            
      
      

      
            
            A. Doeschl-Wilson,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            C. Yau,
            
            
      
      

      
            
            C. Ponting,
            
            
      
      

      
            
            I. Uings,
            
            
      
      

      
            
            F. Rao,
            
            
      
      

      
            
            A. Nicolas,
            
            
      
      

      
            
            C. Marshall,
            
            
      
      

      
            
            L. Wilson,
            
            
      
      

      
            
            . APPreSci Consortium,
            
            
      
      

      
            
            J. Baillie,
            
            
      
      
            and

            
            D. Mole
        
      
    </span>

    <span class="periodical">
    
      <i>bioRxiv 539569</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://doi.org/10.1101/539569" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Neyton2018,
  title = {Multiomic definition of generalizable endotypes in human acute pancreatitis},
  author = {Neyton, L.P.A. and Zheng, X. and Skouras, C. and Doeschl-Wilson, A. and Gutmann, M.U. and Yau, C. and Ponting, C. and Uings, I. and Rao, F. and Nicolas, A. and Marshall, C. and Wilson, L.-M. and the APPreSci Consortium and Baillie, J.K. and Mole, D.J.},
  journal = {bioRxiv 539569},
  year = {2018},
  arxiv = {https://doi.org/10.1101/539569}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Thomas2016">
  
    <span class="title">Likelihood-Free Inference by Ratio Estimation</span>
    <span class="author">
      
      

      
            
            O. Thomas,
            
            
      
      

      
            
            R. Dutta,
            
            
      
      

      
            
            J. Corander,
            
            
      
      

      
            
            S. Kaski,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>arXiv:1611.10242</i>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/1611.10242" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of "closeness" is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on toy problems and use it to perform inference for stochastic nonlinear dynamical systems.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Thomas2016,
  title = {Likelihood-Free Inference by Ratio Estimation},
  author = {Thomas, O. and Dutta, R. and Corander, J. and Kaski, S. and Gutmann, M.U.},
  journal = {arXiv:1611.10242},
  year = {2016},
  arxiv = {https://arxiv.org/abs/1611.10242},
  keywords = {approximate Bayesian computation, synthetic likelihood, summary statistics selection, density-ratio estimation; logistic regression; probabilistic classification, stochastic dynamical systems}
}
</pre>
  </span>
  
  
</div>


</li></ol>

<h3 id="refereed-papers">Refereed papers</h3>
<ol class="bibliography"><li>
  





<div id="Li2020">
  
    <span class="title">Genome-wide CRISPR screen Identifies Host Dependency Factors for Influenza A Virus Infection</span>
    <span class="author">
      
      

      
            
            B. Li,
            
            
      
      

      
            
            S. Clohisey,
            
            
      
      

      
            
            B. Chia,
            
            
      
      

      
            
            B. Wang,
            
            
      
      

      
            
            A. Cui,
            
            
      
      

      
            
            T. Eisenhaure,
            
            
      
      

      
            
            L. Schweitzer,
            
            
      
      

      
            
            P. Hoover,
            
            
      
      

      
            
            N. Parkinson,
            
            
      
      

      
            
            A. Nachshon,
            
            
      
      

      
            
            N. Smith,
            
            
      
      

      
            
            T. Regan,
            
            
      
      

      
            
            D. Farr,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            S. Bukhari,
            
            
      
      

      
            
            A. Law,
            
            
      
      

      
            
            M. Sangesland,
            
            
      
      

      
            
            I. Gat-viks,
            
            
      
      

      
            
            P. Digard,
            
            
      
      

      
            
            S. Vasudevan,
            
            
      
      

      
            
            D. Lingwood,
            
            
      
      

      
            
            D. Dockrell,
            
            
      
      

      
            
            J. Doench,
            
            
      
      

      
            
            J. Baillie,
            
            
      
      
            and

            
            N. Hacohen
        
      
    </span>

    <span class="periodical">
    
      <i>Nature Communications</i>
    
    
      2020
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Li2020.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1038/s41467-019-13965-x" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Host dependency factors that are required for influenza A virus infection may serve as therapeutic targets as the virus is less likely to bypass them under drug-mediated selection pressure. Previous attempts to identify host factors have produced largely divergent results, with few overlapping hits across different studies. Here, we perform a genome-wide CRISPR/Cas9 screen and devise a new approach, meta-analysis by information content (MAIC) to systematically combine our results with prior evidence for influenza host factors. MAIC out33 performs other meta-analysis methods when using our CRISPR screen as validation data. We validate the host factors, WDR7, CCDC115 and TMEM199, demonstrating that these genes are essential for viral entry and regulation of V-type ATPase assembly. We also find that CMTR1, a human mRNA cap methyltransferase, is required for efficient viral cap snatching and regulation of a cell autonomous immune response, and provides synergistic protection with the influenza endonuclease inhibitor Xofluza.
Host dependency factors that are required for influenza A virus infection may serve as therapeutic targets as the virus is less likely to bypass them under drug-mediated selection pressure. Previous attempts to identify host factors have produced largely divergent results, with few overlapping hits across different studies. Here, we perform a genome-wide CRISPR/Cas9 screen and devise a new approach, meta-analysis by information content (MAIC) to systematically combine our results with prior evidence for influenza host factors. MAIC out33 performs other meta-analysis methods when using our CRISPR screen as validation data. We validate the host factors, WDR7, CCDC115 and TMEM199, demonstrating that these genes are essential for viral entry and regulation of V-type ATPase assembly. We also find that CMTR1, a human mRNA cap methyltransferase, is required for efficient viral cap snatching and regulation of a cell autonomous immune response, and provides synergistic protection with the influenza endonuclease inhibitor Xofluza.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Li2020,
  title = {Genome-wide CRISPR screen Identifies Host Dependency Factors for Influenza A Virus Infection},
  author = {Li, Bo and Clohisey, Sara M. and Chia, Bing Shao and Wang, Bo and Cui, Ang and Eisenhaure, Thomas and Schweitzer, Lawrence D. and Hoover, Paul and Parkinson, Nicholas J. and Nachshon, Aharon and Smith, Nikki and Regan, Tim and Farr, David and Gutmann, Michael U. and Bukhari, Syed Irfan and Law, Andrew and Sangesland, Maya and Gat-viks, Irit and Digard, Paul and Vasudevan, Shobha and Lingwood, Daniel and Dockrell, David H. and Doench, John G. and Baillie, J. Kenneth and Hacohen, Nir},
  journal = {Nature Communications},
  year = {2020},
  month = jan,
  number = {1},
  pages = {164--},
  volume = {11},
  doi = {https://doi.org/10.1038/s41467-019-13965-x},
  issn = {2041-1723},
  url = {https://doi.org/10.1038/s41467-019-13965-x},
  month_numeric = {1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Ikonomov2020">
  
    <span class="title">Robust Optimisation Monte Carlo</span>
    <span class="author">
      
      

      
            
            B. Ikonomov,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2020
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Ikonomov2020.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/1904.00670" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>This paper is on Bayesian inference for parametric statistical models that are implicitly defined by a stochastic simulator which specifies how data is generated. While exact sampling is possible, evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate a previously unrecognised important failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant posterior density into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as part of OMC or entirely as post-processing. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Ikonomov2020,
  title = {Robust Optimisation {M}onte {C}arlo},
  author = {Ikonomov, B. and Gutmann, M.U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2020},
  arxiv = {https://arxiv.org/abs/1904.00670}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Jarvenpaa2020">
  
    <span class="title">Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations</span>
    <span class="author">
      
      

      
            
            M. Järvenpää,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            A. Vehtari,
            
            
      
      
            and

            
            P. Marttinen
        
      
    </span>

    <span class="periodical">
    
      <i>Bayesian Analysis</i>
    
    
      2020
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    [<a href="https://arxiv.org/abs/1905.01252" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We consider Bayesian inference when only a limited number of noisy log-likelihood evaluations can be obtained. This occurs for example when complex simulator-based statistical models are fitted to data, and synthetic likelihood (SL) method is used to form the noisy log-likelihood estimates using computationally costly forward simulations. We frame the inference task as a sequential Bayesian experimental design problem, where the log-likelihood function is modelled with a hierarchical Gaussian process (GP) surrogate model, which is used to efficiently select additional log-likelihood evaluation locations. Motivated by recent progress in the related problem of batch Bayesian optimisation, we develop various batch-sequential design strategies which allow to run some of the potentially costly simulations in parallel. We analyse the properties of the resulting method theoretically and empirically. Experiments with several toy problems and simulation models suggest that our method is robust, highly parallelisable, and sample-efficient.
We consider Bayesian inference when only a limited number of noisy log-likelihood evaluations can be obtained. This occurs for example when complex simulator-based statistical models are fitted to data, and synthetic likelihood (SL) method is used to form the noisy log-likelihood estimates using computationally costly forward simulations. We frame the inference task as a sequential Bayesian experimental design problem, where the log-likelihood function is modelled with a hierarchical Gaussian process (GP) surrogate model, which is used to efficiently select additional log-likelihood evaluation locations. Motivated by recent progress in the related problem of batch Bayesian optimisation, we develop various batch-sequential design strategies which allow to run some of the potentially costly simulations in parallel. We analyse the properties of the resulting method theoretically and empirically. Experiments with several toy problems and simulation models suggest that our method is robust, highly parallelisable, and sample-efficient.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Jarvenpaa2020,
  title = {Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations},
  author = {J\"arvenp\"a\"a, Marko and Gutmann, Michael U. and Vehtari, Aki and Marttinen, Pekka},
  journal = {Bayesian Analysis},
  year = {2020},
  arxiv = {https://arxiv.org/abs/1905.01252}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Srivastava2020">
  
    <span class="title">Generative Ratio Matching Networks</span>
    <span class="author">
      
      

      
            
            A. Srivastava,
            
            
      
      

      
            
            K. Xu,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            C. Sutton
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Learning Representations (ICLR)</i>
    
    
      2020
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Srivastava2020.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://openreview.net/forum?id=SJg7spEYDS" target="_blank">url</a>]
    
    
    [<a href="https://arxiv.org/abs/1806.00101" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Deep generative models can learn to generate realistic-looking images on several natural image datasets, but many of the most effective methods are adversarial methods, which require careful balancing of training between a generator network and a discriminator network. Maximum mean discrepancy networks (MMD-nets) avoid this issue using the kernel trick, but unfortunately they have not on their own been able to match the performance of adversarial training. We present a new method of training MMD-nets, based on learning a mapping of samples from the data and from the model into a lower dimensional space, in which MMD training can be more effective. We call these networks ratio matching MMD networks (RMMMDnets). We train the mapping to preserve density ratios between the densities over the low-dimensional space and the original space. This ensures that matching the model distribution to the data in the low-dimensional space will also match the original distributions. We show that RM-MMDnets have better performance and better stability than recent adversarial methods for training MMD-nets.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Srivastava2020,
  title = {Generative Ratio Matching Networks},
  author = {Srivastava, Akash and Xu, Kai and Gutmann, M.U. and Sutton, Charles},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year = {2020},
  arxiv = {https://arxiv.org/abs/1806.00101},
  url = {https://openreview.net/forum?id=SJg7spEYDS}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Chen2019">
  
    <span class="title">Adaptive Gaussian Copula ABC</span>
    <span class="author">
      
      

      
            
            Y. Chen,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Chen2019.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Chen2019.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v89/chen19d" target="_blank">url</a>]
    
    
    [<a href="https://arxiv.org/abs/1902.10704" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches — regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that our method is fast, accurate, and easy to use.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Chen2019,
  title = {Adaptive {G}aussian Copula {ABC}},
  author = {Chen, Y. and Gutmann, M.U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2019},
  arxiv = {https://arxiv.org/abs/1902.10704},
  url = {http://proceedings.mlr.press/v89/chen19d}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Kleinegesse2019">
  
    <span class="title">Efficient Bayesian Experimental Design for Implicit Models</span>
    <span class="author">
      
      

      
            
            S. Kleinegesse,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Kleinegesse2019.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v89/kleinegesse19a" target="_blank">url</a>]
    
    
    [<a href="https://arxiv.org/abs/1810.09912" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search. We find that this increases efficiency and allows us to consider higher design dimensions.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Kleinegesse2019,
  title = {Efficient {B}ayesian Experimental Design for Implicit Models},
  author = {Kleinegesse, S. and Gutmann, M.U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2019},
  arxiv = {https://arxiv.org/abs/1810.09912},
  url = {http://proceedings.mlr.press/v89/kleinegesse19a}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2019a">
  
    <span class="title">Resolving outbreak dynamics using approximate Bayesian computation for stochastic birth-death models</span>
    <span class="author">
      
      

      
            
            J. Lintusaari,
            
            
      
      

      
            
            P. Blomstedt,
            
            
      
      

      
            
            B. Rose,
            
            
      
      

      
            
            T. Sivula,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            S. Kaski,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Wellcome Open Research</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2019a.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://wellcomeopenresearch.org/articles/4-14/v2" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2019a,
  title = {Resolving outbreak dynamics using approximate {B}ayesian computation for stochastic birth-death models},
  author = {Lintusaari, J and Blomstedt, P and Rose, B and Sivula, T and Gutmann, MU and Kaski, S and Corander, J},
  journal = {Wellcome Open Research},
  year = {2019},
  number = {14},
  volume = {4},
  doi = {10.12688/wellcomeopenres.15048.2},
  url = {https://wellcomeopenresearch.org/articles/4-14/v2}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Rhodes2019">
  
    <span class="title">Variational Noise-Contrastive Estimation</span>
    <span class="author">
      
      

      
            
            B. Rhodes,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Rhodes2019.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Rhodes2019.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v89/rhodes19a" target="_blank">url</a>]
    
    
    [<a href="https://arxiv.org/abs/1810.08010" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Rhodes2019,
  title = {Variational Noise-Contrastive Estimation},
  author = {Rhodes, B. and Gutmann, M.U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2019},
  arxiv = {https://arxiv.org/abs/1810.08010},
  keywords = {noise-contrastive estimation, latent variables},
  url = {http://proceedings.mlr.press/v89/rhodes19a}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Todorovic2019">
  
    <span class="title">Bayesian inference of atomistic structure in functional materials</span>
    <span class="author">
      
      

      
            
            M. Todorović,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            J. Corander,
            
            
      
      
            and

            
            P. Rinke
        
      
    </span>

    <span class="periodical">
    
      <i>npj Computational Materials</i>
    
    
      2019
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Todorovic2019.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://www.nature.com/articles/s41524-019-0175-2" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Tailoring the functional properties of advanced organic/inorganic heterogeneous devices to their intended technological applications requires knowledge and control of the microscopic structure inside the device. Atomistic quantum mechanical simulation methods deliver accurate energies and properties for individual configurations, however, finding the most favourable configurations remains computationally prohibitive. We propose a "building block"-based Bayesian Optimisation Structure Search (BOSS) approach for addressing extended organic/inorganic interface problems and demonstrate its feasibility in a molecular surface adsorption study. In BOSS, a Bayesian model identifies material energy landscapes in an accelerated fashion from atomistic configurations sampled during active learning. This allowed us to identify several most favourable molecular adsorption configurations for C60 on the (101) surface of TiO2 anatase and clarify the key molecule-surface interactions governing structural assembly. Inferred structures were in good agreement with detailed experimental images of this surface adsorbate, demonstrating good predictive power of BOSS and opening the route towards large-scale surface adsorption studies of molecular aggregates and films.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Todorovic2019,
  title = {Bayesian inference of atomistic structure in functional materials},
  author = {Todorovi\'c, M. and Gutmann, M.U. and Corander, J. and Rinke, P.},
  journal = {npj Computational Materials},
  year = {2019},
  number = {1},
  pages = {35--},
  volume = {5},
  issn = {2057-3960},
  refid = {Todorovic2019},
  url = {https://www.nature.com/articles/s41524-019-0175-2}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Arnold2018">
  
    <span class="title">Weak Epistasis May Drive Adaptation in Recombining Bacteria</span>
    <span class="author">
      
      

      
            
            B. Arnold,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            Y. Grad,
            
            
      
      

      
            
            S. Sheppard,
            
            
      
      

      
            
            J. Corander,
            
            
      
      

      
            
            M. Lipsitch,
            
            
      
      
            and

            
            W. Hanage
        
      
    </span>

    <span class="periodical">
    
      <i>Genetics</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Arnold2018.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1534/genetics.117.300662" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The impact of epistasis on the evolution of multilocus traits depends on recombination. While sexually-reproducing eukaryotes recombine so frequently that epistasis between polymorphisms is not considered to play a large role in short-term adaptation, many bacteria also recombine, some to the degree that their populations are described as &#039;panmictic&#039; or &#039;freely recombining&#039;. However, whether this recombination is sufficient to limit the ability of selection to act on epistatic contributions to fitness is unknown. We quantify homologous recombination in five bacterial pathogens and use these parameter estimates in a multilocus model of bacterial evolution with additive and epistatic effects. We find that even for highly recombining species (e.g. Streptococcus pneumoniae or Helicobacter pylori), selection on weak interactions between distant mutations is nearly as efficient as for an asexual species, likely because homologous recombination typically transfers only short segments. However, for strong epistasis, bacterial recombination accelerates selection, with the dynamics dependent on the amount of recombination and the number of loci. Epistasis may thus play an important role in both the short- and long-term adaptive evolution of bacteria and, unlike in eukaryotes, is not limited to strong effect sizes, closely linked loci, or other conditions that limit the impact of recombination.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Arnold2018,
  title = {Weak Epistasis May Drive Adaptation in Recombining Bacteria},
  author = {Arnold, B.J. and Gutmann, M.U. and Grad, Y.H. and Sheppard, S.K. and Corander, J. and Lipsitch, M. and Hanage, W.P.},
  journal = {Genetics},
  year = {2018},
  month = jan,
  number = {3},
  pages = {1247--1260},
  volume = {208},
  url = {https://doi.org/10.1534/genetics.117.300662},
  month_numeric = {1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Ceylan2018">
  
    <span class="title">Conditional Noise-Contrastive Estimation of Unnormalised Models</span>
    <span class="author">
      
      

      
            
            C. Ceylan,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the 35th International Conference on Machine Learning (ICML)</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Ceylan2018.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Ceylan2018.supp.pdf" target="_blank">supp</a>]
    
    
    [<a href="/assets/papers/Ceylan2018.merged.pdf" target="_blank">merged</a>]
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v80/ceylan18a.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Ceylan2018,
  title = {Conditional Noise-Contrastive Estimation of Unnormalised Models},
  author = {Ceylan, C. and Gutmann, M.U},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  year = {2018},
  editor = {Dy, Jennifer and Krause, Andreas},
  pages = {725--733},
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  keywords = {unnormalised models, noise-contrastive estimation},
  pdf = {http://proceedings.mlr.press/v80/ceylan18a/ceylan18a.pdf},
  url = {http://proceedings.mlr.press/v80/ceylan18a.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2018">
  
    <span class="title">Likelihood-free inference via classification</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            R. Dutta,
            
            
      
      

      
            
            S. Kaski,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Statistics and Computing</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2018.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2018.supp.pdf" target="_blank">supp</a>]
    
    
    
    [<a href="/assets/papers/Gutmann2018.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1007/s11222-017-9738-6" target="_blank">url</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2018.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2018.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2018,
  title = {Likelihood-free inference via classification},
  author = {Gutmann, M.U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal = {Statistics and Computing},
  year = {2018},
  number = {2},
  pages = {411--425},
  volume = {28},
  day = {13},
  doi = {10.1007/s11222-017-9738-6},
  issn = {1573-1375},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {https://doi.org/10.1007/s11222-017-9738-6}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Jarvenpaa2018">
  
    <span class="title">Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria</span>
    <span class="author">
      
      

      
            
            M. Järvenpää,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            A. Vehtari,
            
            
      
      
            and

            
            P. Marttinen
        
      
    </span>

    <span class="periodical">
    
      <i>The Annals of Applied Statistics</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Jarvenpaa2018.pdf" target="_blank">pdf</a>]
    
    
    
    
    [<a href="/assets/papers/Jarvenpaa2018.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://projecteuclid.org/euclid.aoas/1542078043" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Approximate Bayesian computation (ABC) can be used for model fitting when the likelihood function is intractable but simulating from the model is feasible. However, even a single evaluation of a complex model may take several hours, limiting the number of model evaluations available. Modelling the discrepancy between the simulated and observed data using a Gaussian process (GP) can be used to reduce the number of model evaluations required by ABC, but the sensitivity of this approach to a specific GP formulation has not yet been thoroughly investigated. We begin with a comprehensive empirical evaluation of using GPs in ABC, including various transformations of the discrepancies and two novel GP formulations. Our results indicate the choice of GP may significantly affect the accuracy of the estimated posterior distribution. Selection of an appropriate GP model is thus important. We formulate expected utility to measure the accuracy of classifying discrepancies below or above the ABC threshold, and show that it can be used to automate the
GP model selection step. Finally, based on the understanding gained with toy examples, we fit a population genetic model for bacteria, providing insight into horizontal gene transfer events within the population and from external origins.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Jarvenpaa2018,
  title = {Gaussian process modeling in approximate {B}ayesian computation to estimate horizontal gene transfer in bacteria},
  author = {J\"arvenp\"a\"a, M. and Gutmann, M.U. and Vehtari, A. and Marttinen, P.},
  journal = {The Annals of Applied Statistics},
  year = {2018},
  number = {4},
  pages = {2228--2251},
  volume = {12},
  doi = {https://doi.org/10.1214/18-AOAS1150},
  keywords = {Approximation Bayesian computation, emulation, Gaussian processes, model selection},
  url = {https://projecteuclid.org/euclid.aoas/1542078043}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Jarvenpaa2018b">
  
    <span class="title">Efficient acquisition rules for model-based approximate Bayesian computation</span>
    <span class="author">
      
      

      
            
            M. Järvenpää,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            A. Vehtari,
            
            
      
      
            and

            
            P. Marttinen
        
      
    </span>

    <span class="periodical">
    
      <i>Bayesian Analysis</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    
    
    
    [<a href="/assets/papers/Jarvenpaa2018b.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1704.00520" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, surrogate models and Bayesian optimisation (BO) have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next, but standard BO strategies are designed for optimisation and not specifically for ABC inference. Our paper addresses this gap in the literature. We propose a new acquisition rule that selects the next evaluation where the uncertainty in the posterior distribution is largest. Experiments show that the proposed method often produces the most accurate approximations, especially in high-dimensional cases or in the presence of strong prior information, compared to common alternatives.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Jarvenpaa2018b,
  title = {Efficient acquisition rules for model-based approximate {B}ayesian computation},
  author = {J\"arvenp\"a\"a, M. and Gutmann, M.U. and Vehtari, A. and Marttinen, P.},
  journal = {Bayesian Analysis},
  year = {2018},
  volume = {in press},
  keywords = {Approximation Bayesian computation, Bayesian optimisation},
  url = {https://arxiv.org/abs/1704.00520}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2018">
  
    <span class="title">ELFI: Engine for Likelihood-Free Inference</span>
    <span class="author">
      
      

      
            
            J. Lintusaari,
            
            
      
      

      
            
            H. Vuollekoski,
            
            
      
      

      
            
            A. Kangasrääsiö,
            
            
      
      

      
            
            K. Skytén,
            
            
      
      

      
            
            M. Järvenpää,
            
            
      
      

      
            
            P. Marttinen,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            A. Vehtari,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Journal of Machine Learning Research</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2018.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://jmlr.org/papers/v19/17-374.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2018,
  title = {{ELFI}: Engine for Likelihood-Free Inference},
  author = {Lintusaari, Jarno and Vuollekoski, Henri and Kangasr\"a\"asi\"o, Antti and Skyt\'en, Kusti and J\"arvenp\"a\"a, Marko and Marttinen, Pekka and Gutmann, M.U. and Vehtari, Aki and Corander, Jukka},
  journal = {Journal of Machine Learning Research},
  year = {2018},
  number = {16},
  pages = {1--7},
  volume = {19},
  keywords = {Likelihood-free inference, approximate Bayesian computation, Python, software, parallel computing},
  url = {http://jmlr.org/papers/v19/17-374.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="LopezGuevara2018">
  
    <span class="title">To Stir or Not to Stir: Online Estimation of Liquid Properties for Pouring Actions</span>
    <span class="author">
      
      

      
            
            T. Lopez Guevara,
            
            
      
      

      
            
            R. Pucci,
            
            
      
      

      
            
            N. Taylor,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            S. Ramamoorthy,
            
            
      
      
            and

            
            K. Subr
        
      
    </span>

    <span class="periodical">
    
      <i>In Workshop on Learning and Inference in Robotics: Integrating Structure, Priors and Models</i>
    
    
      2018
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/LopezGuevara2018.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://drive.google.com/file/d/1qHdn03oZZe6aQiKxEyFTIiy_Y68OzQ1N/view" target="_blank">url</a>]
    
    
    [<a href="https://arxiv.org/abs/1904.02431" target="_blank">arxiv</a>]
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Our brains are able to exploit coarse physical models of fluids to solve complex manipulation tasks. There has been considerable interest in developing such a capability in robots so that they can autonomously manipulate fluids adapting to different conditions. In this paper, we investigate the problem of adaptation to liquids with different characteristics. We develop a simple training task (stirring with a stick) that enables rapid inference of the parameters of the liquid with relatively inexpensive measurement equipment (standard webcams) that robots may be assumed to have access to in the wild. We perform the inference in the space of simulation parameters rather than on physically accurate parameters. This facilitates prediction and optimization tasks since the inferred parameters may be fed directly to the simulator. First, we +demonstrate that our âstirringâ learner performs better than when the robot is trained with pouring actions. Further, we show that our method is able to infer properties of three very different liquids â water, glycerin and gel â and improve spillage with increased training. We present various experimental results performed by executing stirring and pouring actions on a UR10. We believe that this decoupling of the training actions from the goal task is a significant first step towards simple and autonomous learning of the behavior of different fluids in unstructured environments.
Our brains are able to exploit coarse physical models of fluids to solve complex manipulation tasks. There has been considerable interest in developing such a capability in robots so that they can autonomously manipulate fluids adapting to different conditions. In this paper, we investigate the problem of adaptation to liquids with different characteristics. We develop a simple training task (stirring with a stick) that enables rapid inference of the parameters of the liquid with relatively inexpensive measurement equipment (standard webcams) that robots may be assumed to have access to in the wild. We perform the inference in the space of simulation parameters rather than on physically accurate parameters. This facilitates prediction and optimization tasks since the inferred parameters may be fed directly to the simulator. First, we +demonstrate that our âstirringâ learner performs better than when the robot is trained with pouring actions. Further, we show that our method is able to infer properties of three very different liquids â water, glycerin and gel â and improve spillage with increased training. We present various experimental results performed by executing stirring and pouring actions on a UR10. We believe that this decoupling of the training actions from the goal task is a significant first step towards simple and autonomous learning of the behavior of different fluids in unstructured environments.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{LopezGuevara2018,
  title = {To Stir or Not to Stir: Online Estimation of Liquid Properties for Pouring Actions},
  author = {Lopez Guevara, Tatiana and Pucci, Rita and Taylor, Nicholas and Gutmann, Michael U. and Ramamoorthy, Subramanian and Subr, Kartic},
  booktitle = {Workshop on Learning and Inference in Robotics: Integrating Structure, Priors and Models},
  year = {2018},
  arxiv = {https://arxiv.org/abs/1904.02431},
  url = {https://drive.google.com/file/d/1qHdn03oZZe6aQiKxEyFTIiy_Y68OzQ1N/view}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="LopezGuevara2017a">
  
    <span class="title">Adaptable Pouring: Teaching Robots Not to Spill using Fast but Approximate Fluid Simulation</span>
    <span class="author">
      
      

      
            
            T. Lopez-Guevara,
            
            
      
      

      
            
            N. Taylor,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            S. Ramamoorthy,
            
            
      
      
            and

            
            K. Subr
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the 1st Annual Conference on Robot Learning (CoRL)</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/LopezGuevara2017a.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v78/lopez-guevara17a.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Humans manipulate fluids intuitively using intuitive approximations of the underlying physical model. In this paper, we explore a general methodology that robots may use to develop and improve strategies for overcoming manipulation tasks associated with appropriately defined loss functions. We focus on the specific task of pouring a liquid from a container (pourer) to another container (receiver) while minimizing the mass of liquid that spills outside the receiver. We present a solution, based on guidance from approximate simulation, that is fast, flexible and adaptable to novel containers as long as their shapes can be sensed. Our key idea is to decouple the optimization of the parameter space of the simulator from the optimization over action space for determining robot control actions. We perform the former in a training (calibration) stage and the latter during run-time (deployment). For the purpose of this paper we use pouring in both stages, even though separate actions could be chosen. We compare four different strategies for calibration and three different strategies for deployment. Our results demonstrate that fast fluid simulations are effective, even if they are only approximate, in guiding automatic strategies for pouring liquids.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{LopezGuevara2017a,
  title = {Adaptable Pouring: Teaching Robots Not to Spill using Fast but Approximate Fluid Simulation},
  author = {Lopez-Guevara, T. and Taylor, N.K. and Gutmann, M.U. and Ramamoorthy, S. and Subr, K.},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning (CoRL)},
  year = {2017},
  editor = {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  month = nov,
  pages = {77--86},
  series = {Proceedings of Machine Learning Research},
  volume = {78},
  pdf = {http://proceedings.mlr.press/v78/lopez-guevara17a/lopez-guevara17a.pdf},
  url = {http://proceedings.mlr.press/v78/lopez-guevara17a.html},
  month_numeric = {11}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2017">
  
    <span class="title">Fundamentals and Recent Developments in Approximate Bayesian Computation</span>
    <span class="author">
      
      

      
            
            J. Lintusaari,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            R. Dutta,
            
            
      
      

      
            
            S. Kaski,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Systematic Biology</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1093/sysbio/syw077" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Lintusaari2017.slides.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Lintusaari2017.slides1.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible. We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2017,
  title = {Fundamentals and Recent Developments in Approximate {B}ayesian Computation},
  author = {Lintusaari, J. and Gutmann, M.U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal = {Systematic Biology},
  year = {2017},
  month = jan,
  number = {1},
  pages = {e66--e82},
  volume = {66},
  doi = {10.1093/sysbio/syw077},
  issn = {1063-5157},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {http://dx.doi.org/10.1093/sysbio/syw077},
  month_numeric = {1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Corander2017">
  
    <span class="title">Frequency-dependent selection in vaccine-associated pneumococcal population dynamics</span>
    <span class="author">
      
      

      
            
            J. Corander,
            
            
      
      

      
            
            C. Fraser,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            B. Arnold,
            
            
      
      

      
            
            W. Hanage,
            
            
      
      

      
            
            S. Bentley,
            
            
      
      

      
            
            M. Lipsitch,
            
            
      
      
            and

            
            N. Croucher
        
      
    </span>

    <span class="periodical">
    
      <i>Nature Ecology &amp; Evolution</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1038/s41559-017-0337-x" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many bacterial species are composed of multiple lineages distinguished by extensive variation in gene content. These often cocirculate in the same habitat, but the evolutionary and ecological processes that shape these complex populations are poorly understood. Addressing these questions is particularly important for Streptococcus pneumoniae, a nasopharyngeal commensal and respiratory pathogen, because the changes in population structure associated with the recent introduction of partial-coverage vaccines have substantially reduced pneumococcal disease. Here we show that pneumococcal lineages from multiple populations each have a distinct combination of intermediate-frequency genes. Functional analysis suggested that these loci may be subject to negative frequency-dependent selection (NFDS) through interactions with other bacteria, hosts or mobile elements. Correspondingly, these genes had similar frequencies in four populations with dissimilar lineage compositions. These frequencies were maintained following substantial alterations in lineage prevalences once vaccination programmes began. Fitting a multilocus NFDS model of post-vaccine population dynamics to three genomic datasets using Approximate Bayesian Computation generated reproducible estimates of the influence of NFDS on pneumococcal evolution, the strength of which varied between loci. Simulations replicated the stable frequency of lineages unperturbed by vaccination, patterns of serotype switching and clonal replacement. This framework highlights how bacterial ecology affects the impact of clinical interventions.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Corander2017,
  title = {Frequency-dependent selection in vaccine-associated pneumococcal population dynamics},
  author = {Corander, J. and Fraser, C. and Gutmann, M.U. and Arnold, B. and Hanage, W.P. and Bentley, S.D. and Lipsitch, M. and Croucher, N.J.},
  journal = {Nature Ecology \&amp; Evolution},
  year = {2017},
  pages = {1950--1960},
  volume = {1},
  issn = {2397-334X},
  refid = {Corander2017},
  url = {https://doi.org/10.1038/s41559-017-0337-x}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2017">
  
    <span class="title">Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure</span>
    <span class="author">
      
      

      
            
            H. Sasaki,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            H. Shouno,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>Neural Computation</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1162/neco_a_01006" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they usually concentrated on higher-order correlations such as energy (square) correlations. Yet, linear correlations are a most fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods, so they can only be analyzed by developing new methods which explicitly allow for linearly correlated components. In this paper, we propose a probabilistic model of linear non-Gaussian components which are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parametrized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score matching (HyvÃ¤rinen, 2005), the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method improves identifiability of non-Gaussian components by simultaneously learning their correlation structure. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data show that the method finds new kinds of dependencies between the components.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Sasaki2017,
  title = {Simultaneous Estimation of Non-{G}aussian Components and their Correlation Structure},
  author = {Sasaki, H. and Gutmann, M.U. and Shouno, H. and Hyv\"arinen, A.},
  journal = {Neural Computation},
  year = {2017},
  number = {11},
  pages = {2887--2924},
  volume = {29},
  doi = {10.1162/neco_a_01006},
  keywords = {Extensions of ICA, speech data, natural images},
  url = {http://dx.doi.org/10.1162/neco_a_01006}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Srivastava2017">
  
    <span class="title">VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning</span>
    <span class="author">
      
      

      
            
            A. Srivastava,
            
            
      
      

      
            
            A. Valkov,
            
            
      
      

      
            
            C. Russell,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            C. Sutton
        
      
    </span>

    <span class="periodical">
    
      <i>In Advances in Neural Information Processing Systems 30 (NIPS)</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Srivastava2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1705.07761" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Srivastava2017,
  title = {{VEEGAN}: Reducing Mode Collapse in {GAN}s using Implicit Variational Learning},
  author = {Srivastava, A. and Valkov, A. and Russell, C. and Gutmann, M.U. and Sutton, C.},
  booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  url = {https://arxiv.org/abs/1705.07761}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Tietavainen2017">
  
    <span class="title">Bayesian inference of physiologically meaningful parameters from body sway measurements</span>
    <span class="author">
      
      

      
            
            A. Tietäväinen,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            E. Keski-Vakkuri,
            
            
      
      

      
            
            J. Corander,
            
            
      
      
            and

            
            E. Haeggström
        
      
    </span>

    <span class="periodical">
    
      <i>Scientific Reports</i>
    
    
      2017
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Tietavainen2017.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1038/s41598-017-02372-1" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The control of the human body sway by the central nervous system, muscles, and conscious brain is of interest since body sway carries information about the physiological status of a person. Several models have been proposed to describe body sway in an upright standing position, however, due to the statistical intractability of the more realistic models, no formal parameter inference has previously been conducted and the expressive power of such models for real human subjects remains unknown. Using the latest advances in Bayesian statistical inference for intractable models, we fitted a nonlinear control model to posturographic measurements, and we showed that it can accurately predict the sway characteristics of both simulated and real subjects. Our method provides a full statistical characterization of the uncertainty related to all model parameters as quantified by posterior probability density functions, which is useful for comparisons across subjects and test settings. The ability to infer intractable control models from sensor data opens new possibilities for monitoring and predicting body status in health applications.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Tietavainen2017,
  title = {Bayesian inference of physiologically meaningful parameters from body sway measurements},
  author = {Tiet\"av\"ainen, A. and Gutmann, M.U. and Keski-Vakkuri, E. and Corander, J. and Haeggstr\"om, E.},
  journal = {Scientific Reports},
  year = {2017},
  number = {3771},
  pages = {1--14},
  volume = {7},
  issn = {2045-2322},
  keywords = {Approximate Bayesian inference, nonlinear dynamical systems, balance and muscular control},
  url = {https://doi.org/10.1038/s41598-017-02372-1}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2016a">
  
    <span class="title">Bayesian optimization for likelihood-free inference of simulator-based statistical models</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Journal of Machine Learning Research</i>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2016a.pdf" target="_blank">pdf</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2016a.arxiv.pdf" target="_blank">arxiv</a>]
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://jmlr.org/papers/v17/15-017.html" target="_blank">url</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2016a.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2016a.slides2.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2016a.slides3.pdf" target="_blank">slides</a>]
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2016a,
  title = {Bayesian optimization for likelihood-free inference of simulator-based statistical models},
  author = {Gutmann, M.U. and Corander, J},
  journal = {Journal of Machine Learning Research},
  year = {2016},
  number = {125},
  pages = {1--47},
  volume = {17},
  keywords = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  url = {http://jmlr.org/papers/v17/15-017.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Lintusaari2016">
  
    <span class="title">On the identifiability of transmission dynamic models for infectious diseases</span>
    <span class="author">
      
      

      
            
            J. Lintusaari,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            S. Kaski,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Genetics</i>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Lintusaari2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a href="/assets/papers/Lintusaari2016.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1534/genetics.115.180034" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Understanding the transmission dynamics of infectious diseases is important for both biological research and public health applications. It has been widely demonstrated that statistical modeling provides a firm basis for inferring relevant epidemiological quantities from incidence and molecular data. However, the complexity of transmission dynamic models causes two challenges: Firstly, the likelihood function of the models is generally not computable and computationally intensive simulation-based inference methods need to be employed. Secondly, the model may not be fully identifiable from the available data. While the first difficulty can be tackled by computational and algorithmic advances, the second obstacle is more fundamental. Identifiability issues may lead to inferences which are more driven by the prior assumptions than the data themselves. We here consider a popular and relatively simple, yet analytically intractable model for the spread of tuberculosis based on classical IS6110 fingerprinting data. We report on the identifiability of the model, presenting also some methodological advances regarding the inference. Using likelihood approximations, it is shown that the reproductive value cannot be identified from the data available and that the posterior distributions obtained in previous work have likely been substantially dominated by the assumed prior distribution. Further, we show that the inferences are influenced by the assumed infectious population size which has generally been kept fixed in previous work. We demonstrate that the infectious population size can be inferred if the remaining epidemiological parameters are already known with sufficient precision.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Lintusaari2016,
  title = {On the identifiability of transmission dynamic models for infectious diseases},
  author = {Lintusaari, J. and Gutmann, M.U. and Kaski, S. and Corander, J.},
  journal = {Genetics},
  year = {2016},
  number = {3},
  pages = {911--918},
  volume = {202},
  keywords = {Intractable likelihoods, epidemiology of infectious disease},
  url = {https://doi.org/10.1534/genetics.115.180034}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Numminen2016">
  
    <span class="title">The impact of host metapopulation structure on the population genetics of colonizing bacteria</span>
    <span class="author">
      
      

      
            
            E. Numminen,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            M. Shubin,
            
            
      
      

      
            
            P. Marttinen,
            
            
      
      

      
            
            G. Méric,
            
            
      
      

      
            
            W. Schaik,
            
            
      
      

      
            
            T. Coque,
            
            
      
      

      
            
            F. Baquero,
            
            
      
      

      
            
            R. Willems,
            
            
      
      

      
            
            S. Sheppard,
            
            
      
      

      
            
            E. Feil,
            
            
      
      

      
            
            W. Hanage,
            
            
      
      
            and

            
            J. Corander
        
      
    </span>

    <span class="periodical">
    
      <i>Journal of Theoretical Biology</i>
    
    
      2016
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Numminen2016.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.sciencedirect.com/science/article/pii/S0022519316001156" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Many key bacterial pathogens are frequently carried asymptomatically, and the emergence and spread of these opportunistic pathogens can be driven, or mitigated, via demographic changes within the host population. These inter-host transmission dynamics combine with basic evolutionary parameters such as rates of mutation and recombination, population size and selection, to shape the genetic diversity within bacterial populations. Whilst many studies have focused on how molecular processes underpin bacterial population structure, the impact of host migration and the connectivity of the local populations has received far less attention. A stochastic neutral model incorporating heightened local transmission has been previously shown to fit closely with genetic data for several bacterial species. However, this model did not incorporate transmission limiting population stratification, nor the possibility of migration of strains between subpopulations, which we address here by presenting an extended model. We study the consequences of migration in terms of shared genetic variation and show by simulation that the previously used summary statistic, the allelic mismatch distribution, can be insensitive to even large changes in microepidemic and migration rates. Using likelihood-free inference with genotype network topological summaries we fit a simpler model to commensal and hospital samples from the common nosocomial pathogens Staphylococcus aureus, Staphylococcus epidermidis, Enterococcus faecalis and Enterococcus faecium. Only the hospital data for E. faecium display clearly marked deviations from the model predictions which may be attributable to its adaptation to the hospital environment.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Numminen2016,
  title = {The impact of host metapopulation structure on the population genetics of colonizing bacteria},
  author = {Numminen, E. and Gutmann, Michael and Shubin, Mikhail and Marttinen, Pekka and M\'eric, Guillaume and van Schaik, Willem and Coque, Teresa and Baquero, Fernando and Willems, R.J.L. and Sheppard, S.K. and Feil, E.J. and Hanage, W.P. and Corander, Jukka},
  journal = {Journal of Theoretical Biology},
  year = {2016},
  pages = {53--62},
  volume = {396},
  keywords = {Bacterial evolution, Genetic structure, Migration, Population dynamics},
  url = {http://www.sciencedirect.com/science/article/pii/S0022519316001156}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Marttinen2015">
  
    <span class="title">Recombination produces coherent bacterial species clusters in both core and accessory genomes</span>
    <span class="author">
      
      

      
            
            P. Marttinen,
            
            
      
      

      
            
            N. Croucher,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            J. Corander,
            
            
      
      
            and

            
            W. Hanage
        
      
    </span>

    <span class="periodical">
    
      <i>Microbial Genomics</i>
    
    
      2015
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Marttinen2015.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Marttinen2015.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1099/mgen.0.000038" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Background:
Population samples show bacterial genomes can be divided into a core of ubiquitous genes and accessory genes that are present in a fraction of isolates. The ecological significance of this variation in gene content remains unclear. However, microbiologists agree that a bacterial species should be ’genomically coherent’, even though there is no consensus on how this should be determined.

Results:
We use a parsimonious model combining diversification in both the core and accessory genome, including mutation, homologous recombination (HR) and horizontal gene transfer (HGT) introducing new loci, to produce a population of interacting clusters of strains with varying genome content. New loci introduced by HGT may then be transferred on by HR. The model fits well to a systematic population sample of 616 pneumococcal genomes, capturing the major features of the population structure with parameter values that agree well with empirical estimates.

Conclusions:
The model does not include explicit selection on individual genes, suggesting that crude comparisons of gene content may be a poor predictor of ecological function. We identify a clearly divergent subpopulation of pneumococci that are inconsistent with the model and may be considered genomically incoherent with the rest of the population. These strains have a distinct disease tropism and may be rationally defined as a separate species. We also find deviations from the model that may be explained by recent population bottlenecks or spatial structure.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Marttinen2015,
  title = {Recombination produces coherent bacterial species clusters in both core and accessory genomes},
  author = {Marttinen, P. and Croucher, N.J. and Gutmann, M. and Corander, J. and Hanage, W.P.},
  journal = {Microbial Genomics},
  year = {2015},
  number = {5},
  volume = {1},
  doi = {http://dx.doi.org/10.1099/mgen.0.000038},
  keywords = {core/accessory genome, evolution, computational modeling, speciation, recombination},
  url = {http://dx.doi.org/10.1099/mgen.0.000038}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2014">
  
    <span class="title">Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            V. Laparra,
            
            
      
      

      
            
            A. Hyvärinen,
            
            
      
      
            and

            
            J. Malo
        
      
    </span>

    <span class="periodical">
    
      <i>PLOS ONE</i>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2014.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1371%2Fjournal.pone.0086481" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Independent component and canonical correlation analysis are two general-purpose statistical methods with wide applicability. In neuroscience, independent component analysis of chromatic natural images explains the spatio-chromatic structure of primary cortical receptive fields in terms of properties of the visual environment. Canonical correlation analysis explains similarly chromatic adaptation to different illuminations. But, as we show in this paper, neither of the two methods generalizes well to explain both spatio-chromatic processing and adaptation at the same time. We propose a statistical method which combines the desirable properties of independent component and canonical correlation analysis: It finds independent components in each data set which, across the two data sets, are related to each other via linear or higher-order correlations. The new method is as widely applicable as canonical correlation analysis, and also to more than two data sets. We call it higher-order canonical correlation analysis. When applied to chromatic natural images, we found that it provides a single (unified) statistical framework which accounts for both spatio-chromatic processing and adaptation. Filters with spatio-chromatic tuning properties as in the primary visual cortex emerged and corresponding-colors psychophysics was reproduced reasonably well. We used the new method to make a theory-driven testable prediction on how the neural response to colored patterns should change when the illumination changes. We predict shifts in the responses which are comparable to the shifts reported for chromatic contrast habituation.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2014,
  title = {Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images},
  author = {Gutmann, M.U. and Laparra, Valero and Hyv\"arinen, Aapo and Malo, Jes\'us},
  journal = {PLOS ONE},
  year = {2014},
  number = {2},
  pages = {e86481},
  volume = {9},
  doi = {10.1371/journal.pone.0086481},
  keywords = {Generalization of canonical correlation analysis, data fusion, natural image statistics, spatio-chromatic adaptation},
  publisher = {Public Library of Science},
  url = {http://dx.doi.org/10.1371%2Fjournal.pone.0086481}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Liu2014">
  
    <span class="title">Direct learning of sparse changes in Markov networks by density ratio estimation</span>
    <span class="author">
      
      

      
            
            S. Liu,
            
            
      
      

      
            
            J. Quinn,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            T. Suzuki,
            
            
      
      
            and

            
            M. Sugiyama
        
      
    </span>

    <span class="periodical">
    
      <i>Neural Computation</i>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Liu2014.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1162/NECO_a_00589" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, a critical bottleneck of the naive approach, can be remarkably mitigated. We also give the dual formulation of the optimization problem, which further reduces the computation cost for large-scale Markov networks. Through experiments, we demonstrate the usefulness of our method.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Liu2014,
  title = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  author = {Liu, S. and Quinn, J.A. and Gutmann, M.U. and Suzuki, T. and Sugiyama, M.},
  journal = {Neural Computation},
  year = {2014},
  number = {6},
  pages = {1169--1197},
  volume = {26},
  doi = {10.1162/NECO_a_00589},
  keywords = {Undirected graphical models, density ratio estimation},
  url = {http://dx.doi.org/10.1162/NECO_a_00589}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2014">
  
    <span class="title">Estimating dependency structures for non-Gaussian components with linear and energy correlations</span>
    <span class="author">
      
      

      
            
            H. Sasaki,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            H. Shouno,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2014
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2014.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Sasaki2014.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v33/sasaki14.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Sasaki2014,
  title = {Estimating dependency structures for non-{G}aussian components with linear and energy correlations},
  author = {Sasaki, H. and Gutmann, M.U. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2014},
  keywords = {Extensions of ICA, natural image statistics},
  url = {http://proceedings.mlr.press/v33/sasaki14.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2013">
  
    <span class="title">A three-layer model of natural image statistics</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>Journal of Physiology-Paris</i>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2013.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.sciencedirect.com/science/article/pii/S0928425713000028" target="_blank">url</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2013.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2013.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>An important property of visual systems is to be simultaneously both selective to specific patterns found in the sensory input and invariant to possible variations. Selectivity and invariance (tolerance) are opposing requirements. It has been suggested that they could be joined by iterating a sequence of elementary selectivity and tolerance computations. It is, however, unknown what should be selected or tolerated at each level of the hierarchy. We approach this issue by learning the computations from natural images. We propose and estimate a probabilistic model of natural images that consists of three processing layers. Two natural image data sets are considered: image patches, and complete visual scenes downsampled to the size of small patches. For both data sets, we find that in the first two layers, simple and complex cell-like computations are performed. In the third layer, we mainly find selectivity to longer contours; for patch data, we further find some selectivity to texture, while for the downsampled complete scenes, some selectivity to curvature is observed.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2013,
  title = {A three-layer model of natural image statistics},
  author = {Gutmann, M.U. and Hyv\"arinen, A.},
  journal = {Journal of Physiology-Paris},
  year = {2013},
  number = {5},
  pages = {369--398},
  volume = {107},
  booktitle = {Special issue: Neural Coding and Natural Image Statistics},
  doi = {10.1016/j.jphysparis.2013.01.001},
  issn = {0928-4257},
  keywords = {Natural images, probabilistic modeling, visual processing, selectivity, invariance, sparse coding, unsupervised deep learning},
  url = {http://www.sciencedirect.com/science/article/pii/S0928425713000028}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2013b">
  
    <span class="title">Estimation of unnormalized statistical models without numerical integration</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the Workshop on Information Theoretic Methods in Science and Engineering</i>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2013b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.me.inf.kyushu-u.ac.jp/witmse2013/" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Parametric statistical models of continuous or discrete valued data are often not properly normalized, that is, they do not integrate or sum to unity. The normalization is essential for maximum likelihood estimation. While in principle, models can always be normalized by dividing them by their integral or sum (their partition function), this can in practice be extremely difficult. We have been developing methods for the estimation of unnormalized models which do not approximate the partition function using numerical integration. We review these methods, score matching and noise-contrastive estimation, point out extensions and connections both between them and methods by other authors, and discuss their pros and cons.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2013b,
  title = {Estimation of unnormalized statistical models without numerical integration},
  author = {Gutmann, M.U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Workshop on Information Theoretic Methods in Science and Engineering},
  year = {2013},
  keywords = {Unnormalized models, energy-based models, intractable likelihood},
  url = {http://www.me.inf.kyushu-u.ac.jp/witmse2013/}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Liu2013">
  
    <span class="title">Direct learning of sparse changes in Markov networks by density ratio estimation</span>
    <span class="author">
      
      

      
            
            S. Liu,
            
            
      
      

      
            
            J. Quinn,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            M. Sugiyama
        
      
    </span>

    <span class="periodical">
    
      <i>In Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</i>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Liu2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1007/978-3-642-40991-2_38" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, which is a critical computational bottleneck of the naive approach, can be remarkably mitigated. Through experiments on gene expression and Twitter data analysis, we demonstrate the usefulness of our method.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Liu2013,
  title = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  author = {Liu, Song and Quinn, JohnA. and Gutmann, Michael and Sugiyama, Masashi},
  booktitle = {Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  year = {2013},
  issn = {978-3-642-40990-5},
  keywords = {Undirected graphical models},
  url = {http://dx.doi.org/10.1007/978-3-642-40991-2_38}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2013">
  
    <span class="title">Correlated topographic analysis: estimating an ordering of correlated components</span>
    <span class="author">
      
      

      
            
            H. Sasaki,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            H. Shouno,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>Machine Learning</i>
    
    
      2013
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2013.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://dx.doi.org/10.1007/s10994-013-5351-x" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>This paper describes a novel method, which we call correlated topographic analysis (CTA), to estimate non-Gaussian components and their ordering (topography). The method is inspired by a central motivation of recent variants of independent component analysis (ICA), namely, to make use of the residual statistical dependency which ICA cannot remove. We assume that components nearby on the topographic arrangement have both linear and energy correlations, while far-away components are statistically independent. We use these dependencies to fix the ordering of the components. We start by proposing the generative model for the components. Then, we derive an approximation of the likelihood based on the model. Furthermore, since gradient methods tend to get stuck in local optima, we propose a three-step optimization method which dramatically improves topographic estimation. Using simulated data, we show that CTA estimates an ordering of the components and generalizes a previous method in terms of topography estimation. Finally, to demonstrate that CTA is widely applicable, we learn topographic representations for three kinds of real data: natural images, outputs of simulated complex cells and text data.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Sasaki2013,
  title = {Correlated topographic analysis: estimating an ordering of correlated components},
  author = {Sasaki, Hiroaki and Gutmann, Michael and Shouno, Hayaru and Hyv\"arinen, Aapo},
  journal = {Machine Learning},
  year = {2013},
  number = {2-3},
  pages = {285--317},
  volume = {92},
  issn = {0885-6125},
  keywords = {independent component analysis, topographic representation, natural image statistics, higher order features, natural language processing},
  publisher = {Springer US},
  url = {http://dx.doi.org/10.1007/s10994-013-5351-x}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2012a">
  
    <span class="title">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>Journal of Machine Learning Research</i>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2012a.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://www.jmlr.org/papers/v13/gutmann12a.html" target="_blank">url</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2012a.slides1.pdf" target="_blank">slides</a>]
    
    
    [<a href="/assets/papers/Gutmann2012a.slides2.pdf" target="_blank">slides</a>]
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2012a,
  title = {{N}oise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  journal = {Journal of Machine Learning Research},
  year = {2012},
  pages = {307--361},
  volume = {13},
  keywords = {unnormalised models, unsupervised deep learning, natural image statistics},
  url = {http://www.jmlr.org/papers/v13/gutmann12a.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2012b">
  
    <span class="title">Learning a selectivity–invariance–selectivity feature extraction architecture for images</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Pattern Recognition (ICPR)</i>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2012b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2012b.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Selectivity and invariance are thought to be important ingredients in biological or artificial visual systems. A fundamental problem is, however, to know what the visual system should be selective to and what to be invariant to. Building a statistical model of images, we learn here a three-layer feature extraction system where the selectivity and invariance emerges from the properties of the images.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2012b,
  title = {Learning a selectivity--invariance--selectivity feature extraction architecture for images},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Pattern Recognition (ICPR)},
  year = {2012},
  keywords = {Natural image statistics,unnormalised model, unsupervised deep learning}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Sasaki2012">
  
    <span class="title">Topographic analysis of correlated components</span>
    <span class="author">
      
      

      
            
            H. Sasaki,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            H. Shouno,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In JMLR: Workshop and Conference Proceedings</i>
    
    
      2012
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Sasaki2012.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v25/sasaki12" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants are assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the source where the components can have linear and higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Sasaki2012,
  title = {Topographic analysis of correlated components},
  author = {Sasaki, H. and Gutmann, M. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {JMLR: Workshop and Conference Proceedings},
  year = {2012},
  pages = {1--14},
  series = {Asian Conference on Machine Learning (ACML)},
  volume = {25},
  keywords = {independent component analysis, topographic representation, higher order correlation, linear correlation, natural image statistics, natural language processing.},
  url = {http://proceedings.mlr.press/v25/sasaki12}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2011b">
  
    <span class="title">Bregman divergence as general framework to estimate unnormalized statistical models</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            J. Hirayama
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</i>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2011b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://arxiv.org/abs/1202.3727" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in unsupervised learning.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2011b,
  title = {{B}regman divergence as general framework to estimate unnormalized statistical models},
  author = {Gutmann, M.U. and Hirayama, J.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2011},
  keywords = {Unnormalized (energy-based) models, estimation theory, density ratio estimation},
  url = {https://arxiv.org/abs/1202.3727}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2011a">
  
    <span class="title">Extracting coactivated features from multiple data sets</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</i>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2011a.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2011a.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-21735-7_40" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2011a.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a nonlinear generalization of Canonical Correlation Analysis (CCA) to find related structure in multiple data sets. The new method allows to analyze an arbitrary number of data sets, and the extracted features capture higher-order statistical dependencies. The features are independent components that are coupled across the data sets. The coupling takes the form of coactivation (dependencies of variances). We validate the new method on artificial data, and apply it to natural images and brain imaging data</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2011a,
  title = {{E}xtracting coactivated features from multiple data sets},
  author = {Gutmann, M. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2011},
  doi = {10.1007/978-3-642-21735-7_40},
  keywords = {Data fusion, natural image statistics},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-21735-7_40}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Laparra2011">
  
    <span class="title">Complex-valued independent component analysis of natural images</span>
    <span class="author">
      
      

      
            
            V. Laparra,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            J. Malo,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</i>
    
    
      2011
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Laparra2011.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-21738-8_28" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Linear independent component analysis (ICA) learns simple cell receptive fields from natural images. Here, we show that linear complex-valued ICA learns complex cell properties from Fourier-transformed natural images, i.e. two Gabor-like filters with quadrature-phase relationship. Conventional methods for complex-valued ICA assume that the phases of the output signals have uniform distribution. We show here that for natural images the phase distributions are, however, often far from uniform. We thus relax the uniformity assumption and model also the phase of the sources in complex-valued ICA. Compared to the original complex ICA model, the new model provides a better fit to the data, and leads to Gabor filters of qualitatively different shape.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Laparra2011,
  title = {{C}omplex-valued independent component analysis of natural images},
  author = {Laparra, V. and Gutmann, M. and Malo, J. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2011},
  doi = {10.1007/978-3-642-21738-8_28},
  keywords = {complex independent components analysis, natural image statistics, modeling Fourier phase distribution},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-21738-8_28}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2010">
  
    <span class="title">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</i>
    
    
      2010
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2010.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://proceedings.mlr.press/v9/gutmann10a.html" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2010,
  title = {{N}oise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2010},
  keywords = {unnormalized models, energy-based models, estimation theory, natural image statistics},
  url = {http://proceedings.mlr.press/v9/gutmann10a.html}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Pihlaja2010">
  
    <span class="title">A family of computationally efficient and simple estimators for unnormalized statistical models</span>
    <span class="author">
      
      

      
            
            M. Pihlaja,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</i>
    
    
      2010
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Pihlaja2010.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We introduce a new family of estimators for unnormalized statistical models. Our family of estimators is parameterized by two nonlinear functions and uses a single sample from an auxiliary distribution, generalizing Maximum Likelihood Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we can estimate the partition function like any other parameter in the model. The estimation is done by optimizing an algebraically simple, well defined objective function, which allows for the use of dedicated optimization methods. We establish consistency of the estimator family and give an expression for the asymptotic covariance matrix, which enables us to further analyze the influence of the nonlinearities and the auxiliary density on estimation performance. Some estimators in our family are particularly stable for a wide range of auxiliary densities. Interestingly, a specific choice of the nonlinearity establishes a connection between density estimation and classification by nonlinear logistic regression. Finally, the optimal amount of auxiliary samples relative to the given amount of the data is considered from the perspective of computational efficiency.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Pihlaja2010,
  title = {{A} family of computationally efficient and simple estimators for unnormalized statistical models},
  author = {Pihlaja, M. and Gutmann, M.U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  year = {2010},
  keywords = {unnormalized models, energy-based models, estimation theory}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2009b">
  
    <span class="title">Learning features by contrasting natural images with noise</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Conference on Artificial Neural Networks (ICANN)</i>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2009b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-04277-5_63" target="_blank">url</a>]
    
    
    
    
    [<a href="/assets/papers/Gutmann2009b.slides.pdf" target="_blank">slides</a>]
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Modeling the statistical structure of natural images is interesting for reasons related to neuroscience as well as engineering. Currently, this modeling relies heavily on generative probabilistic models. The estimation of such models is, however, difficult, especially when they consist of multiple layers. If the goal lies only in estimating the features, i.e. in pinpointing structure in natural images, one could also estimate instead a discriminative probabilistic model where multiple layers are more easily handled. For that purpose, we propose to estimate a classifier that can tell natural images apart from reference data which has been constructed to contain some known structure of natural images. The features of the classifier then reveal the interesting structure. Here, we use a classifier with one layer of features and reference data which contains the covariance-structure of natural images. We show that the features of the classifier are similar to those which are obtained from generative probabilistic models. Furthermore, we investigate the optimal shape of the nonlinearity that is used within the classifier.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2009b,
  title = {{L}earning features by contrasting natural images with noise},
  author = {Gutmann, M. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  year = {2009},
  keywords = {natural image statistics, learning by comparison, unnormalized models, energy-based models},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-04277-5_63}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2009">
  
    <span class="title">Learning reconstruction and prediction of natural stimuli by a population of spiking neurons</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In European Symposium on Artificial Neural Networks (ESANN)</i>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2009.pdf" target="_blank">pdf</a>]
    
    
    [<a href="/assets/papers/Gutmann2009.supp.pdf" target="_blank">supp</a>]
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We propose a model for learning representations of time dependent data with a population of spiking neurons. Encoding is based on a standard spiking neuron model, and the spike timings of the neurons represent the stimulus. Learning is based on the sole principle of maximization of representation accuracy: the stimulus can be decoded from the spike timings with minimum error. Since the encoding is causal, we propose two different representation strategies: The spike timings represent the stimulus either in a predictive manner or by reconstructing past input. We apply the model to speech data and discuss differences between the emergent representations.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2009,
  title = {{L}earning reconstruction and prediction of natural stimuli by a population of spiking neurons},
  author = {Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
  year = {2009},
  keywords = {Learning data representations, spiking neurons}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Koster2009">
  
    <span class="title">Learning natural image structure with a horizonal product model</span>
    <span class="author">
      
      

      
            
            U. Köster,
            
            
      
      

      
            
            J. Lindgren,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            A. Hyvärinen
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings on the International Conference on Independent Component Analysis and Signal Separation</i>
    
    
      2009
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Koster2009.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/chapter/10.1007/978-3-642-00599-2_64" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We present a novel extension to Independent Component Analysis (ICA), where the data is generated as the product of two submodels, each of which follow an ICA model, and which combine in a horizontal fashion. This is in contrast to previous nonlinear extensions to ICA which were based on a hierarchy of layers. We apply the product model to natural image patches and report the emergence of localized masks in the additional network layer, while the Gabor features that are obtained in the primary layer change their tuning properties and become less localized. As an interpretation we suggest that the model learns to separate the localization of image features from other properties, since identity and position of a feature are plausibly independent. We also show that the horizontal model can be interpreted as an overcomplete model where the features are no longer independent.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Koster2009,
  title = {{L}earning natural image structure with a horizonal product model},
  author = {K\"oster, U. and Lindgren, J. and Gutmann, M. and Hyv\"arinen, A.},
  booktitle = {Proceedings on the International Conference on Independent Component Analysis and Signal Separation},
  year = {2009},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-00599-2_64}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2008">
  
    <span class="title">Toward data representation with spiking neurons</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            K. Aihara
        
      
    </span>

    <span class="periodical">
    
      <i>Artificial Life and Robotics</i>
    
    
      2008
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2008.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    [<a href="/assets/papers/Gutmann2008.preprint.pdf" target="_blank">preprint</a>]
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://link.springer.com/article/10.1007/s10015-007-0471-7" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Notable advances in the understanding of neural processing have been made when sensory systems were investigated from the viewpoint of adaptation to the statistical structure of its input space. For this purpose, mathematical methods for data representation were used. Here, we point out that emphasis on the input structure has happened at cost of the biological plausibility of the corresponding neuron models which process the natural stimuli. The signal transformation of the data representation methods does not correspond well to the signal transformations happening on the single cell level in neural systems. Hence, we propose here data representation by means of spiking neuron models. We formulate the data representation problem as an optimization problem and derive the fundamental quantities for an iterative learning scheme.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Gutmann2008,
  title = {{T}oward data representation with spiking neurons},
  author = {Gutmann, M. and Aihara, K.},
  journal = {Artificial Life and Robotics},
  year = {2008},
  pages = {223-226},
  volume = {12},
  doi = {10.1007/s10015-007-0471-7},
  keywords = {spiking neuron, encoding, decoding, learning data representation},
  url = {https://link.springer.com/article/10.1007/s10015-007-0471-7}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Gutmann2008c">
  
    <span class="title">Learning encoding and decoding filters for data representation with a spiking neuron</span>
    <span class="author">
      
      

      
            
            M. Gutmann,
            
            
      
      

      
            
            A. Hyvärinen,
            
            
      
      
            and

            
            K. Aihara
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International Joint Conference on Neural Networks (IJCNN)</i>
    
    
      2008
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Gutmann2008c.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://doi.org/10.1109/IJCNN.2008.4633797" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Data representation methods related to ICA and sparse coding have successfully been used to model neural representation. However, they are highly abstract methods, and the neural encoding does not correspond to a detailed neuron model. This limits their power to provide deeper insight into the sensory systems on a cellular level. We propose here data representation where the encoding happens with a spiking neuron. The data representation problem is formulated as an optimization problem: Encode the input so that it can be decoded from the spike train, and optionally, so that energy consumption is minimized. The optimization leads to a learning rule for the encoder and decoder which features synergistic interaction: The decoder provides feedback affecting the plasticity of the encoder while the encoder provides optimal learning data for the decoder.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Gutmann2008c,
  title = {{L}earning encoding and decoding filters for data representation with a spiking neuron},
  author = {Gutmann, M. and Hyv{\"a}rinen, A. and Aihara, K.},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  year = {2008},
  doi = {10.1109/IJCNN.2008.4633797},
  keywords = {spiking neurons, data representation},
  url = {https://doi.org/10.1109/IJCNN.2008.4633797}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Hyvarinen2005">
  
    <span class="title">Statistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in V2</span>
    <span class="author">
      
      

      
            
            A. Hyvärinen,
            
            
      
      

      
            
            M. Gutmann,
            
            
      
      
            and

            
            P. Hoyer
        
      
    </span>

    <span class="periodical">
    
      <i>BMC Neuroscience</i>
    
    
      2005
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Hyvarinen2005.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-6-12" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Background
It has been shown that the classical receptive fields of simple and complex cells in the primary visual cortex emerge from the statistical properties of natural images by forcing the cell responses to be maximally sparse or independent. We investigate how to learn features beyond the primary visual cortex from the statistical properties of modelled complex-cell outputs. In previous work, we showed that a new model, non-negative sparse coding, led to the emergence of features which code for contours of a given spatial frequency band.

Results
We applied ordinary independent component analysis to modelled outputs of complex cells that span different frequency bands. The analysis led to the emergence of features which pool spatially coherent across-frequency activity in the modelled primary visualcortex. Thus, the statistically optimal way of processing complex-cell outputs abandons separate frequency channels, while preserving and even enhancing orientation tuning and spatial localization. As a technical aside, we found that the non-negativity constraint is not necessary: ordinary independent component analysis produces essentially the same results as our previous work.

Conclusion
We propose that the pooling that emerges allows the features to code for realistic low-level image features related to step edges. Further, the results prove the viability of statistical modelling of natural images as a framework that produces quantitative predictions of visual processing.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@article{Hyvarinen2005,
  title = {{S}tatistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in {V}2},
  author = {Hyv{\"a}rinen, A. and Gutmann, M. and Hoyer, P.O.},
  journal = {BMC Neuroscience},
  year = {2005},
  volume = {6:12},
  doi = {https://doi.org/10.1186/1471-2202-6-12},
  keywords = {Natural image statistics, computational neuroscience, V2},
  url = {https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-6-12}
}
</pre>
  </span>
  
  
</div>


</li>
<li>
  





<div id="Hyvarinen2005b">
  
    <span class="title">Statistical models of images and early vision</span>
    <span class="author">
      
      

      
            
            A. Hyvärinen,
            
            
      
      

      
            
            P. Hoyer,
            
            
      
      

      
            
            J. Hurri,
            
            
      
      
            and

            
            M. Gutmann
        
      
    </span>

    <span class="periodical">
    
      <i>In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR)</i>
    
    
      2005
    
    </span>
  

  <span class="links">
    
    [<a href="/assets/papers/Hyvarinen2005b.pdf" target="_blank">pdf</a>]
    
    
    
    
    
    
    [<a class="abstract">abs</a>]
    
    
    [<a class="bibtex">bib</a>]
    
    
    [<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.3281" target="_blank">url</a>]
    
    
    
    
    
    
    
    
    
    
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>A fundamental question in visual neuroscience is: Why are the receptive fields and response properties of visual neurons as they are? A modern approach to this problem emphasizes the importance of adaptation to ecologically valid input. In this paper, we will review work on modelling statistical regularities in ecologically valid visual input ("natural images") and the obtained functional explanation of the properties of visual neurons. A seminal statistical model for natural images was linear sparse coding which is equivalent to the model called independent component analysis (ICA). Linear features estimated by ICA resemble wavelets or Gabor functions, and provide a very good description of the properties of simple cells in the primary visual cortex. We have introduced extensions of ICA that are based on modelling dependencies of the "independent" components estimated by basic ICA. The dependencies of the components are used to define either a grouping or a topographic order between the components. With natural image data, these models lead to emergence of further properties of visual neurons: the topographic organization and complex cell receptive fields. We have also modelled the temporal structure of natural image sequences, which provides an alternative approach to the sparseness used in most models. These models can be combined in a unifying framework that we call bubble coding. Finally, we will discuss a promising new direction of research: predictive visual neuroscience. There, the goal is to try to predict response properties of neurons in areas that are poorly understood, still based on statistical modelling of natural input.</p>
  </span>
  

  <!-- Hidden bibtex block -->
  
  <span class="bibtex hidden">
    <pre class="pre pre-scrollable collapse">@inproceedings{Hyvarinen2005b,
  title = {{S}tatistical models of images and early vision},
  author = {Hyv\"arinen, A. and Hoyer, P.O. and Hurri, J. and Gutmann, M.},
  booktitle = {Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR)},
  year = {2005},
  keywords = {natural image statistics, computational neuroscience},
  url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.3281}
}
</pre>
  </span>
  
  
</div>


</li></ol>

  </article>
  
  

  

</div>

      </div>
    </div>

    
<footer>

  <div class="wrapper">
    
        Last updated: 2 March 2020.
    
    Powered by <a href="http://jekyllrb.com/">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> and <a href="https://github.com/inukshuk/jekyll-scholar/">Jekyll-Scholar</a>.

    &copy; Copyright 2020 Michael U. Gutmann.
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://michaelgutmann.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
<script src="https://michaelgutmann.github.io/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/font-awesome.min.css">
<link rel="stylesheet" href="https://michaelgutmann.github.io/assets/css/academicons.min.css">


<!-- Load Mathjax -->
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXX-X', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
