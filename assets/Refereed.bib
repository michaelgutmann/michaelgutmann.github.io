% Encoding: UTF-8

@Article{Arnold2018,
  author    = {Arnold, B.J. and Gutmann, M. U. and Grad, Y.H. and Sheppard, S.K. and Corander, J. and Lipsitch, M. and Hanage, W.P.},
  journal   = {Genetics},
  title     = {Weak Epistasis May Drive Adaptation in Recombining Bacteria},
  year      = {2018},
  month     = jan,
  number    = {3},
  pages     = {1247--1260},
  volume    = {208},
  abstract  = {The impact of epistasis on the evolution of multilocus traits depends on recombination. While sexually-reproducing eukaryotes recombine so frequently that epistasis between polymorphisms is not considered to play a large role in short-term adaptation, many bacteria also recombine, some to the degree that their populations are described as &#039;panmictic&#039; or &#039;freely recombining&#039;. However, whether this recombination is sufficient to limit the ability of selection to act on epistatic contributions to fitness is unknown. We quantify homologous recombination in five bacterial pathogens and use these parameter estimates in a multilocus model of bacterial evolution with additive and epistatic effects. We find that even for highly recombining species (e.g. Streptococcus pneumoniae or Helicobacter pylori), selection on weak interactions between distant mutations is nearly as efficient as for an asexual species, likely because homologous recombination typically transfers only short segments. However, for strong epistasis, bacterial recombination accelerates selection, with the dynamics dependent on the amount of recombination and the number of loci. Epistasis may thus play an important role in both the short- and long-term adaptive evolution of bacteria and, unlike in eukaryotes, is not limited to strong effect sizes, closely linked loci, or other conditions that limit the impact of recombination.},
  arxiv     = {https://www.biorxiv.org/content/10.1101/119958v1},
  doi       = {https://doi.org/10.1534/genetics.117.300662},
  file      = {Arnold2018.pdf:Arnold2018.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2018.01.31},
  url       = {https://www.genetics.org/content/208/3/1247},
}

@Inproceedings{Ceylan2018,
  author    = {Ceylan, C. and Gutmann, M. U.},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML)},
  title     = {Conditional Noise-Contrastive Estimation of Unnormalised Models},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  pages     = {725--733},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  abstract  = {Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.},
  arxiv     = {https://arxiv.org/abs/1806.03664},
  file      = {Ceylan2018.pdf:Ceylan2018.pdf:PDF},
  keywords  = {unnormalised models, noise-contrastive estimation},
  owner     = {mgutmann},
  pdf       = {http://proceedings.mlr.press/v80/ceylan18a/ceylan18a.pdf},
  timestamp = {2018.06.13},
  url       = {http://proceedings.mlr.press/v80/ceylan18a.html},
}

@Inproceedings{Chen2019,
  author    = {Chen, Y. and Gutmann, M. U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Adaptive {G}aussian Copula {ABC}},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  month     = {16--18 Apr},
  number    = {89},
  pages     = {1584--1592},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  abstract  = {Approximate Bayesian computation (ABC) is a set of techniques for Bayesian inference when the likelihood is intractable but sampling from the model is possible. This work presents a simple yet effective ABC algorithm based on the combination of two classical ABC approaches --- regression ABC and sequential ABC. The key idea is that rather than learning the posterior directly, we first target another auxiliary distribution that can be learned accurately by existing methods, through which we then subsequently learn the desired posterior with the help of a Gaussian copula. During this process, the complexity of the model changes adaptively according to the data at hand. Experiments on a synthetic dataset as well as three real-world inference tasks demonstrates that our method is fast, accurate, and easy to use.},
  arxiv     = {https://arxiv.org/abs/1902.10704},
  file      = {Chen2019.pdf:Chen2019.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2019.01.14},
  url       = {http://proceedings.mlr.press/v89/chen19d},
}

@Article{Corander2017,
  author    = {Corander, J. and Fraser, C. and Gutmann, M. U. and Arnold, B. and Hanage, W.P. and Bentley, S.D. and Lipsitch, M. and Croucher, N.J.},
  journal   = {Nature Ecology \& Evolution},
  title     = {Frequency-dependent selection in vaccine-associated pneumococcal population dynamics},
  year      = {2017},
  pages     = {1950--1960},
  volume    = {1},
  abstract  = {Many bacterial species are composed of multiple lineages distinguished by extensive variation in gene content. These often cocirculate in the same habitat, but the evolutionary and ecological processes that shape these complex populations are poorly understood. Addressing these questions is particularly important for Streptococcus pneumoniae, a nasopharyngeal commensal and respiratory pathogen, because the changes in population structure associated with the recent introduction of partial-coverage vaccines have substantially reduced pneumococcal disease. Here we show that pneumococcal lineages from multiple populations each have a distinct combination of intermediate-frequency genes. Functional analysis suggested that these loci may be subject to negative frequency-dependent selection (NFDS) through interactions with other bacteria, hosts or mobile elements. Correspondingly, these genes had similar frequencies in four populations with dissimilar lineage compositions. These frequencies were maintained following substantial alterations in lineage prevalences once vaccination programmes began. Fitting a multilocus NFDS model of post-vaccine population dynamics to three genomic datasets using Approximate Bayesian Computation generated reproducible estimates of the influence of NFDS on pneumococcal evolution, the strength of which varied between loci. Simulations replicated the stable frequency of lineages unperturbed by vaccination, patterns of serotype switching and clonal replacement. This framework highlights how bacterial ecology affects the impact of clinical interventions.},
  doi       = {https://doi.org/10.1038/s41559-017-0337-x},
  file      = {:Corander2017.pdf:PDF},
  issn      = {2397-334X},
  owner     = {mgutmann},
  refid     = {Corander2017},
  timestamp = {2017.10.18},
  url       = {https://www.nature.com/articles/s41559-017-0337-x},
}

@Article{Gutmann2008,
  author    = {Gutmann, M. U. and Aihara, K.},
  journal   = {Artificial Life and Robotics},
  title     = {{T}oward data representation with spiking neurons},
  year      = {2008},
  pages     = {223-226},
  volume    = {12},
  abstract  = {Notable advances in the understanding of neural processing have been made when sensory systems were investigated from the viewpoint of adaptation to the statistical structure of its input space. For this purpose, mathematical methods for data representation were used. Here, we point out that emphasis on the input structure has happened at cost of the biological plausibility of the corresponding neuron models which process the natural stimuli. The signal transformation of the data representation methods does not correspond well to the signal transformations happening on the single cell level in neural systems. Hence, we propose here data representation by means of spiking neuron models. We formulate the data representation problem as an optimization problem and derive the fundamental quantities for an iterative learning scheme.},
  doi       = {10.1007/s10015-007-0471-7},
  file      = {:Gutmann2008.pdf:PDF},
  keywords  = {spiking neuron, encoding, decoding, learning data representation},
  owner     = {gutmann},
  timestamp = {2007.10.05},
  url       = {https://link.springer.com/article/10.1007/s10015-007-0471-7},
}

@Article{Gutmann2016a,
  author    = {Gutmann, M. U. and Corander, J},
  journal   = {Journal of Machine Learning Research},
  title     = {Bayesian optimization for likelihood-free inference of simulator-based statistical models},
  year      = {2016},
  number    = {125},
  pages     = {1--47},
  volume    = {17},
  abstract  = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are generated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of identifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
  arxiv     = {http://arxiv.org/abs/1501.03291},
  file      = {journal:Gutmann2016a.pdf:PDF},
  keywords  = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  owner     = {gutmann},
  timestamp = {2015.01.15},
  url       = {http://jmlr.org/papers/v17/15-017.html},
}

@Article{Gutmann2018,
  author    = {Gutmann, M. U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal   = {Statistics and Computing},
  title     = {Likelihood-free inference via classification},
  year      = {2018},
  number    = {2},
  pages     = {411--425},
  volume    = {28},
  abstract  = {Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.},
  arxiv     = {https://arxiv.org/abs/1407.4981},
  day       = {13},
  doi       = {10.1007/s11222-017-9738-6},
  file      = {journal:Gutmann2018.pdf:PDF},
  issn      = {1573-1375},
  keywords  = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  owner     = {mgutmann},
  timestamp = {2017.09.16},
  url       = {https://doi.org/10.1007/s11222-017-9738-6},
}

@Inproceedings{Gutmann2011b,
  author    = {Gutmann, M. U. and Hirayama, J.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  title     = {{B}regman divergence as general framework to estimate unnormalized statistical models},
  year      = {2011},
  abstract  = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in unsupervised learning.},
  arxiv     = {https://arxiv.org/abs/1202.3727},
  file      = {:Gutmann2011b.pdf:PDF},
  keywords  = {Unnormalized (energy-based) models, estimation theory, density ratio estimation},
  owner     = {gutmann},
  timestamp = {2011.08.19},
  url       = {https://arxiv.org/abs/1202.3727},
}

@Inproceedings{Gutmann2011a,
  author    = {Gutmann, M. U. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  title     = {{E}xtracting coactivated features from multiple data sets},
  year      = {2011},
  abstract  = {We present a nonlinear generalization of Canonical Correlation Analysis (CCA) to find related structure in multiple data sets. The new method allows to analyze an arbitrary number of data sets, and the extracted features capture higher-order statistical dependencies. The features are independent components that are coupled across the data sets. The coupling takes the form of coactivation (dependencies of variances). We validate the new method on artificial data, and apply it to natural images and brain imaging data},
  doi       = {10.1007/978-3-642-21735-7_40},
  file      = {:Gutmann2011a.pdf:Djvu},
  keywords  = {Data fusion, natural image statistics},
  owner     = {gutmann},
  timestamp = {2009.06.05},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-21735-7_40},
}

@Inproceedings{Gutmann2009b,
  author    = {Gutmann, M. U. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  title     = {{L}earning features by contrasting natural images with noise},
  year      = {2009},
  abstract  = {Modeling the statistical structure of natural images is interesting for reasons related to neuroscience as well as engineering. Currently, this modeling relies heavily on generative probabilistic models. The estimation of such models is, however, difficult, especially when they consist of multiple layers. If the goal lies only in estimating the features, i.e. in pinpointing structure in natural images, one could also estimate instead a discriminative probabilistic model where multiple layers are more easily handled. For that purpose, we propose to estimate a classifier that can tell natural images apart from reference data which has been constructed to contain some known structure of natural images. The features of the classifier then reveal the interesting structure. Here, we use a classifier with one layer of features and reference data which contains the covariance-structure of natural images. We show that the features of the classifier are similar to those which are obtained from generative probabilistic models. Furthermore, we investigate the optimal shape of the nonlinearity that is used within the classifier.},
  doi       = {https://doi.org/10.1007/978-3-642-04277-5_63},
  file      = {:Gutmann2009b.pdf:PDF},
  keywords  = {natural image statistics, learning by comparison, unnormalized models, energy-based models},
  owner     = {gutmann},
  timestamp = {2009.06.05},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-04277-5_63},
}

@Inproceedings{Gutmann2008c,
  author    = {Gutmann, M. and Hyv{\"a}rinen, A. and Aihara, K.},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks (IJCNN)},
  title     = {{L}earning encoding and decoding filters for data representation with a spiking neuron},
  year      = {2008},
  abstract  = {Data representation methods related to ICA and sparse coding have successfully been used to model neural representation. However, they are highly abstract methods, and the neural encoding does not correspond to a detailed neuron model. This limits their power to provide deeper insight into the sensory systems on a cellular level. We propose here data representation where the encoding happens with a spiking neuron. The data representation problem is formulated as an optimization problem: Encode the input so that it can be decoded from the spike train, and optionally, so that energy consumption is minimized. The optimization leads to a learning rule for the encoder and decoder which features synergistic interaction: The decoder provides feedback affecting the plasticity of the encoder while the encoder provides optimal learning data for the decoder.},
  doi       = {https://ieeexplore.ieee.org/document/4633797},
  file      = {:Gutmann2008c.pdf:PDF},
  keywords  = {spiking neurons, data representation},
  owner     = {gutmann},
  timestamp = {2011.09.05},
  url       = {https://doi.org/10.1109/IJCNN.2008.4633797},
}

@Article{Gutmann2013,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  journal   = {Journal of Physiology-Paris},
  title     = {A three-layer model of natural image statistics},
  year      = {2013},
  number    = {5},
  pages     = {369--398},
  volume    = {107},
  abstract  = {An important property of visual systems is to be simultaneously both selective to specific patterns found in the sensory input and invariant to possible variations. Selectivity and invariance (tolerance) are opposing requirements. It has been suggested that they could be joined by iterating a sequence of elementary selectivity and tolerance computations. It is, however, unknown what should be selected or tolerated at each level of the hierarchy. We approach this issue by learning the computations from natural images. We propose and estimate a probabilistic model of natural images that consists of three processing layers. Two natural image data sets are considered: image patches, and complete visual scenes downsampled to the size of small patches. For both data sets, we find that in the first two layers, simple and complex cell-like computations are performed. In the third layer, we mainly find selectivity to longer contours; for patch data, we further find some selectivity to texture, while for the downsampled complete scenes, some selectivity to curvature is observed.},
  booktitle = {Special issue: Neural Coding and Natural Image Statistics},
  doi       = {10.1016/j.jphysparis.2013.01.001},
  file      = {official:Gutmann2013.pdf:PDF},
  issn      = {0928-4257},
  keywords  = {Natural images, probabilistic modeling, visual processing, selectivity, invariance, sparse coding, unsupervised deep learning},
  owner     = {gutmann},
  timestamp = {2013.10.17},
  url       = {http://www.sciencedirect.com/science/article/pii/S0928425713000028},
}

@Inproceedings{Gutmann2013b,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Workshop on Information Theoretic Methods in Science and Engineering},
  title     = {Estimation of unnormalized statistical models without numerical integration},
  year      = {2013},
  abstract  = {Parametric statistical models of continuous or discrete valued data are often not properly normalized, that is, they do not integrate or sum to unity. The normalization is essential for maximum likelihood estimation. While in principle, models can always be normalized by dividing them by their integral or sum (their partition function), this can in practice be extremely difficult. We have been developing methods for the estimation of unnormalized models which do not approximate the partition function using numerical integration. We review these methods, score matching and noise-contrastive estimation, point out extensions and connections both between them and methods by other authors, and discuss their pros and cons.},
  file      = {:Gutmann2013b.pdf:PDF},
  keywords  = {Unnormalized models, energy-based models, intractable likelihood},
  owner     = {gutmann},
  timestamp = {2013.08.19},
  url       = {http://www.me.inf.kyushu-u.ac.jp/witmse2013/},
}

@Article{Gutmann2012a,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  journal   = {Journal of Machine Learning Research},
  title     = {{N}oise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
  year      = {2012},
  pages     = {307--361},
  volume    = {13},
  abstract  = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, the model is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective function for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to behave like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimation methods for unnormalized models. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
  comment   = {%{\newline \url{http://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf}}},
  file      = {:Gutmann2012a.pdf:PDF},
  keywords  = {unnormalised models, unsupervised deep learning, natural image statistics},
  owner     = {gutmann},
  timestamp = {2012.03.03},
  url       = {http://www.jmlr.org/papers/v13/gutmann12a.html},
}

@Inproceedings{Gutmann2012b,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Pattern Recognition (ICPR)},
  title     = {Learning a selectivity--invariance--selectivity feature extraction architecture for images},
  year      = {2012},
  abstract  = {Selectivity and invariance are thought to be important ingredients in biological or artificial visual systems. A fundamental problem is, however, to know what the visual system should be selective to and what to be invariant to. Building a statistical model of images, we learn here a three-layer feature extraction system where the selectivity and invariance emerges from the properties of the images.},
  file      = {:Gutmann2012b.pdf:PDF},
  keywords  = {Natural image statistics,unnormalised model, unsupervised deep learning},
  owner     = {gutmann},
  timestamp = {2012.08.22},
  url       = {https://ieeexplore.ieee.org/document/6460284},
}

@Inproceedings{Gutmann2010,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {{N}oise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
  year      = {2010},
  address   = {Chia Laguna Resort, Sardinia, Italy},
  editor    = {Yee Whye Teh and Mike Titterington},
  month     = {13--15 May},
  pages     = {297--304},
  publisher = {JMLR Workshop and Conference Proceedings},
  series    = {Proceedings of Machine Learning Research},
  volume    = {9},
  abstract  = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity. We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance. In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
  file      = {:Gutmann2010.pdf:PDF},
  keywords  = {unnormalized models, energy-based models, estimation theory, natural image statistics},
  owner     = {gutmann},
  timestamp = {2010.03.11},
  url       = {http://proceedings.mlr.press/v9/gutmann10a.html},
}

@Inproceedings{Gutmann2009,
  author    = {Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {European Symposium on Artificial Neural Networks (ESANN)},
  title     = {{L}earning reconstruction and prediction of natural stimuli by a population of spiking neurons},
  year      = {2009},
  abstract  = {We propose a model for learning representations of time dependent data with a population of spiking neurons. Encoding is based on a standard spiking neuron model, and the spike timings of the neurons represent the stimulus. Learning is based on the sole principle of maximization of representation accuracy: the stimulus can be decoded from the spike timings with minimum error. Since the encoding is causal, we propose two different representation strategies: The spike timings represent the stimulus either in a predictive manner or by reconstructing past input. We apply the model to speech data and discuss differences between the emergent representations.},
  file      = {:Gutmann2009.pdf:PDF},
  keywords  = {Learning data representations, spiking neurons},
  owner     = {gutmann},
  timestamp = {2009.02.18},
  url       = {https://www.esann.org/proceedings/2009},
}

@Article{Gutmann2014,
  author    = {Gutmann, M. U. and Laparra, V. and Hyv\"arinen, A. and Malo, J.},
  journal   = {PLOS ONE},
  title     = {Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images},
  year      = {2014},
  number    = {2},
  pages     = {e86481},
  volume    = {9},
  abstract  = {Independent component and canonical correlation analysis are two general-purpose statistical methods with wide applicability. In neuroscience, independent component analysis of chromatic natural images explains the spatio-chromatic structure of primary cortical receptive fields in terms of properties of the visual environment. Canonical correlation analysis explains similarly chromatic adaptation to different illuminations. But, as we show in this paper, neither of the two methods generalizes well to explain both spatio-chromatic processing and adaptation at the same time. We propose a statistical method which combines the desirable properties of independent component and canonical correlation analysis: It finds independent components in each data set which, across the two data sets, are related to each other via linear or higher-order correlations. The new method is as widely applicable as canonical correlation analysis, and also to more than two data sets. We call it higher-order canonical correlation analysis. When applied to chromatic natural images, we found that it provides a single (unified) statistical framework which accounts for both spatio-chromatic processing and adaptation. Filters with spatio-chromatic tuning properties as in the primary visual cortex emerged and corresponding-colors psychophysics was reproduced reasonably well. We used the new method to make a theory-driven testable prediction on how the neural response to colored patterns should change when the illumination changes. We predict shifts in the responses which are comparable to the shifts reported for chromatic contrast habituation.},
  doi       = {https://doi.org/10.1371/journal.pone.0086481},
  file      = {journal:Gutmann2014.pdf:PDF},
  keywords  = {Generalization of canonical correlation analysis, data fusion, natural image statistics, spatio-chromatic adaptation},
  owner     = {gutmann},
  publisher = {Public Library of Science},
  timestamp = {2014.02.17},
  url       = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086481},
}

@Article{Hyvarinen2005,
  author    = {Hyv{\"a}rinen, A. and Gutmann, M. and Hoyer, P.O.},
  journal   = {BMC Neuroscience},
  title     = {{S}tatistical model of natural stimuli predicts edge-like pooling of spatial frequency channels in {V}2},
  year      = {2005},
  volume    = {6:12},
  abstract  = {Background
It has been shown that the classical receptive fields of simple and complex cells in the primary visual cortex emerge from the statistical properties of natural images by forcing the cell responses to be maximally sparse or independent. We investigate how to learn features beyond the primary visual cortex from the statistical properties of modelled complex-cell outputs. In previous work, we showed that a new model, non-negative sparse coding, led to the emergence of features which code for contours of a given spatial frequency band.

Results
We applied ordinary independent component analysis to modelled outputs of complex cells that span different frequency bands. The analysis led to the emergence of features which pool spatially coherent across-frequency activity in the modelled primary visualcortex. Thus, the statistically optimal way of processing complex-cell outputs abandons separate frequency channels, while preserving and even enhancing orientation tuning and spatial localization. As a technical aside, we found that the non-negativity constraint is not necessary: ordinary independent component analysis produces essentially the same results as our previous work.

Conclusion
We propose that the pooling that emerges allows the features to code for realistic low-level image features related to step edges. Further, the results prove the viability of statistical modelling of natural images as a framework that produces quantitative predictions of visual processing.},
  doi       = {https://doi.org/10.1186/1471-2202-6-12},
  file      = {:Hyvarinen2005.pdf:PDF},
  keywords  = {Natural image statistics, computational neuroscience, V2},
  owner     = {gutmann},
  timestamp = {2011.09.05},
  url       = {https://bmcneurosci.biomedcentral.com/articles/10.1186/1471-2202-6-12},
}

@Inproceedings{Hyvarinen2005b,
  Title                    = {{S}tatistical models of images and early vision},
  Author                   = {Hyv\"arinen, A. and Hoyer, P.O. and Hurri, J. and Gutmann, M.},
  Booktitle                = {Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR)},
  Year                     = {2005},

  Abstract                 = {A fundamental question in visual neuroscience is: Why are the receptive fields and response properties of visual neurons as they are? A modern approach to this problem emphasizes the importance of adaptation to ecologically valid input. In this paper, we will review work on modelling statistical regularities in ecologically valid visual input ("natural images") and the obtained functional explanation of the properties of visual neurons. A seminal statistical model for natural images was linear sparse coding which is equivalent to the model called independent component analysis (ICA). Linear features estimated by ICA resemble wavelets or Gabor functions, and provide a very good description of the properties of simple cells in the primary visual cortex. We have introduced extensions of ICA that are based on modelling dependencies of the "independent" components estimated by basic ICA. The dependencies of the components are used to define either a grouping or a topographic order between the components. With natural image data, these models lead to emergence of further properties of visual neurons: the topographic organization and complex cell receptive fields. We have also modelled the temporal structure of natural image sequences, which provides an alternative approach to the sparseness used in most models. These models can be combined in a unifying framework that we call bubble coding. Finally, we will discuss a promising new direction of research: predictive visual neuroscience. There, the goal is to try to predict response properties of neurons in areas that are poorly understood, still based on statistical modelling of natural input.},
  File                     = {:Hyvarinen2005b.pdf:PDF},
  Keywords                 = {natural image statistics, computational neuroscience},
  Owner                    = {gutmann},
  Timestamp                = {2006.05.11},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.3281}
}

@Article{Jarvenpaa2018,
  author    = {J\"arvenp\"a\"a, M. and Gutmann, M. U. and Vehtari, A. and Marttinen, P.},
  journal   = {The Annals of Applied Statistics},
  title     = {Gaussian process modeling in approximate {B}ayesian computation to estimate horizontal gene transfer in bacteria},
  year      = {2018},
  number    = {4},
  pages     = {2228--2251},
  volume    = {12},
  abstract  = {Approximate Bayesian computation (ABC) can be used for model fitting when the likelihood function is intractable but simulating from the model is feasible. However, even a single evaluation of a complex model may take several hours, limiting the number of model evaluations available. Modelling the discrepancy between the simulated and observed data using a Gaussian process (GP) can be used to reduce the number of model evaluations required by ABC, but the sensitivity of this approach to a specific GP formulation has not yet been thoroughly investigated. We begin with a comprehensive empirical evaluation of using GPs in ABC, including various transformations of the discrepancies and two novel GP formulations. Our results indicate the choice of GP may significantly affect the accuracy of the estimated posterior distribution. Selection of an appropriate GP model is thus important. We formulate expected utility to measure the accuracy of classifying discrepancies below or above the ABC threshold, and show that it can be used to automate the
GP model selection step. Finally, based on the understanding gained with toy examples, we fit a population genetic model for bacteria, providing insight into horizontal gene transfer events within the population and from external origins.},
  arxiv     = {https://arxiv.org/abs/1610.06462},
  doi       = {https://doi.org/10.1214/18-AOAS1150},
  file      = {Jarvenpaa2018.pdf:Jarvenpaa2018.pdf:PDF},
  keywords  = {Approximation Bayesian computation, emulation, Gaussian processes, model selection},
  owner     = {mgutmann},
  timestamp = {2017.04.27},
  url       = {https://projecteuclid.org/euclid.aoas/1542078043},
}

@Article{Jarvenpaa2019a,
  author    = {J\"arvenp\"a\"a, M. and Gutmann, M. U. and Vehtari, A. and Marttinen, P.},
  journal   = {Bayesian Analysis},
  title     = {Efficient acquisition rules for model-based approximate {B}ayesian computation},
  year      = {2019},
  number    = {2},
  pages     = {595--622},
  volume    = {14},
  abstract  = {Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, Bayesian optimisation (BO) and surrogate models such as Gaussian processes have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next but common BO strategies are not designed for the goal of estimating the posterior distribution. Our paper addresses this gap in the literature. We propose to compute the uncertainty in the ABC posterior density, which is due to a lack of simulations to estimate this quantity accurately, and define a loss function that measures this uncertainty. We then propose to select the next evaluation location to minimise the expected loss. Experiments show that the proposed method often produces the most accurate approximations as compared to common BO strategies.},
  arxiv     = {https://arxiv.org/abs/1704.00520},
  doi       = {doi:10.1214/18-BA1121},
  file      = {:Jarvenpaa2019a.pdf:PDF},
  keywords  = {Approximation Bayesian computation, Bayesian optimisation},
  owner     = {mgutmann},
  timestamp = {2017.04.27},
  url       = {https://projecteuclid.org/euclid.ba/1537258134},
}

@Article{Jarvenpaa2020,
  author    = {J\"arvenp\"a\"a, M. and Gutmann, M. U. and Vehtari, A. and Marttinen, P.},
  journal   = {Bayesian Analysis},
  title     = {Parallel {G}aussian process surrogate {B}ayesian inference with noisy likelihood evaluations},
  year      = {2021},
  number    = {1},
  pages     = {147--178},
  volume    = {16},
  abstract  = {We consider Bayesian inference when only a limited number of noisy log-likelihood evaluations can be obtained. This occurs for example when complex simulator-based statistical models are fitted to data, and synthetic likelihood (SL) method is used to form the noisy log-likelihood estimates using computationally costly forward simulations. We frame the inference task as a sequential Bayesian experimental design problem, where the log-likelihood function is modelled with a hierarchical Gaussian process (GP) surrogate model, which is used to efficiently select additional log-likelihood evaluation locations. Motivated by recent progress in the related problem of batch Bayesian optimisation, we develop various batch-sequential design strategies which allow to run some of the potentially costly simulations in parallel. We analyse the properties of the resulting method theoretically and empirically. Experiments with several toy problems and simulation models suggest that our method is robust, highly parallelisable, and sample-efficient.
We consider Bayesian inference when only a limited number of noisy log-likelihood evaluations can be obtained. This occurs for example when complex simulator-based statistical models are fitted to data, and synthetic likelihood (SL) method is used to form the noisy log-likelihood estimates using computationally costly forward simulations. We frame the inference task as a sequential Bayesian experimental design problem, where the log-likelihood function is modelled with a hierarchical Gaussian process (GP) surrogate model, which is used to efficiently select additional log-likelihood evaluation locations. Motivated by recent progress in the related problem of batch Bayesian optimisation, we develop various batch-sequential design strategies which allow to run some of the potentially costly simulations in parallel. We analyse the properties of the resulting method theoretically and empirically. Experiments with several toy problems and simulation models suggest that our method is robust, highly parallelisable, and sample-efficient.},
  arxiv     = {https://arxiv.org/abs/1905.01252},
  doi       = {10.1214/20-BA1200},
  file      = {:Jarvenpaa2020.pdf:PDF},
  keywords  = {expensive likelihoods, Gaussian processes, likelihood-free inference, parallel computing, sequential experiment design, surrogate modelling},
  owner     = {mgutmann},
  publisher = {International Society for Bayesian Analysis},
  timestamp = {2020.03.02},
  url       = {https://doi.org/10.1214/20-BA1200},
}

@Inproceedings{Koster2009,
  author    = {K\"oster, U. and Lindgren, J. and Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {Proceedings on the International Conference on Independent Component Analysis and Signal Separation},
  title     = {{L}earning natural image structure with a horizonal product model},
  year      = {2009},
  abstract  = {We present a novel extension to Independent Component Analysis (ICA), where the data is generated as the product of two submodels, each of which follow an ICA model, and which combine in a horizontal fashion. This is in contrast to previous nonlinear extensions to ICA which were based on a hierarchy of layers. We apply the product model to natural image patches and report the emergence of localized masks in the additional network layer, while the Gabor features that are obtained in the primary layer change their tuning properties and become less localized. As an interpretation we suggest that the model learns to separate the localization of image features from other properties, since identity and position of a feature are plausibly independent. We also show that the horizontal model can be interpreted as an overcomplete model where the features are no longer independent.},
  file      = {:Koster2009.pdf:PDF},
  owner     = {gutmann},
  timestamp = {2009.02.18},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-00599-2_64},
}

@Inproceedings{Laparra2011,
  author    = {Laparra, V. and Gutmann, M. U. and Malo, J. and Hyv{\"a}rinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Neural Networks (ICANN)},
  title     = {{C}omplex-valued independent component analysis of natural images},
  year      = {2011},
  abstract  = {Linear independent component analysis (ICA) learns simple cell receptive fields from natural images. Here, we show that linear complex-valued ICA learns complex cell properties from Fourier-transformed natural images, i.e. two Gabor-like filters with quadrature-phase relationship. Conventional methods for complex-valued ICA assume that the phases of the output signals have uniform distribution. We show here that for natural images the phase distributions are, however, often far from uniform. We thus relax the uniformity assumption and model also the phase of the sources in complex-valued ICA. Compared to the original complex ICA model, the new model provides a better fit to the data, and leads to Gabor filters of qualitatively different shape.},
  doi       = {10.1007/978-3-642-21738-8_28},
  file      = {:Laparra2011.pdf:PDF},
  keywords  = {complex independent components analysis, natural image statistics, modeling Fourier phase distribution},
  owner     = {gutmann},
  timestamp = {2009.06.05},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-21738-8_28},
}

@Article{Li2020,
  Title                    = {Genome-wide CRISPR screen Identifies Host Dependency Factors for Influenza A Virus Infection},
  Author                   = {Li, Bo and Clohisey, Sara M. and Chia, Bing Shao and Wang, Bo and Cui, Ang and Eisenhaure, Thomas and Schweitzer, Lawrence D. and Hoover, Paul and Parkinson, Nicholas J. and Nachshon, Aharon and Smith, Nikki and Regan, Tim and Farr, David and Gutmann, Michael U. and Bukhari, Syed Irfan and Law, Andrew and Sangesland, Maya and Gat-viks, Irit and Digard, Paul and Vasudevan, Shobha and Lingwood, Daniel and Dockrell, David H. and Doench, John G. and Baillie, J. Kenneth and Hacohen, Nir},
  Journal                  = {Nature Communications},
  Year                     = {2020},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {164--},
  Volume                   = {11},

  Abstract                 = {Host dependency factors that are required for influenza A virus infection may serve as therapeutic targets as the virus is less likely to bypass them under drug-mediated selection pressure. Previous attempts to identify host factors have produced largely divergent results, with few overlapping hits across different studies. Here, we perform a genome-wide CRISPR/Cas9 screen and devise a new approach, meta-analysis by information content (MAIC) to systematically combine our results with prior evidence for influenza host factors. MAIC out33 performs other meta-analysis methods when using our CRISPR screen as validation data. We validate the host factors, WDR7, CCDC115 and TMEM199, demonstrating that these genes are essential for viral entry and regulation of V-type ATPase assembly. We also find that CMTR1, a human mRNA cap methyltransferase, is required for efficient viral cap snatching and regulation of a cell autonomous immune response, and provides synergistic protection with the influenza endonuclease inhibitor Xofluza.
Host dependency factors that are required for influenza A virus infection may serve as therapeutic targets as the virus is less likely to bypass them under drug-mediated selection pressure. Previous attempts to identify host factors have produced largely divergent results, with few overlapping hits across different studies. Here, we perform a genome-wide CRISPR/Cas9 screen and devise a new approach, meta-analysis by information content (MAIC) to systematically combine our results with prior evidence for influenza host factors. MAIC out33 performs other meta-analysis methods when using our CRISPR screen as validation data. We validate the host factors, WDR7, CCDC115 and TMEM199, demonstrating that these genes are essential for viral entry and regulation of V-type ATPase assembly. We also find that CMTR1, a human mRNA cap methyltransferase, is required for efficient viral cap snatching and regulation of a cell autonomous immune response, and provides synergistic protection with the influenza endonuclease inhibitor Xofluza.},
  Doi                      = {https://doi.org/10.1038/s41467-019-13965-x},
  File                     = {Li2020.pdf:Li2020.pdf:PDF},
  ISSN                     = {2041-1723},
  Owner                    = {mgutmann},
  Timestamp                = {2020.03.02},
  Url                      = {https://doi.org/10.1038/s41467-019-13965-x}
}

@Article{Lintusaari2019a,
  author    = {Lintusaari, J and Blomstedt, P and Rose, B and Sivula, T and Gutmann, M. U. and Kaski, S and Corander, J},
  journal   = {Wellcome Open Research},
  title     = {Resolving outbreak dynamics using approximate {B}ayesian computation for stochastic birth-death models},
  year      = {2019},
  number    = {14},
  volume    = {4},
  arxiv     = {https://www.biorxiv.org/content/10.1101/215533v2},
  doi       = {10.12688/wellcomeopenres.15048.2},
  file      = {Lintusaari2019a.pdf:Lintusaari2019a.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2019.10.29},
  url       = {https://wellcomeopenresearch.org/articles/4-14/v2},
}

@Article{Lintusaari2017,
  author    = {Lintusaari, J. and Gutmann, M. U. and Dutta, R. and Kaski, S. and Corander, J.},
  journal   = {Systematic Biology},
  title     = {Fundamentals and Recent Developments in Approximate {B}ayesian Computation},
  year      = {2017},
  month     = jan,
  number    = {1},
  pages     = {e66--e82},
  volume    = {66},
  abstract  = {Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible. We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
  comment   = {10.1093/sysbio/syw077},
  doi       = {10.1093/sysbio/syw077},
  file      = {:Lintusaari2017.pdf:PDF},
  issn      = {1063-5157},
  keywords  = {Intractable likelihoods, generative models, latent variables, approximate Bayesian computation},
  owner     = {mgutmann},
  timestamp = {2017.03.10},
  url       = {http://dx.doi.org/10.1093/sysbio/syw077},
}

@Article{Lintusaari2016,
  author    = {Lintusaari, J. and Gutmann, M. U. and Kaski, S. and Corander, J.},
  journal   = {Genetics},
  title     = {On the identifiability of transmission dynamic models for infectious diseases},
  year      = {2016},
  number    = {3},
  pages     = {911--918},
  volume    = {202},
  abstract  = {Understanding the transmission dynamics of infectious diseases is important for both biological research and public health applications. It has been widely demonstrated that statistical modeling provides a firm basis for inferring relevant epidemiological quantities from incidence and molecular data. However, the complexity of transmission dynamic models causes two challenges: Firstly, the likelihood function of the models is generally not computable and computationally intensive simulation-based inference methods need to be employed. Secondly, the model may not be fully identifiable from the available data. While the first difficulty can be tackled by computational and algorithmic advances, the second obstacle is more fundamental. Identifiability issues may lead to inferences which are more driven by the prior assumptions than the data themselves. We here consider a popular and relatively simple, yet analytically intractable model for the spread of tuberculosis based on classical IS6110 fingerprinting data. We report on the identifiability of the model, presenting also some methodological advances regarding the inference. Using likelihood approximations, it is shown that the reproductive value cannot be identified from the data available and that the posterior distributions obtained in previous work have likely been substantially dominated by the assumed prior distribution. Further, we show that the inferences are influenced by the assumed infectious population size which has generally been kept fixed in previous work. We demonstrate that the infectious population size can be inferred if the remaining epidemiological parameters are already known with sufficient precision.},
  arxiv     = {https://www.biorxiv.org/content/10.1101/021972v2},
  doi       = {https://doi.org/10.1534/genetics.115.180034},
  file      = {journal:Lintusaari2016.pdf:PDF},
  keywords  = {Intractable likelihoods, epidemiology of infectious disease},
  owner     = {gutmann},
  timestamp = {2015.09.27},
  url       = {https://www.genetics.org/content/202/3/911},
}

@Article{Lintusaari2018,
  author    = {Lintusaari, Jarno and Vuollekoski, Henri and Kangasr\"a\"asi\"o, Antti and Skyt\'en, Kusti and J\"arvenp\"a\"a, Marko and Marttinen, Pekka and Gutmann, Michael U. and Vehtari, Aki and Corander, Jukka},
  journal   = {Journal of Machine Learning Research},
  title     = {{ELFI}: Engine for Likelihood-Free Inference},
  year      = {2018},
  number    = {16},
  pages     = {1--7},
  volume    = {19},
  abstract  = {Engine for Likelihood-Free Inference (ELFI) is a Python software library for performing likelihood-free inference (LFI). ELFI provides a convenient syntax for arranging components in LFI, such as priors, simulators, summaries or distances, to a network called ELFI graph. The components can be implemented in a wide variety of languages. The stand-alone ELFI graph can be used with any of the available inference methods without modifications. A central method implemented in ELFI is Bayesian Optimization for Likelihood-Free Inference (BOLFI), which has recently been shown to accelerate likelihood-free inference up to several orders of magnitude by surrogate-modelling the distance. ELFI also has an inbuilt support for output data storing for reuse and analysis, and supports parallelization of computation from multiple cores up to a cluster environment. ELFI is designed to be extensible and provides interfaces for widening its functionality. This makes the adding of new inference methods to ELFI straightforward and automatically compatible with the inbuilt features.},
  arxiv     = {https://arxiv.org/abs/1708.00707},
  file      = {Lintusaari2018.pdf:Lintusaari2018.pdf:PDF},
  keywords  = {Likelihood-free inference, approximate Bayesian computation, Python, software, parallel computing},
  owner     = {mgutmann},
  timestamp = {2017.09.20},
  url       = {http://jmlr.org/papers/v19/17-374.html},
}

@Inproceedings{Liu2013,
  author    = {Liu, Song and Quinn, JohnA. and Gutmann, Michael U. and Sugiyama, Masashi},
  booktitle = {Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
  title     = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  year      = {2013},
  abstract  = {We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, which is a critical computational bottleneck of the naive approach, can be remarkably mitigated. Through experiments on gene expression and Twitter data analysis, we demonstrate the usefulness of our method.},
  doi       = {http://dx.doi.org/10.1007/978-3-642-40991-2_38},
  file      = {:Liu2013.pdf:PDF},
  issn      = {978-3-642-40990-5},
  keywords  = {Undirected graphical models},
  owner     = {gutmann},
  timestamp = {2014.02.25},
  url       = {https://link.springer.com/chapter/10.1007%2F978-3-642-40991-2_38},
}

@Article{Liu2014,
  author    = {Liu, S. and Quinn, J.A. and Gutmann, M. U. and Suzuki, T. and Sugiyama, M.},
  journal   = {Neural Computation},
  title     = {Direct learning of sparse changes in {M}arkov networks by density ratio estimation},
  year      = {2014},
  number    = {6},
  pages     = {1169--1197},
  volume    = {26},
  abstract  = {We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we directly learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, a critical bottleneck of the naive approach, can be remarkably mitigated. We also give the dual formulation of the optimization problem, which further reduces the computation cost for large-scale Markov networks. Through experiments, we demonstrate the usefulness of our method.},
  arxiv     = {https://arxiv.org/abs/1304.6803},
  doi       = {http://dx.doi.org/10.1162/NECO_a_00589},
  file      = {:Liu2014.pdf:PDF},
  keywords  = {Undirected graphical models, density ratio estimation},
  owner     = {gutmann},
  timestamp = {2014.01.13},
  url       = {https://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00589},
}

@Inproceedings{LopezGuevara2017a,
  author    = {Lopez-Guevara, T. and Taylor, N.K. and Gutmann, M. U. and Ramamoorthy, S. and Subr, K.},
  booktitle = {Proceedings of the 1st Annual Conference on Robot Learning (CoRL)},
  title     = {Adaptable Pouring: Teaching Robots Not to Spill using Fast but Approximate Fluid Simulation},
  year      = {2017},
  editor    = {Sergey Levine and Vincent Vanhoucke and Ken Goldberg},
  month     = {Nov},
  pages     = {77--86},
  series    = {Proceedings of Machine Learning Research},
  volume    = {78},
  abstract  = {Humans manipulate fluids intuitively using intuitive approximations of the underlying physical model. In this paper, we explore a general methodology that robots may use to develop and improve strategies for overcoming manipulation tasks associated with appropriately defined loss functions. We focus on the specific task of pouring a liquid from a container (pourer) to another container (receiver) while minimizing the mass of liquid that spills outside the receiver. We present a solution, based on guidance from approximate simulation, that is fast, flexible and adaptable to novel containers as long as their shapes can be sensed. Our key idea is to decouple the optimization of the parameter space of the simulator from the optimization over action space for determining robot control actions. We perform the former in a training (calibration) stage and the latter during run-time (deployment). For the purpose of this paper we use pouring in both stages, even though separate actions could be chosen. We compare four different strategies for calibration and three different strategies for deployment. Our results demonstrate that fast fluid simulations are effective, even if they are only approximate, in guiding automatic strategies for pouring liquids.},
  file      = {LopezGuevara2017a.pdf:LopezGuevara2017a.pdf:PDF},
  pdf       = {http://proceedings.mlr.press/v78/lopez-guevara17a/lopez-guevara17a.pdf},
  url       = {http://proceedings.mlr.press/v78/lopez-guevara17a.html},
}

@Article{Marttinen2015,
  author    = {Marttinen, P. and Croucher, N.J. and Gutmann, M. U. and Corander, J. and Hanage, W.P.},
  journal   = {Microbial Genomics},
  title     = {Recombination produces coherent bacterial species clusters in both core and accessory genomes},
  year      = {2015},
  number    = {5},
  volume    = {1},
  abstract  = {Background:
Population samples show bacterial genomes can be divided into a core of ubiquitous genes and accessory genes that are present in a fraction of isolates. The ecological significance of this variation in gene content remains unclear. However, microbiologists agree that a bacterial species should be 'genomically coherent', even though there is no consensus on how this should be determined.

Results:
We use a parsimonious model combining diversification in both the core and accessory genome, including mutation, homologous recombination (HR) and horizontal gene transfer (HGT) introducing new loci, to produce a population of interacting clusters of strains with varying genome content. New loci introduced by HGT may then be transferred on by HR. The model fits well to a systematic population sample of 616 pneumococcal genomes, capturing the major features of the population structure with parameter values that agree well with empirical estimates.

Conclusions:
The model does not include explicit selection on individual genes, suggesting that crude comparisons of gene content may be a poor predictor of ecological function. We identify a clearly divergent subpopulation of pneumococci that are inconsistent with the model and may be considered genomically incoherent with the rest of the population. These strains have a distinct disease tropism and may be rationally defined as a separate species. We also find deviations from the model that may be explained by recent population bottlenecks or spatial structure.},
  doi       = {http://dx.doi.org/10.1099/mgen.0.000038},
  file      = {journal:Marttinen2015.pdf:PDF},
  keywords  = {core/accessory genome, evolution, computational modeling, speciation, recombination},
  owner     = {gutmann},
  timestamp = {2014.07.30},
  url       = {https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000038},
}

@Article{Numminen2016,
  author    = {Numminen, E. and Gutmann, Michael U and Shubin, Mikhail and Marttinen, Pekka and M\'eric, Guillaume and van Schaik, Willem and Coque, Teresa and Baquero, Fernando and Willems, R.J.L. and Sheppard, S.K. and Feil, E.J. and Hanage, W.P. and Corander, Jukka},
  journal   = {Journal of Theoretical Biology},
  title     = {The impact of host metapopulation structure on the population genetics of colonizing bacteria},
  year      = {2016},
  pages     = {53--62},
  volume    = {396},
  abstract  = {Many key bacterial pathogens are frequently carried asymptomatically, and the emergence and spread of these opportunistic pathogens can be driven, or mitigated, via demographic changes within the host population. These inter-host transmission dynamics combine with basic evolutionary parameters such as rates of mutation and recombination, population size and selection, to shape the genetic diversity within bacterial populations. Whilst many studies have focused on how molecular processes underpin bacterial population structure, the impact of host migration and the connectivity of the local populations has received far less attention. A stochastic neutral model incorporating heightened local transmission has been previously shown to fit closely with genetic data for several bacterial species. However, this model did not incorporate transmission limiting population stratification, nor the possibility of migration of strains between subpopulations, which we address here by presenting an extended model. We study the consequences of migration in terms of shared genetic variation and show by simulation that the previously used summary statistic, the allelic mismatch distribution, can be insensitive to even large changes in microepidemic and migration rates. Using likelihood-free inference with genotype network topological summaries we fit a simpler model to commensal and hospital samples from the common nosocomial pathogens Staphylococcus aureus, Staphylococcus epidermidis, Enterococcus faecalis and Enterococcus faecium. Only the hospital data for E. faecium display clearly marked deviations from the model predictions which may be attributable to its adaptation to the hospital environment.},
  arxiv     = {https://www.biorxiv.org/content/10.1101/027581v1},
  doi       = {https://doi.org/10.1016/j.jtbi.2016.02.019},
  file      = {journal:Numminen2016.pdf:PDF},
  keywords  = {Bacterial evolution, Genetic structure, Migration, Population dynamics},
  owner     = {gutmann},
  timestamp = {2015.09.30},
  url       = {http://www.sciencedirect.com/science/article/pii/S0022519316001156},
}

@Inproceedings{Pihlaja2010,
  author    = {Pihlaja, M. and Gutmann, M. U. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)},
  title     = {{A} family of computationally efficient and simple estimators for unnormalized statistical models},
  year      = {2010},
  abstract  = {We introduce a new family of estimators for unnormalized statistical models. Our family of estimators is parameterized by two nonlinear functions and uses a single sample from an auxiliary distribution, generalizing Maximum Likelihood Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we can estimate the partition function like any other parameter in the model. The estimation is done by optimizing an algebraically simple, well defined objective function, which allows for the use of dedicated optimization methods. We establish consistency of the estimator family and give an expression for the asymptotic covariance matrix, which enables us to further analyze the influence of the nonlinearities and the auxiliary density on estimation performance. Some estimators in our family are particularly stable for a wide range of auxiliary densities. Interestingly, a specific choice of the nonlinearity establishes a connection between density estimation and classification by nonlinear logistic regression. Finally, the optimal amount of auxiliary samples relative to the given amount of the data is considered from the perspective of computational efficiency.},
  arxiv     = {https://arxiv.org/abs/1203.3506},
  file      = {:Pihlaja2010.pdf:PDF},
  keywords  = {unnormalized models, energy-based models, estimation theory},
  owner     = {gutmann},
  timestamp = {2010.06.10},
}

@Inproceedings{Rhodes2019,
  author    = {Rhodes, B. and Gutmann, M. U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Variational Noise-Contrastive Estimation},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  month     = {16--18 Apr},
  number    = {89},
  pages     = {1584--1592},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  abstract  = {Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.},
  arxiv     = {https://arxiv.org/abs/1810.08010},
  file      = {Rhodes2019.pdf:Rhodes2019.pdf:PDF},
  keywords  = {noise-contrastive estimation, latent variables},
  owner     = {mgutmann},
  timestamp = {2018.10.23},
  url       = {http://proceedings.mlr.press/v89/rhodes19a},
}

@Article{Sasaki2017,
  author    = {Sasaki, H. and Gutmann, M. U. and Shouno, H. and Hyv\"arinen, A.},
  journal   = {Neural Computation},
  title     = {Simultaneous Estimation of Non-{G}aussian Components and their Correlation Structure},
  year      = {2017},
  number    = {11},
  pages     = {2887--2924},
  volume    = {29},
  abstract  = {The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they usually concentrated on higher-order correlations such as energy (square) correlations. Yet, linear correlations are a most fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods, so they can only be analyzed by developing new methods which explicitly allow for linearly correlated components. In this paper, we propose a probabilistic model of linear non-Gaussian components which are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parametrized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score matching (Hyvrinen, 2005), the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method improves identifiability of non-Gaussian components by simultaneously learning their correlation structure. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data show that the method finds new kinds of dependencies between the components.},
  arxiv     = {https://arxiv.org/abs/1506.05666},
  doi       = {10.1162/neco_a_01006},
  file      = {:Sasaki2017.pdf:PDF},
  keywords  = {Extensions of ICA, speech data, natural images},
  owner     = {gutmann},
  timestamp = {2015.09.30},
  url       = {http://dx.doi.org/10.1162/neco_a_01006},
}

@Inproceedings{Sasaki2014,
  author    = {Sasaki, H. and Gutmann, M. U. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Estimating dependency structures for non-{G}aussian components with linear and energy correlations},
  year      = {2014},
  abstract  = {The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the ICA components. It would be very useful to estimate the dependency structure from data. However, most models have concentrated on higher-order correlations such as energy correlations, neglecting linear correlations. Linear correlations might be a strong and informative form of a dependency for some real data sets, but they are usually completely removed by ICA and related methods, and not analyzed at all. In this paper, we propose a probabilistic model of non-Gaussian components which are allowed to have both linear and energy correlations. The dependency structure of the components is explicitly parametrized by a parameter matrix, which defines an undirected graphical model over the latent components. Furthermore, the estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using artificial data, we demonstrate that the proposed method is able to estimate non-Gaussian components and their dependency structures, as it is designed to do. When applied to natural images and outputs of simulated complex cells in the primary visual cortex, novel dependencies between the estimated features are discovered.},
  file      = {official:Sasaki2014.pdf:PDF},
  keywords  = {Extensions of ICA, natural image statistics},
  owner     = {gutmann},
  timestamp = {2014.02.23},
  url       = {http://proceedings.mlr.press/v33/sasaki14.html},
}

@Article{Sasaki2013,
  author    = {Sasaki, Hiroaki and Gutmann, Michael U. and Shouno, Hayaru and Hyv\"arinen, Aapo},
  journal   = {Machine Learning},
  title     = {Correlated topographic analysis: estimating an ordering of correlated components},
  year      = {2013},
  number    = {2-3},
  pages     = {285--317},
  volume    = {92},
  abstract  = {This paper describes a novel method, which we call correlated topographic analysis (CTA), to estimate non-Gaussian components and their ordering (topography). The method is inspired by a central motivation of recent variants of independent component analysis (ICA), namely, to make use of the residual statistical dependency which ICA cannot remove. We assume that components nearby on the topographic arrangement have both linear and energy correlations, while far-away components are statistically independent. We use these dependencies to fix the ordering of the components. We start by proposing the generative model for the components. Then, we derive an approximation of the likelihood based on the model. Furthermore, since gradient methods tend to get stuck in local optima, we propose a three-step optimization method which dramatically improves topographic estimation. Using simulated data, we show that CTA estimates an ordering of the components and generalizes a previous method in terms of topography estimation. Finally, to demonstrate that CTA is widely applicable, we learn topographic representations for three kinds of real data: natural images, outputs of simulated complex cells and text data.},
  doi       = {http://dx.doi.org/10.1007/s10994-013-5351-x},
  file      = {:Sasaki2013.pdf:PDF},
  issn      = {0885-6125},
  keywords  = {independent component analysis, topographic representation, natural image statistics, higher order features, natural language processing},
  owner     = {gutmann},
  publisher = {Springer US},
  timestamp = {2013.05.15},
  url       = {https://link.springer.com/article/10.1007/s10994-013-5351-x},
}

@Inproceedings{Sasaki2012,
  author    = {Sasaki, H. and Gutmann, M. U. and Shouno, H. and Hyv\"arinen, A.},
  booktitle = {JMLR: Workshop and Conference Proceedings},
  title     = {Topographic analysis of correlated components},
  year      = {2012},
  pages     = {1--14},
  series    = {Asian Conference on Machine Learning (ACML)},
  volume    = {25},
  abstract  = {Independent component analysis (ICA) is a method to estimate components which are as statistically independent as possible. However, in many practical applications, the estimated components are not independent. Recent variants of ICA have made use of such residual dependencies to estimate an ordering (topography) of the components. Like in ICA, the components in those variants are assumed to be uncorrelated, which might be a rather strict condition. In this paper, we address this shortcoming. We propose a generative model for the source where the components can have linear and higher order correlations, which generalizes models in use so far. Based on the model, we derive a method to estimate topographic representations. In numerical experiments on artificial data, the new method is shown to be more widely applicable than previously proposed extensions of ICA. We learn topographic representations for two kinds of real data sets: for outputs of simulated complex cells in the primary visual cortex and for text data.},
  file      = {:Sasaki2012.pdf:PDF},
  keywords  = {independent component analysis, topographic representation, higher order correlation, linear correlation, natural image statistics, natural language processing.},
  owner     = {gutmann},
  timestamp = {2012.10.04},
  url       = {http://proceedings.mlr.press/v25/sasaki12},
}

@Article{Simola2020,
  author    = {Simola, U. and Cisewski-Kehe, J. and Gutmann, M. U. and Corander, J.},
  journal   = {Bayesian Analysis},
  title     = {Adaptive Approximate {B}ayesian Computation Tolerance Selection},
  year      = {2021},
  number    = {2},
  pages     = {397--423},
  volume    = {16},
  abstract  = {Approximate Bayesian Computation (ABC) methods are increasingly used for inference in situations in which the likelihood function is either computationally costly or intractable to evaluate. Extensions of the basic ABC rejection algorithm have improved the computational efficiency of the procedure and broadened its applicability. The ABC  Population Monte Carlo (ABC-PMC) approach has become a popular choice for approximate sampling from the posterior. ABC-PMC is a sequential sampler with an iteratively decreasing value of the tolerance, which specifies how close the simulated data need to be to the real data for acceptance. We propose a method for adaptively selecting a sequence of tolerances that improves the computational efficiency of the algorithm over other common techniques. In addition we define a stopping rule as a by-product of the adaptation procedure, which assists in automating termination of sampling. The proposed automatic ABC-PMC algorithm can be easily implemented and we present several examples demonstrating its benefits in terms of computational efficiency.},
  arxiv     = {https://arxiv.org/abs/1907.01505},
  doi       = {10.1214/20-BA1211},
  file      = {:Simola2020.pdf:PDF},
  keywords  = {complex stochastic modeling, likelihood-free methods, sequential Monte Carlo},
  owner     = {mgutmann},
  publisher = {International Society for Bayesian Analysis},
  timestamp = {2019.07.11},
  url       = {https://doi.org/10.1214/20-BA1211},
}

@Inproceedings{Srivastava2017,
  author    = {Srivastava, A. and Valkov, A. and Russell, C. and Gutmann, M. U. and Sutton, C.},
  booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  title     = {{VEEGAN}: Reducing Mode Collapse in {GAN}s using Implicit Variational Learning},
  year      = {2017},
  abstract  = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
  arxiv     = {https://arxiv.org/abs/1705.07761},
  file      = {Srivastava2017.pdf:Srivastava2017.pdf:PDF},
  journal   = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
  owner     = {mgutmann},
  timestamp = {2017.05.24},
  url       = {http://papers.nips.cc/paper/6923-veegan-reducing-mode-collapse-in-gans-using-implicit-variational-learning},
}

@Inproceedings{Srivastava2020,
  author    = {Srivastava, A. and Xu, K. and Gutmann, M. U. and Sutton, C.},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  title     = {Generative Ratio Matching Networks},
  year      = {2020},
  abstract  = {Deep generative models can learn to generate realistic-looking images on several natural image datasets, but many of the most effective methods are adversarial methods, which require careful balancing of training between a generator network and a discriminator network. Maximum mean discrepancy networks (MMD-nets) avoid this issue using the kernel trick, but unfortunately they have not on their own been able to match the performance of adversarial training. We present a new method of training MMD-nets, based on learning a mapping of samples from the data and from the model into a lower dimensional space, in which MMD training can be more effective. We call these networks ratio matching MMD networks (RMMMDnets). We train the mapping to preserve density ratios between the densities over the low-dimensional space and the original space. This ensures that matching the model distribution to the data in the low-dimensional space will also match the original distributions. We show that RM-MMDnets have better performance and better stability than recent adversarial methods for training MMD-nets.},
  arxiv     = {https://arxiv.org/abs/1806.00101},
  file      = {Srivastava2020.pdf:Srivastava2020.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2018.07.26},
  url       = {https://openreview.net/forum?id=SJg7spEYDS},
}

@Article{Tietavainen2017,
  author    = {Tiet\"av\"ainen, A. and Gutmann, M. U. and Keski-Vakkuri, E. and Corander, J. and Haeggstr\"om, E.},
  journal   = {Scientific Reports},
  title     = {Bayesian inference of physiologically meaningful parameters from body sway measurements},
  year      = {2017},
  number    = {3771},
  pages     = {1--14},
  volume    = {7},
  abstract  = {The control of the human body sway by the central nervous system, muscles, and conscious brain is of interest since body sway carries information about the physiological status of a person. Several models have been proposed to describe body sway in an upright standing position, however, due to the statistical intractability of the more realistic models, no formal parameter inference has previously been conducted and the expressive power of such models for real human subjects remains unknown. Using the latest advances in Bayesian statistical inference for intractable models, we fitted a nonlinear control model to posturographic measurements, and we showed that it can accurately predict the sway characteristics of both simulated and real subjects. Our method provides a full statistical characterization of the uncertainty related to all model parameters as quantified by posterior probability density functions, which is useful for comparisons across subjects and test settings. The ability to infer intractable control models from sensor data opens new possibilities for monitoring and predicting body status in health applications.},
  doi       = {https://doi.org/10.1038/s41598-017-02372-1},
  file      = {:Tietavainen2017.pdf:PDF},
  issn      = {2045-2322},
  keywords  = {Approximate Bayesian inference, nonlinear dynamical systems, balance and muscular control},
  timestamp = {2017.07.24},
  url       = {https://www.nature.com/articles/s41598-017-02372-1},
}

@Article{Todorovic2019,
  author    = {Todorovi\'c, M. and Gutmann, M. U. and Corander, J. and Rinke, P.},
  journal   = {npj Computational Materials},
  title     = {Bayesian inference of atomistic structure in functional materials},
  year      = {2019},
  number    = {1},
  pages     = {35},
  volume    = {5},
  abstract  = {Tailoring the functional properties of advanced organic/inorganic heterogeneous devices to their intended technological applications requires knowledge and control of the microscopic structure inside the device. Atomistic quantum mechanical simulation methods deliver accurate energies and properties for individual configurations, however, finding the most favourable configurations remains computationally prohibitive. We propose a "building block"-based Bayesian Optimisation Structure Search (BOSS) approach for addressing extended organic/inorganic interface problems and demonstrate its feasibility in a molecular surface adsorption study. In BOSS, a Bayesian model identifies material energy landscapes in an accelerated fashion from atomistic configurations sampled during active learning. This allowed us to identify several most favourable molecular adsorption configurations for C60 on the (101) surface of TiO2 anatase and clarify the key molecule-surface interactions governing structural assembly. Inferred structures were in good agreement with detailed experimental images of this surface adsorbate, demonstrating good predictive power of BOSS and opening the route towards large-scale surface adsorption studies of molecular aggregates and films.},
  arxiv     = {https://arxiv.org/abs/1708.09274},
  doi       = {https://doi.org/10.1038/s41524-019-0175-2},
  file      = {:Todorovic2019.pdf:PDF},
  issn      = {2057-3960},
  owner     = {mgutmann},
  refid     = {Todorovic2019},
  timestamp = {2019.03.18},
  url       = {https://www.nature.com/articles/s41524-019-0175-2},
}

@Inproceedings{Kleinegesse2020a,
  author    = {Kleinegesse, S. and Gutmann, M. U.},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  title     = {Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation},
  year      = {2020},
  editor    = {Daum\'e, Hal III and Singh, Aarti},
  month     = {13--18 Jul},
  pages     = {5316--5326},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  abstract  = {Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.},
  arxiv     = {https://arxiv.org/abs/2002.08129},
  file      = {:Kleinegesse2020a.pdf:PDF},
  owner     = {mgutmann},
  pdf       = {http://proceedings.mlr.press/v119/kleinegesse20a/kleinegesse20a.pdf},
  timestamp = {2020.03.02},
  url       = {http://proceedings.mlr.press/v119/kleinegesse20a.html},
}

@Article{Neyton2020,
  author    = {Neyton, Lucile P. A. and Zheng, Xiaozhong and Skouras, Christos and Doeschl-Wilson, Andrea and Gutmann, Michael U. and Uings, Iain and Rao, Francesco V. and Nicolas, Armel and Marshall, Craig and Wilson, Lisa-Marie and Baillie, J. Kenneth and Mole, Damian J.},
  journal   = {Annals of Surgery},
  title     = {Molecular Patterns in Acute Pancreatitis Reflect Generalizable Endotypes of the Host Response to Systemic Injury in Humans},
  year      = {2020},
  number    = {in press},
  abstract  = {Objective: Acute Pancreatitis (AP) is sudden onset pancreas inflammation that causes systemic injury with a wide and markedly heterogeneous range of clinical consequences. Here, we hypothesized that this observed clinical diversity corresponds to diversity in molecular subtypes that can be identified in clinical and multiomics data. Summary Background Data: Observational cohort study. n = 57 for the discovery cohort (clinical, transcriptomics, proteomics, and metabolomics data) and n = 312 for the validation cohort (clinical and metabolomics data). Methods: We integrated coincident transcriptomics, proteomics, and metabolomics data at serial time points between admission to hospital and up to 48hours after recruitment from a cohort of patients presenting with acute pancreatitis. We systematically evaluated 4 different metrics for patient similarity using unbiased mathematical, biological, and clinical measures of internal and external validity. We next compared the AP molecular endotypes with previous descriptions of endotypes in a critically ill population with acute respiratory distress syndrome (ARDS). Results: Our results identify 4 distinct and stable AP molecular endotypes. We validated our findings in a second independent cohort of patients with AP. We observed that 2 endotypes in AP recapitulate disease endotypes previously reported in ARDS. Conclusions: Our results show that molecular endotypes exist in AP and reflect biological patterns that are also present in ARDS, suggesting that generalizable patterns exist in diverse presentations of critical illness.},
  arxiv     = {https://www.biorxiv.org/content/10.1101/539569v3},
  doi       = {https://doi.org/10.1101/539569},
  issn      = {0003-4932},
  keywords  = {acute pancreatitis, cluster analysis, critical illness trajectory, endotypes, multiomics, time series},
  owner     = {mgutmann},
  refid     = {00000658-900000000-94475},
  timestamp = {2020.06.23},
  url       = {https://journals.lww.com/annalsofsurgery/Fulltext/9000/Molecular_Patterns_in_Acute_Pancreatitis_Reflect.94475.aspx},
}

@Inproceedings{Ikonomov2020,
  author    = {Ikonomov, B. and Gutmann, M. U.},
  booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Robust Optimisation {M}onte {C}arlo},
  year      = {2020},
  address   = {Online},
  editor    = {Chiappa, Silvia and Calandra, Roberto},
  month     = {26--28 Aug},
  pages     = {2819--2829},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {108},
  abstract  = {This paper is on Bayesian inference for parametric statistical models that are defined by a stochastic simulator which specifies how data is generated. Exact sampling is then possible but evaluating the likelihood function is typically prohibitively expensive. Approximate Bayesian Computation (ABC) is a framework to perform approximate inference in such situations. While basic ABC algorithms are widely applicable, they are notoriously slow and much research has focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has recently been proposed as an efficient and embarrassingly parallel method that leverages optimisation to accelerate the inference. In this paper, we demonstrate an important previously unrecognised failure mode of OMC: It generates strongly overconfident approximations by collapsing regions of similar or near-constant likelihood into a single point. We propose an efficient, robust generalisation of OMC that corrects this. It makes fewer assumptions, retains the main benefits of OMC, and can be performed either as post-processing to OMC or as a stand-alone computation. We demonstrate the effectiveness of the proposed Robust OMC on toy examples and tasks in inverse-graphics where we perform Bayesian inference with a complex image renderer.},
  arxiv     = {https://arxiv.org/abs/1904.00670},
  file      = {Ikonomov2020.pdf:Ikonomov2020.pdf:PDF},
  owner     = {mgutmann},
  pdf       = {http://proceedings.mlr.press/v108/ikonomov20a/ikonomov20a.pdf},
  timestamp = {2020.06.29},
  url       = {http://proceedings.mlr.press/v108/ikonomov20a.html},
}

@Inproceedings{Kleinegesse2019,
  author    = {Kleinegesse, S. and Gutmann, M. U.},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  title     = {Efficient {B}ayesian Experimental Design for Implicit Models},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  month     = {16--18 Apr},
  number    = {89},
  pages     = {1584--1592},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {89},
  abstract  = {Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search. We find that this increases efficiency and allows us to consider higher design dimensions.},
  arxiv     = {https://arxiv.org/abs/1810.09912},
  file      = {Kleinegesse2019.pdf:Kleinegesse2019.pdf:PDF},
  owner     = {mgutmann},
  timestamp = {2018.10.24},
  url       = {http://proceedings.mlr.press/v89/kleinegesse19a},
}

@InProceedings{LopezGuevara2020,
  author    = {{Lopez Guevara}, Tatiana and Rita Pucci and Nicholas Taylor and Gutmann, Michael U. and Ram Ramamoorthy and Kartic Subr},
  booktitle = {Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)},
  title     = {Stir to Pour: Efficient Calibration of Liquid Properties for Pouring Actions},
  year      = {2020},
  month     = jul,
  abstract  = {Humans use simple probing actions to develop intuition about the physical behaviour of common objects. Such intuition is particularly useful for adaptive estimation of favourable manipulation strategies of those objects in novel contexts. For example, observing the effect of tilt on a transparent bottle containing an unknown liquid provides clues on how the liquid might be poured. It is desirable to equip general-purpose robotic systems with this capability because it is inevitable that they will encounter novel objects and scenarios. In this paper, we teach a robot to use a simple, specified probing strategy  stirring with a stick  to reduce spillage while pouring unknown liquids. In the probing step, we continuously observe the effects of a real robot stirring a liquid, while simultaneously tuning the parameters to a model (simulator) until the two outputs are in agreement. We obtain optimal simulation parameters, characterising the unknown liquid, via a Bayesian Optimiser that minimises the discrepancy between real and simulated outcomes. Then, we optimise the pouring policy conditioning on the optimal simulation parameters determined via stirring. We show that using stirring as a probing strategy results in reduced spillage for three qualitatively different liquids when executed on a UR10 Robot, compared to probing via pouring. Finally, we provide quantitative insights into the reason for stirring being a suitable calibration task for pouring  a step towards automatic discovery of probing strategies.},
  day       = {1},
  file      = {:LopezGuevara2020.pdf:PDF},
  language  = {English},
  owner     = {mgutmann},
  timestamp = {2020.07.23},
  url       = {https://www.iros2020.org/index.html},
}

@Article{Kleinegesse2020b,
  author    = {Kleinegesse, S. and Drovandi, C. and Gutmann, M. U.},
  journal   = {Bayesian Analysis},
  title     = {Sequential {B}ayesian Experimental Design for Implicit Models via Mutual Information},
  year      = {2021},
  number    = {16},
  pages     = {773--802},
  volume    = {3},
  abstract  = {Bayesian experimental design (BED) is a framework that uses statistical models and decision making under uncertainty to optimise the cost and performance of a scientific experiment. Sequential BED, as opposed to static BED, considers the scenario where we can sequentially update our beliefs about the model parameters through data gathered in the experiment. A class of models of particular interest for the natural and medical sciences are implicit models, where the data generating distribution is intractable, but sampling from it is possible. Even though there has been a lot of work on static BED for implicit models in the past few years, the notoriously difficult problem of sequential BED for implicit models has barely been touched upon. We address this gap in the literature by devising a novel sequential design framework for parameter estimation that uses the Mutual Information (MI) between model parameters and simulated data as a utility function to find optimal experimental designs, which has not been done before for implicit models. Our approach uses likelihood-free inference by ratio estimation to simultaneously estimate posterior distributions and the MI. During the sequential BED procedure we utilise Bayesian optimisation to help us optimise the MI utility. We find that our framework is efficient for the various implicit models tested, yielding accurate parameter estimates after only a few iterations.},
  arxiv     = {https://arxiv.org/abs/2003.09379},
  doi       = {10.1214/20-BA1225},
  file      = {:Kleinegesse2020b.pdf:PDF},
  keywords  = {Approximate Bayesian Computation, Bayesian experimental design, implicit models, likelihood-free inference, mutual information},
  owner     = {mgutmann},
  publisher = {International Society for Bayesian Analysis},
  timestamp = {2020.03.23},
}

@Article{Thomas2020,
  author    = {Thomas, O. and Dutta, R. and Corander, J. and Kaski, S. and Gutmann, M. U.},
  journal   = {Bayesian Analysis},
  title     = {Likelihood-Free Inference by Ratio Estimation},
  year      = {2020},
  number    = {advance publication},
  abstract  = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of "closeness" is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on toy problems and use it to perform inference for stochastic nonlinear dynamical systems.},
  arxiv     = {https://arxiv.org/abs/1611.10242},
  doi       = {https://doi.org/10.1214/20-BA1238},
  file      = {:Thomas2020.pdf:PDF},
  keywords  = {approximate Bayesian computation, synthetic likelihood, summary statistics selection, density-ratio estimation; logistic regression; probabilistic classification, stochastic dynamical systems},
  owner     = {mgutmann},
  timestamp = {2017.04.27},
  url       = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-1/Likelihood-Free-Inference-by-Ratio-Estimation/10.1214/20-BA1238.full},
}

@Inproceedings{Rhodes2020a,
  author    = {Rhodes, B. and Xu, K. and Gutmann, M. U.},
  booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS 2020)},
  title     = {Telescoping Density-Ratio Estimation},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  note      = {(Spotlight, top 4\% of submissions)},
  pages     = {4905--4916},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling.},
  arxiv     = {https://arxiv.org/abs/2006.12204},
  file      = {:Rhodes2020a.pdf:PDF},
  keywords  = {density ratio estimation, mutual information, energy-based models},
  owner     = {mgutmann},
  timestamp = {2020.06.29},
  url       = {https://papers.nips.cc/paper/2020/hash/33d3b157ddc0896addfb22fa2a519097-Abstract.html},
}

@Inproceedings{Chen2021a,
  author    = {Chen, Y. and Zhang, D. and Gutmann, M. U. and Courville, A. and Zhu, Z.},
  booktitle = {International Conference on Learning Representations (ICLR)},
  title     = {Neural Approximate Sufficient Statistics for Implicit Models},
  year      = {2021},
  arxiv     = {https://arxiv.org/abs/2010.10079},
  file      = {:Chen2021a.pdf:PDF},
  keywords  = {Sufficient statistics, mutual information, likelihood-free inference, implicit models, approximate Bayesian inference},
  owner     = {mgutmann},
  timestamp = {2020.10.21},
  url       = {https://openreview.net/forum?id=SRDuJssQud},
}

@Inproceedings{Ivanova2021,
  author    = {Ivanova, Desi R and Adam Foster and Steven Kleinegesse and Gutmann, Michael U and Tom Rainforth},
  booktitle = {Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems (NeuRIPS 2021)},
  title     = {Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods},
  year      = {2021},
  publisher = {Neural Information Processing Systems},
  abstract  = {We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment. The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models.},
  arxiv     = {https://arxiv.org/abs/2111.02329},
  keywords  = {Experimental design},
  url       = {https://papers.nips.cc/paper/2021/hash/d811406316b669ad3d370d78b51b1d2e-Abstract.html},
}

@InProceedings{Valentin2021a,
  author    = {Simon Valentin and Steven Kleinegesse and Bramley, Neil R and Gutmann, Michael U and Lucas, Christopher G},
  booktitle = {NeurIPS 2021 Workshop {"}AI for Science{"}},
  title     = {Bayesian Optimal Experimental Design for Simulator Models of Cognition},
  year      = {2021},
  abstract  = {Bayesian optimal experimental design (BOED) is a methodology to identify experiments that are expected to yield informative data. Recent work in cognitive science considered BOED for computational models of human behavior with tractable and known likelihood functions. However, tractability often comes at the cost of realism; simulator models that can capture the richness of human behavior are often intractable. In this work, we combine recent advances in BOED and approximate inference for intractable models, using machine-learning methods to find optimal experimental designs, approximate sufficient summary statistics and amortized posterior distributions. Our simulation experiments on multi-armed bandit tasks show that our method results in improved model discrimination and parameter estimation, as compared to experimental designs commonly used in the literature.},
  arxiv     = {https://arxiv.org/abs/2110.15632},
}

@Article{Gutmann2022a,
  author    = {Gutmann, Michael U. and Kleinegesse, Steven and Rhodes, Benjamin},
  journal   = {Behaviormetrika},
  title     = {Statistical applications of contrastive learning},
  year      = {2022},
  month     = jun,
  abstract  = {The likelihood function plays a crucial role in statistical inference and experimental design. However, it is computationally intractable for several important classes of statistical models, including energy-based models and simulator-based models. Contrastive learning is an intuitive and computationally feasible alternative to likelihood-based learning. We here first provide an introduction to contrastive learning and then show how we can use it to derive methods for diverse statistical problems, namely parameter estimation for energy-based models, Bayesian inference for simulator-based models, as well as experimental design.},
  arxiv     = {https://arxiv.org/abs/2204.13999},
  issn      = {1349-6964},
  owner     = {mgutmann},
  refid     = {Gutmann2022},
  timestamp = {2022.06.03},
  url       = {https://doi.org/10.1007/s41237-022-00168-w},
}

@Article{Wang2022a,
  author    = {Wang, Bo and Law, Andy and Regan, Tim and Parkinson, Nicholas and Cole, Joby and Russell, Clark D. and Dockrell, David H. and Gutmann, Michael U. and Baillie, J. Kenneth},
  journal   = {Bioinformatics},
  title     = {Systematic comparison of ranking aggregation methods for gene lists in experimental results},
  year      = {2022},
  month     = sep,
  number    = {21},
  pages     = {4927--4933},
  volume    = {38},
  abstract  = {A common experimental output in biomedical science is a list of genes implicated in a given biological process or disease. The gene lists resulting from a group of studies answering the same, or similar, questions can be combined by ranking aggregation methods to find a consensus or a more reliable answer. Evaluating a ranking aggregation method on a specific type of data before using it is required to support the reliability since the property of a dataset can influence the performance of an algorithm. Such evaluation on gene lists is usually based on a simulated database because of the lack of a known truth for real data. However, simulated datasets tend to be too small compared to experimental data and neglect key features, including heterogeneity of quality, relevance and the inclusion of unranked lists.In this study, a group of existing methods and their variations which are suitable for meta-analysis of gene lists are compared using simulated and real data. Simulated data was used to explore the performance of the aggregation methods as a function of emulating the common scenarios of real genomic data, with various heterogeneity of quality, noise level, and a mix of unranked and ranked data using 20000 possible entities. In addition to the evaluation with simulated data, a comparison using real genomic data on the SARS-CoV-2 virus, cancer (NSCLC), and bacteria (macrophage apoptosis) was performed. We summarise the results of our evaluation in a simple flowchart to select a ranking aggregation method, and in an automated implementation using the meta-analysis by information content (MAIC) algorithm to infer heterogeneity of data quality across input data sets.The code for simulated data generation and running edited version of algorithms: https://github.com/baillielab/comparison_of_RA_methods.Code to perform an optimal selection of methods based on the results of this review, using the MAIC algorithm to infer the characteristics of an input dataset, can be downloaded here: https://github.com/baillielab/maic. An online service for running MAIC: https://baillielab.net/maic. Supplementary file 1 is available at Bioinformatics online. Supporting data 1-7 (supporting results and collected real genomic data) are available on GitHub at: https://github.com/baillielab/comparison_of_RA_methods.},
  arxiv     = {https://www.biorxiv.org/content/early/2022/01/10/2022.01.09.475491},
  issn      = {1367-4803},
  owner     = {mgutmann},
  timestamp = {2022.09.13},
  url       = {https://doi.org/10.1093/bioinformatics/btac621},
}

@Article{Ocal2022,
  author    = {\"Ocal, Kaan and Gutmann, Michael U. and Sanguinetti, Guido and Grima, Ramon},
  journal   = {Journal of The Royal Society Interface},
  title     = {Inference and uncertainty quantification of stochastic gene expression via synthetic models},
  year      = {2022},
  number    = {192},
  pages     = {20220153},
  volume    = {19},
  arxiv     = {https://www.biorxiv.org/content/10.1101/2022.01.25.477666v1},
  doi       = {10.1098/rsif.2022.0153},
  owner     = {mgutmann},
  publisher = {Royal Society},
  timestamp = {2022.11.18},
  url       = {https://doi.org/10.1098/rsif.2022.0153},
}

@Article{Rhodes2022a,
  author  = {Benjamin Rhodes and Michael U. Gutmann},
  journal = {Transactions on Machine Learning Research},
  title   = {Enhanced gradient-based {MCMC} in discrete spaces},
  year    = {2022},
  arxiv   = {https://arxiv.org/abs/2208.00040},
  url     = {https://openreview.net/forum?id=j2Mid5hFUJ},
}

@Article{Eduardo2023a,
  author    = {Afonso Eduardo and Michael U. Gutmann},
  journal   = {Transactions on Machine Learning Research},
  title     = {Bayesian Optimization with Informative Covariance},
  year      = {2023},
  arxiv     = {https://arxiv.org/abs/2208.02704},
  issn      = {2835-8856},
  keywords  = {Bayesian Optimization, Gaussian processes},
  owner     = {mgutmann},
  timestamp = {2022.11.18},
  url       = {https://openreview.net/forum?id=JwgVBv18RG},
}

@Article{Srivastava2023a,
  author    = {Akash Srivastava and Seungwook Han and Kai Xu and Benjamin Rhodes and Michael U. Gutmann},
  journal   = {Transactions on Machine Learning Research},
  title     = {Estimating the Density Ratio between Distributions with High Discrepancy using Multinomial Logistic Regression},
  year      = {2023},
  arxiv     = {https://arxiv.org/abs/2305.00869},
  issn      = {2835-8856},
  owner     = {mgutmann},
  timestamp = {2023.04.07},
  url       = {https://openreview.net/forum?id=jM8nzUzBWr},
}

@Article{Simkus2023a,
  author    = {Simkus, Vaidotas and Rhodes, Benjamin and Gutmann, Michael U.},
  journal   = {Journal of Machine Learning Research},
  title     = {Variational {{Gibbs}} Inference for Statistical Model Estimation from Incomplete Data},
  year      = {2023},
  number    = {196},
  pages     = {1--72},
  volume    = {24},
  abstract  = {Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose method to estimate the parameters of statistical models from incomplete data. We validate VGI on a set of synthetic and real-world estimation tasks, estimating important machine learning models such as variational autoencoders and normalising flows from incomplete data. The proposed method, whilst general-purpose, achieves competitive or better performance than existing model-specific estimation methods.},
  arxiv     = {https://arxiv.org/abs/2111.13180},
  keywords  = {Statistical Model Estimation, Variational Inference, Gibbs Sampling, Missing Data, Amortised Inference},
  owner     = {mgutmann},
  timestamp = {2023.07.11},
  url       = {http://jmlr.org/papers/v24/21-1373.html},
}

@Article{Chen2023a,
  author    = {Yanzhi Chen and Michael U. Gutmann and Adrian Weller},
  journal   = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  title     = {Is Learning Summary Statistics Necessary for Likelihood-free Inference?},
  year      = {2023},
  abstract  = {Likelihood-free inference (LFI) is a set of techniques for inference in implicit statistical models. A longstanding question in LFI has been how to design or learn good summary statistics of data, but this might now seem unnecessary due to the advent of recent end-to-end (i.e. neural network-based) LFI methods. In this work, we rethink this question with a new method for learning summary statistics. We show that learning sufficient statistics may be easier than direct posterior inference, as the former problem can be reduced to a set of low-dimensional, easy-to-solve learning problems. This suggests us to explicitly decouple summary statistics learning from posterior inference in LFI. Experiments on diverse inference tasks with different data types validate our hypothesis.},
  owner     = {mgutmann},
  timestamp = {2023.07.11},
  url       = {https://openreview.net/forum?id=jjzJ768iV1},
}

@Article{Gkolemis2023a,
  author    = {Vasilis Gkolemis and Michael Gutmann and Henri Pesonen},
  journal   = {Journal of Statistical Software},
  title     = {An Extendable {P}ython Implementation of {R}obust {O}ptimisation {M}onte {C}arlo},
  year      = {2023},
  month     = sep,
  pages     = {1--25},
  abstract  = {Performing inference in statistical models with an intractable likelihood is challenging, therefore, most likelihood-free inference (LFI) methods encounter accuracy and efficiency limitations. In this paper, we present the implementation of the LFI method Robust Optimisation Monte Carlo (ROMC) in the Python package ELFI. ROMC is a novel and efficient (highly-parallelizable) LFI framework that provides accurate weighted samples from the posterior. Our implementation can be used in two ways. First, a scientist may use it as an out-of-the-box LFI algorithm; we provide an easy-to-use API harmonized with the principles of ELFI, enabling effortless comparisons with the rest of the methods included in the package. Additionally, we have carefully split ROMC into isolated components for supporting extensibility. A researcher may experiment with novel method(s) for solving part(s) of ROMC without reimplementing everything from scratch. In both scenarios, the ROMC parts can run in a fully-parallelized manner, exploiting all CPU cores. We also provide helpful functionalities for (i) inspecting the inference process and (ii) evaluating the obtained samples. Finally, we test the robustness of our implementation on some typical LFI examples.},
  arxiv     = {https://arxiv.org/abs/2309.10612},
  day       = {15},
  issn      = {1548-7660},
  keywords  = {Bayesian inference, implicit models, likelihood-free, Python, ELFI},
  language  = {English},
  publisher = {University of California at Los Angeles},
}

@Article{Simkus2023b,
  author    = {Simkus, Vaidotas and Gutmann, Michael U.},
  journal   = {Transactions on Machine Learning Research},
  title     = {Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling},
  year      = {2023},
  arxiv     = {https://arxiv.org/abs/2308.09078},
  issn      = {2835-8856},
  keywords  = {Variational Autoencoder, MCMC, Importance Sampling, Probabilistic Inference, Missing Data},
  owner     = {mgutmann},
  timestamp = {2023.12.14},
  url       = {https://openreview.net/forum?id=I5sJ6PU6JN},
}

@Article{Valentin2024,
  author    = {Valentin, Simon and Kleinegesse, Steven and Bramley, Neil R. and Seri\`es, Peggy and Gutmann, Michael U. and Lucas, Christopher G.},
  journal   = {eLife},
  title     = {Designing optimal behavioral experiments using machine learning},
  year      = {2024},
  issn      = {2050-084X},
  month     = jan,
  pages     = {e86224},
  volume    = {13},
  abstract  = {Computational models are powerful tools for understanding human cognition and behavior. They let us express our theories clearly and precisely and offer predictions that can be subtle and often counter-intuitive. However, this same richness and ability to surprise means our scientific intuitions and traditional tools are ill-suited to designing experiments to test and compare these models. To avoid these pitfalls and realize the full potential of computational modeling, we require tools to design experiments that provide clear answers about what models explain human behavior and the auxiliary assumptions those models must make. Bayesian optimal experimental design (BOED) formalizes the search for optimal experimental designs by identifying experiments that are expected to yield informative data. In this work, we provide a tutorial on leveraging recent advances in BOED and machine learning to find optimal experiments for any kind of model that we can simulate data from, and show how by-products of this procedure allow for quick and straightforward evaluation of models and their parameters against real experimental data. As a case study, we consider theories of how people balance exploration and exploitation in multi-armed bandit decision-making tasks. We validate the presented approach using simulations and a real-world experiment. As compared to experimental designs commonly used in the literature, we show that our optimal designs more efficiently determine which of a set of models best account for individual human behavior, and more efficiently characterize behavior given a preferred model. At the same time, formalizing a scientific question such that it can be adequately addressed with BOED can be challenging and we discuss several potential caveats and pitfalls that practitioners should be aware of. We provide code to replicate all analyses as well as tutorial notebooks and pointers to adapt the methodology to different experimental settings.},
  arxiv     = {https://arxiv.org/abs/2305.07721},
  doi       = {https://doi.org/10.7554/eLife.86224},
  editor    = {Gold, Joshua I.},
  groups    = {[mgutmann:]},
  keywords  = {experimental design, machine learning, computational modeling},
  owner     = {mgutmann},
  publisher = {eLife Sciences Publications, Ltd},
  timestamp = {2024.01.24},
  url       = {https://doi.org/10.7554/eLife.86224},
}

@Article{Simkus2024a,
  author    = {Simkus, Vaidotas and Gutmann, Michael U.},
  journal   = {Transactions on Machine Learning Research (TMLR)},
  title     = {Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families},
  year      = {2024},
  arxiv     = {https://arxiv.org/abs/2403.03069},
  issn      = {2835-8856},
  keywords  = {variational autoencoders, missing data, incomplete data, mixture models},
  owner     = {mgutmann},
  timestamp = {2024.03.06},
  url       = {https://openreview.net/forum?id=lLVmIvZfry},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory-mgutmann-zermatt:../myPapers;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:[mgutmann:]\;2\;1\;\;\;\;;
}
